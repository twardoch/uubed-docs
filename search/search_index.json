{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The QuadB64 Codex","text":""},{"location":"#position-safe-encoding-for-substring-based-search-systems","title":"Position-Safe Encoding for Substring-Based Search Systems","text":"<p>Welcome to The QuadB64 Codex, the comprehensive guide to the QuadB64 encoding family and the <code>uubed</code> library. This documentation covers everything from theoretical foundations to practical implementations of position-safe encoding schemes designed for modern search systems.</p>"},{"location":"#what-is-quadb64","title":"What is QuadB64?","text":"<p>QuadB64 is a revolutionary family of encoding schemes that solve the substring pollution problem inherent in traditional Base64 encoding. When Base64-encoded data is indexed by search engines or vector databases, arbitrary substrings can match across unrelated documents, leading to false positives and degraded search quality.</p> <p>The QuadB64 family provides position-safe encodings that preserve locality and prevent spurious matches, making them ideal for:</p> <ul> <li>\ud83d\udd0d Search Engines: Prevent false matches in substring-based search</li> <li>\ud83d\uddc4\ufe0f Vector Databases: Maintain locality in embedded representations</li> <li>\ud83d\udcca Data Analysis: Preserve meaningful patterns in encoded data</li> <li>\ud83d\udd10 Security Applications: Reduce information leakage through encoding</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#position-safe-encoding","title":"\ud83c\udfaf Position-Safe Encoding","text":"<p>Every position in a QuadB64-encoded string carries positional information, preventing arbitrary substring matches.</p>"},{"location":"#multiple-encoding-schemes","title":"\ud83e\udde9 Multiple Encoding Schemes","text":"<p>Choose from various encoding strategies optimized for different use cases: - Eq64: Full embeddings with position markers - Shq64: SimHash-based compact representations - T8q64: Top-k index encoding for dimensionality reduction - Zoq64: Z-order curve encoding for spatial locality</p>"},{"location":"#high-performance","title":"\u26a1 High Performance","text":"<p>Native implementations with SIMD optimizations ensure encoding/decoding speeds comparable to standard Base64.</p>"},{"location":"#easy-integration","title":"\ud83d\udd27 Easy Integration","text":"<p>Simple Python API with drop-in replacements for standard Base64 operations.</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from uubed import encode_eq64, decode_eq64\n\n# Encode binary data with position safety\ndata = b\"Hello, QuadB64!\"\nencoded = encode_eq64(data)\nprint(f\"Encoded: {encoded}\")\n\n# Decode back to original\ndecoded = decode_eq64(encoded)\nassert decoded == data\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> Quick Start</p> <p>Get up and running with QuadB64 in minutes</p> <p> Quick Start Guide</p> </li> <li> <p> Theory</p> <p>Understand the mathematical foundations</p> <p> Read the Theory</p> </li> <li> <p> API Reference</p> <p>Complete API documentation</p> <p> Browse API</p> </li> <li> <p> Benchmarks</p> <p>Performance comparisons and analysis</p> <p> View Benchmarks</p> </li> </ul>"},{"location":"#why-quadb64","title":"Why QuadB64?","text":"<p>Traditional Base64 encoding was designed for email attachments, not modern search systems. When search engines index Base64-encoded content, they treat it as regular text, leading to:</p> <ul> <li>False Positives: Random substrings match across unrelated documents</li> <li>Poor Relevance: Search results contaminated with irrelevant matches</li> <li>Wasted Resources: Indexing and searching meaningless character sequences</li> </ul> <p>QuadB64 solves these problems by making every encoded position unique and meaningful, dramatically improving search quality while maintaining encoding efficiency.</p>"},{"location":"#project-status","title":"Project Status","text":"<p>The <code>uubed</code> library is under active development. Current features include:</p> <ul> <li>\u2705 Core encoding/decoding algorithms</li> <li>\u2705 Python bindings with type hints</li> <li>\u2705 Comprehensive test suite</li> <li>\ud83d\udea7 Native performance optimizations</li> <li>\ud83d\udea7 Additional encoding schemes</li> <li>\ud83d\udcc5 Streaming API support</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details on how to get involved.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"404/","title":"Page Not Found","text":"<p>Sorry, the page you're looking for doesn't exist.</p> <p>Return to Home</p>"},{"location":"404/#helpful-links","title":"Helpful Links","text":"<ul> <li>Quick Start Guide</li> <li>Installation</li> <li>API Reference</li> <li>Encoding Family</li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#main-functions","title":"Main Functions","text":""},{"location":"api/#encodeembedding-methodauto-kwargs","title":"<code>encode(embedding, method=\"auto\", **kwargs)</code>","text":"<p>Encode embedding vector using specified method.</p> <p>Parameters: - <code>embedding</code> (Union[List[int], np.ndarray, bytes]): Vector to encode (0-255 integers) - <code>method</code> (str): Encoding method - \"eq64\", \"shq64\", \"t8q64\", \"zoq64\", or \"auto\" - <code>**kwargs</code>: Method-specific parameters</p> <p>Returns: - <code>str</code>: Position-safe encoded string</p> <p>Example: <pre><code>import numpy as np\nfrom uubed import encode\n\nembedding = np.random.randint(0, 256, 256, dtype=np.uint8)\nencoded = encode(embedding, method=\"eq64\")\n</code></pre></p>"},{"location":"api/#decodeencoded-methodnone","title":"<code>decode(encoded, method=None)</code>","text":"<p>Decode encoded string back to bytes.</p> <p>Parameters: - <code>encoded</code> (str): Encoded string - <code>method</code> (str, optional): Encoding method (auto-detected if None)</p> <p>Returns: - <code>bytes</code>: Original bytes</p> <p>Note: Only eq64 supports full decoding. Other methods are lossy compressions.</p> <p>Example: <pre><code>from uubed import encode, decode\n\ndata = bytes(range(32))\nencoded = encode(data, method=\"eq64\")\ndecoded = decode(encoded)\nassert data == decoded\n</code></pre></p>"},{"location":"api/#encoding-methods","title":"Encoding Methods","text":""},{"location":"api/#eq64-full-embedding-encoder","title":"Eq64 - Full Embedding Encoder","text":"<p>Encodes full embeddings with position-safe QuadB64.</p> <p>Method: <code>\"eq64\"</code></p> <p>Characteristics: - Lossless encoding/decoding - 2 characters per byte - No dots in native version (dots were in Python prototype)</p>"},{"location":"api/#shq64-simhash-encoder","title":"Shq64 - SimHash Encoder","text":"<p>Generates locality-sensitive hash using random projections.</p> <p>Method: <code>\"shq64\"</code></p> <p>Parameters: - <code>planes</code> (int, default=64): Number of random hyperplanes</p> <p>Characteristics: - Lossy compression to 64 bits - Preserves cosine similarity - Fixed output size (16 characters for 64 planes)</p>"},{"location":"api/#t8q64-top-k-indices-encoder","title":"T8q64 - Top-k Indices Encoder","text":"<p>Encodes the indices of the k highest values.</p> <p>Method: <code>\"t8q64\"</code></p> <p>Parameters: - <code>k</code> (int, default=8): Number of top indices to keep</p> <p>Characteristics: - Sparse representation - Captures most important features - Fixed output size (2k characters)</p>"},{"location":"api/#zoq64-z-order-encoder","title":"Zoq64 - Z-order Encoder","text":"<p>Encodes using Z-order (Morton) curve for spatial locality.</p> <p>Method: <code>\"zoq64\"</code></p> <p>Characteristics: - Spatial locality preservation - Nearby points share prefixes - Fixed output size (8 characters)</p>"},{"location":"api/#native-acceleration","title":"Native Acceleration","text":""},{"location":"api/#is_native_available","title":"<code>is_native_available()</code>","text":"<p>Check if native Rust acceleration is available.</p> <p>Returns: - <code>bool</code>: True if native module is loaded</p> <p>Example: <pre><code>from uubed.native_wrapper import is_native_available\n\nif is_native_available():\n    print(\"Using Rust acceleration!\")\nelse:\n    print(\"Using pure Python implementation\")\n</code></pre></p>"},{"location":"api/#position-safe-alphabets","title":"Position-Safe Alphabets","text":"<p>QuadB64 uses different alphabets for different character positions:</p> <pre><code>ALPHABETS = [\n    \"ABCDEFGHIJKLMNOP\",  # positions 0, 4, 8, ...\n    \"QRSTUVWXYZabcdef\",  # positions 1, 5, 9, ...\n    \"ghijklmnopqrstuv\",  # positions 2, 6, 10, ...\n    \"wxyz0123456789-_\",  # positions 3, 7, 11, ...\n]\n</code></pre> <p>This ensures that a substring like \"abc\" can only match at specific positions, eliminating false positives in search engines.</p>"},{"location":"api/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use native module: Install from source for 40-100x speedup</li> <li>Batch operations: Process multiple embeddings together</li> <li>Choose appropriate method:</li> <li><code>eq64</code>: When you need full precision</li> <li><code>shq64</code>: For fast similarity comparison</li> <li><code>t8q64</code>: For sparse representations</li> <li><code>zoq64</code>: For spatial/prefix searches</li> </ol>"},{"location":"api/#error-handling","title":"Error Handling","text":"<pre><code>from uubed import encode, decode\n\ntry:\n    # Invalid input\n    encode([256, 300], method=\"eq64\")  # Values must be 0-255\nexcept ValueError as e:\n    print(f\"Encoding error: {e}\")\n\ntry:\n    # Invalid decode\n    decode(\"invalid_string\")\nexcept ValueError as e:\n    print(f\"Decoding error: {e}\")\n</code></pre>"},{"location":"basic-usage/","title":"Basic Usage Guide","text":""},{"location":"basic-usage/#getting-started-with-uubed","title":"Getting Started with uubed","text":"<p>This guide covers the essential patterns and common operations you'll use with the uubed library. After reading this, you'll understand how to effectively use QuadB64 encoding in your applications.</p>"},{"location":"basic-usage/#core-concepts","title":"Core Concepts","text":""},{"location":"basic-usage/#import-patterns","title":"Import Patterns","text":"<pre><code># Basic imports\nfrom uubed import encode_eq64, decode_eq64\nfrom uubed import encode_shq64, encode_t8q64, encode_zoq64\n\n# Advanced imports\nfrom uubed import encode, decode, Config\nfrom uubed import has_native_extensions, benchmark\n</code></pre>"},{"location":"basic-usage/#the-unified-api","title":"The Unified API","text":"<p>The <code>encode()</code> and <code>decode()</code> functions provide a unified interface:</p> <pre><code>from uubed import encode, decode\n\n# Specify encoding method\ndata = b\"Hello, world!\"\nencoded = encode(data, method=\"eq64\")\ndecoded = decode(encoded)\n\n# Or use variant-specific functions\nencoded = encode_eq64(data)  # Same result\ndecoded = decode_eq64(encoded)  # Same result\n</code></pre>"},{"location":"basic-usage/#working-with-different-data-types","title":"Working with Different Data Types","text":""},{"location":"basic-usage/#text-data","title":"Text Data","text":"<pre><code>from uubed import encode_eq64, decode_eq64\n\n# String to bytes conversion\ntext = \"QuadB64 prevents substring pollution!\"\ndata = text.encode('utf-8')\n\n# Encode and decode\nencoded = encode_eq64(data)\ndecoded = decode_eq64(encoded)\nrecovered_text = decoded.decode('utf-8')\n\nassert text == recovered_text\nprint(f\"Original: {text}\")\nprint(f\"Encoded:  {encoded}\")\nprint(f\"Recovered: {recovered_text}\")\n</code></pre>"},{"location":"basic-usage/#binary-files","title":"Binary Files","text":"<pre><code># Read binary file\nwith open(\"image.jpg\", \"rb\") as f:\n    image_data = f.read()\n\n# Encode for storage in text-based systems\nencoded = encode_eq64(image_data)\n\n# Store in database, JSON, etc.\ndocument = {\n    \"id\": \"img_001\",\n    \"filename\": \"image.jpg\",\n    \"size\": len(image_data),\n    \"data\": encoded  # Safe for text storage\n}\n\n# Later: retrieve and decode\ndecoded_data = decode_eq64(document[\"data\"])\nassert decoded_data == image_data\n</code></pre>"},{"location":"basic-usage/#numpy-arrays","title":"NumPy Arrays","text":"<pre><code>import numpy as np\nfrom uubed import encode_eq64, decode_eq64\n\n# Create array\narr = np.random.rand(100, 50).astype(np.float32)\n\n# Encode array\nencoded = encode_eq64(arr.tobytes())\n\n# Decode and reconstruct\ndecoded_bytes = decode_eq64(encoded)\nreconstructed = np.frombuffer(decoded_bytes, dtype=np.float32)\nreconstructed = reconstructed.reshape(100, 50)\n\nassert np.array_equal(arr, reconstructed)\n</code></pre>"},{"location":"basic-usage/#ml-embeddings","title":"ML Embeddings","text":"<pre><code># Typical ML workflow\nfrom sentence_transformers import SentenceTransformer\nfrom uubed import encode_shq64, encode_eq64\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings\ntexts = [\n    \"Machine learning is fascinating\",\n    \"Deep learning uses neural networks\",\n    \"I love pizza\"\n]\n\nembeddings = model.encode(texts)\n\n# Full precision encoding (reversible)\nfull_codes = [encode_eq64(emb.tobytes()) for emb in embeddings]\n\n# Compact similarity hashes (irreversible but fast comparison)\nhash_codes = [encode_shq64(emb.tobytes()) for emb in embeddings]\n\nprint(\"Full codes (first 30 chars):\")\nfor i, code in enumerate(full_codes):\n    print(f\"  {i}: {code[:30]}...\")\n\nprint(\"\\nCompact hashes:\")\nfor i, code in enumerate(hash_codes):\n    print(f\"  {i}: {code}\")\n</code></pre>"},{"location":"basic-usage/#configuration-and-performance","title":"Configuration and Performance","text":""},{"location":"basic-usage/#check-native-extensions","title":"Check Native Extensions","text":"<pre><code>from uubed import has_native_extensions, get_implementation_info\n\n# Check if native extensions are available\nif has_native_extensions():\n    print(\"\ud83d\ude80 Native acceleration enabled!\")\nelse:\n    print(\"\u26a0\ufe0f  Using pure Python implementation\")\n    print(\"Install native extensions: pip install uubed[native]\")\n\n# Get detailed implementation info\ninfo = get_implementation_info()\nprint(f\"Implementation: {info['implementation']}\")\nprint(f\"Version: {info['version']}\")\nprint(f\"Features: {info['features']}\")\n</code></pre>"},{"location":"basic-usage/#performance-configuration","title":"Performance Configuration","text":"<pre><code>from uubed import Config, encode_eq64\n\n# Create configuration\nconfig = Config(\n    use_native=True,          # Use native implementation if available\n    chunk_size=8192,          # Process in 8KB chunks\n    num_threads=4,            # Parallel processing threads\n    validate_input=True       # Validate input data\n)\n\n# Use configuration\nlarge_data = b\"x\" * 1000000  # 1MB of data\nencoded = encode_eq64(large_data, config=config)\n</code></pre>"},{"location":"basic-usage/#benchmarking","title":"Benchmarking","text":"<pre><code>from uubed import benchmark\n\n# Run performance benchmark\nresults = benchmark()\nprint(\"Performance Results:\")\nprint(f\"Eq64 encoding: {results['eq64_encode_mb_per_sec']:.1f} MB/s\")\nprint(f\"Shq64 hashing: {results['shq64_encode_mb_per_sec']:.1f} MB/s\")\nprint(f\"Native available: {results['native_available']}\")\n</code></pre>"},{"location":"basic-usage/#error-handling","title":"Error Handling","text":""},{"location":"basic-usage/#validation","title":"Validation","text":"<pre><code>from uubed import validate_eq64, ValidationError\n\nencoded = \"SGVs.bG8s.IFFV.YWRC.NjQh\"\n\n# Basic validation\nif validate_eq64(encoded):\n    decoded = decode_eq64(encoded)\nelse:\n    print(\"Invalid encoding\")\n\n# Detailed validation\ntry:\n    validation = validate_eq64(encoded, detailed=True)\n    if not validation['valid']:\n        print(f\"Validation failed: {validation['errors']}\")\nexcept ValidationError as e:\n    print(f\"Validation error: {e}\")\n</code></pre>"},{"location":"basic-usage/#exception-handling","title":"Exception Handling","text":"<pre><code>from uubed import DecodingError, EncodingError\n\ntry:\n    # This will fail - invalid encoding\n    decoded = decode_eq64(\"invalid.encoding.here\")\nexcept DecodingError as e:\n    print(f\"Decoding failed: {e}\")\n\ntry:\n    # This might fail - very large input\n    huge_data = b\"x\" * (1024 * 1024 * 1024)  # 1GB\n    encoded = encode_eq64(huge_data)\nexcept EncodingError as e:\n    print(f\"Encoding failed: {e}\")\n</code></pre>"},{"location":"basic-usage/#batch-processing","title":"Batch Processing","text":""},{"location":"basic-usage/#encoding-multiple-items","title":"Encoding Multiple Items","text":"<pre><code>from uubed import encode_batch\nfrom concurrent.futures import ProcessPoolExecutor\n\n# Prepare data\ndocuments = [\"doc1\", \"doc2\", \"doc3\"] * 1000\nembeddings = [model.encode(doc) for doc in documents]\n\n# Method 1: Built-in batch encoding\nencoded_batch = encode_batch(\n    [emb.tobytes() for emb in embeddings],\n    method=\"eq64\",\n    num_workers=4\n)\n\n# Method 2: Manual parallel processing\ndef encode_chunk(chunk):\n    return [encode_eq64(emb.tobytes()) for emb in chunk]\n\nchunk_size = 100\nchunks = [embeddings[i:i+chunk_size] \n          for i in range(0, len(embeddings), chunk_size)]\n\nwith ProcessPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(encode_chunk, chunks))\n    all_encoded = [item for chunk in results for item in chunk]\n</code></pre>"},{"location":"basic-usage/#streaming-processing","title":"Streaming Processing","text":"<pre><code>from uubed import StreamEncoder\n\n# For very large files or continuous data\nencoder = StreamEncoder(\"eq64\")\n\ndef process_large_file(input_path, output_path):\n    with open(input_path, \"rb\") as input_file:\n        with open(output_path, \"w\") as output_file:\n            while True:\n                chunk = input_file.read(4096)  # 4KB chunks\n                if not chunk:\n                    break\n\n                encoded_chunk = encoder.encode_chunk(chunk)\n                output_file.write(encoded_chunk + \"\\n\")\n\n            # Write any remaining data\n            final_chunk = encoder.finalize()\n            if final_chunk:\n                output_file.write(final_chunk + \"\\n\")\n</code></pre>"},{"location":"basic-usage/#common-patterns","title":"Common Patterns","text":""},{"location":"basic-usage/#data-pipeline-integration","title":"Data Pipeline Integration","text":"<pre><code># ETL pipeline with QuadB64\nclass DataPipeline:\n    def __init__(self, variant=\"eq64\"):\n        self.variant = variant\n\n    def extract(self, source):\n        \"\"\"Extract data from source\"\"\"\n        # Your extraction logic\n        return raw_data\n\n    def transform(self, data):\n        \"\"\"Transform and encode data\"\"\"\n        processed = self.process_data(data)\n\n        if self.variant == \"eq64\":\n            return encode_eq64(processed)\n        elif self.variant == \"shq64\":\n            return encode_shq64(processed)\n        else:\n            return encode(processed, method=self.variant)\n\n    def load(self, encoded_data, destination):\n        \"\"\"Load encoded data to destination\"\"\"\n        # Store in database, search engine, etc.\n        destination.store(encoded_data)\n\n# Usage\npipeline = DataPipeline(\"shq64\")\nresult = pipeline.transform(input_data)\n</code></pre>"},{"location":"basic-usage/#database-integration","title":"Database Integration","text":"<pre><code>import sqlite3\nfrom uubed import encode_eq64, decode_eq64\n\n# Setup database\nconn = sqlite3.connect('embeddings.db')\ncursor = conn.cursor()\n\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS vectors (\n    id INTEGER PRIMARY KEY,\n    content TEXT,\n    embedding_eq64 TEXT NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n''')\n\n# Insert with encoding\ndef store_embedding(content: str, embedding: np.ndarray):\n    encoded = encode_eq64(embedding.tobytes())\n    cursor.execute(\n        \"INSERT INTO vectors (content, embedding_eq64) VALUES (?, ?)\",\n        (content, encoded)\n    )\n    conn.commit()\n\n# Retrieve with decoding\ndef get_embedding(vector_id: int) -&gt; np.ndarray:\n    cursor.execute(\n        \"SELECT embedding_eq64 FROM vectors WHERE id = ?\",\n        (vector_id,)\n    )\n    encoded = cursor.fetchone()[0]\n    decoded_bytes = decode_eq64(encoded)\n    return np.frombuffer(decoded_bytes, dtype=np.float32)\n</code></pre>"},{"location":"basic-usage/#web-api-integration","title":"Web API Integration","text":"<pre><code>from flask import Flask, request, jsonify\nfrom uubed import encode_eq64, decode_eq64\n\napp = Flask(__name__)\n\n@app.route('/encode', methods=['POST'])\ndef encode_endpoint():\n    try:\n        # Get binary data from request\n        data = request.get_data()\n\n        # Encode\n        encoded = encode_eq64(data)\n\n        return jsonify({\n            'encoded': encoded,\n            'original_size': len(data),\n            'encoded_size': len(encoded)\n        })\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 400\n\n@app.route('/decode', methods=['POST'])\ndef decode_endpoint():\n    try:\n        # Get encoded string from request\n        data = request.json\n        encoded = data['encoded']\n\n        # Decode\n        decoded = decode_eq64(encoded)\n\n        # Return as base64 for JSON compatibility\n        import base64\n        return jsonify({\n            'decoded': base64.b64encode(decoded).decode(),\n            'size': len(decoded)\n        })\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 400\n</code></pre>"},{"location":"basic-usage/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"basic-usage/#dos","title":"Do's \u2705","text":"<ol> <li>Choose the right variant: </li> <li>Eq64 for lossless encoding</li> <li>Shq64 for similarity comparison</li> <li>T8q64 for sparse data</li> <li> <p>Zoq64 for spatial data</p> </li> <li> <p>Use native extensions: Install with <code>pip install uubed[native]</code></p> </li> <li> <p>Validate untrusted input: Use <code>validate_*()</code> functions</p> </li> <li> <p>Handle errors gracefully: Wrap in try-catch blocks</p> </li> <li> <p>Batch when possible: Better performance for multiple items</p> </li> </ol>"},{"location":"basic-usage/#donts","title":"Don'ts \u274c","text":"<ol> <li> <p>Don't modify encoded strings: They become invalid</p> </li> <li> <p>Don't mix variants: Each has specific use cases</p> </li> <li> <p>Don't ignore performance: Check for native extensions</p> </li> <li> <p>Don't store without validation: Validate critical encoded data</p> </li> <li> <p>Don't assume reversibility: Only Eq64 is reversible</p> </li> </ol>"},{"location":"basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Advanced Features</li> <li>Learn about Integration Patterns</li> <li>Read Performance Tuning</li> <li>Check out Real-world Examples</li> </ul>"},{"location":"installation/","title":"Installation Guide","text":""},{"location":"installation/#system-requirements","title":"System Requirements","text":""},{"location":"installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>pip package manager</li> <li>10 MB of disk space</li> </ul>"},{"location":"installation/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>Python 3.10+ for optimal performance</li> <li>C++ compiler for building native extensions (optional)</li> <li>64-bit operating system</li> </ul>"},{"location":"installation/#supported-platforms","title":"Supported Platforms","text":"<ul> <li>Linux: Ubuntu 20.04+, Debian 10+, RHEL 8+, and compatible distributions</li> <li>macOS: 10.15 (Catalina) or later</li> <li>Windows: Windows 10 or later (64-bit)</li> </ul>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#method-1-install-from-pypi-recommended","title":"Method 1: Install from PyPI (Recommended)","text":"<p>The simplest way to install uubed is using pip:</p> <pre><code>pip install uubed\n</code></pre> <p>To install with all optional dependencies:</p> <pre><code>pip install uubed[all]\n</code></pre>"},{"location":"installation/#method-2-install-from-source","title":"Method 2: Install from Source","text":"<p>For the latest development version or to contribute:</p> <pre><code># Clone the repository\ngit clone https://github.com/twardoch/uubed.git\ncd uubed\n\n# Install in development mode\npip install -e .\n\n# Or install with development dependencies\npip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#method-3-using-poetry","title":"Method 3: Using Poetry","text":"<p>If you prefer Poetry for dependency management:</p> <pre><code># Clone the repository\ngit clone https://github.com/twardoch/uubed.git\ncd uubed\n\n# Install using Poetry\npoetry install\n\n# Activate the virtual environment\npoetry shell\n</code></pre>"},{"location":"installation/#method-4-using-conda","title":"Method 4: Using Conda","text":"<p>For Conda users:</p> <pre><code># Using conda-forge channel\nconda install -c conda-forge uubed\n\n# Or using pip within conda environment\nconda create -n uubed-env python=3.10\nconda activate uubed-env\npip install uubed\n</code></pre>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"installation/#performance-extensions","title":"Performance Extensions","text":"<p>For maximum performance, install the native extensions:</p> <pre><code>pip install uubed[native]\n</code></pre> <p>This requires a C++ compiler: - Linux: <code>sudo apt-get install build-essential</code> (Ubuntu/Debian) or <code>sudo yum groupinstall \"Development Tools\"</code> (RHEL/CentOS) - macOS: Install Xcode Command Line Tools: <code>xcode-select --install</code> - Windows: Install Visual Studio Build Tools or MinGW-w64</p>"},{"location":"installation/#machine-learning-integration","title":"Machine Learning Integration","text":"<p>For ML/AI integrations:</p> <pre><code>pip install uubed[ml]\n</code></pre> <p>This includes: - NumPy for array operations - Support for common embedding formats - Optimized vectorized operations</p>"},{"location":"installation/#development-dependencies","title":"Development Dependencies","text":"<p>For contributing to uubed:</p> <pre><code>pip install uubed[dev]\n</code></pre> <p>Includes: - pytest for testing - black for code formatting - mypy for type checking - sphinx for documentation</p>"},{"location":"installation/#verification","title":"Verification","text":""},{"location":"installation/#basic-verification","title":"Basic Verification","text":"<p>Verify your installation:</p> <pre><code>python -c \"import uubed; print(uubed.__version__)\"\n</code></pre>"},{"location":"installation/#run-tests","title":"Run Tests","text":"<p>To ensure everything is working correctly:</p> <pre><code># Install test dependencies\npip install uubed[test]\n\n# Run the test suite\npython -m pytest --pyargs uubed\n</code></pre>"},{"location":"installation/#performance-check","title":"Performance Check","text":"<p>Check if native extensions are available:</p> <pre><code>import uubed\n\n# Check for native acceleration\nif uubed.has_native_extensions():\n    print(\"Native extensions are available!\")\nelse:\n    print(\"Using pure Python implementation\")\n\n# Run a benchmark\nuubed.benchmark()\n</code></pre>"},{"location":"installation/#configuration","title":"Configuration","text":""},{"location":"installation/#environment-variables","title":"Environment Variables","text":"<p>Configure uubed behavior using environment variables:</p> <pre><code># Set default encoding variant\nexport UUBED_DEFAULT_VARIANT=\"eq64\"\n\n# Enable debug logging\nexport UUBED_DEBUG=\"1\"\n\n# Set performance mode\nexport UUBED_PERFORMANCE_MODE=\"aggressive\"\n</code></pre>"},{"location":"installation/#configuration-file","title":"Configuration File","text":"<p>Create a configuration file at <code>~/.uubed/config.json</code>:</p> <pre><code>{\n  \"default_variant\": \"eq64\",\n  \"performance\": {\n    \"use_native\": true,\n    \"chunk_size\": 8192,\n    \"parallel_threshold\": 1048576\n  },\n  \"logging\": {\n    \"level\": \"INFO\",\n    \"format\": \"simple\"\n  }\n}\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#common-issues","title":"Common Issues","text":""},{"location":"installation/#importerror-no-module-named-uubed","title":"ImportError: No module named 'uubed'","text":"<p>Solution: Ensure pip installation completed successfully: <pre><code>pip install --upgrade pip\npip install uubed --force-reinstall\n</code></pre></p>"},{"location":"installation/#native-extensions-not-building","title":"Native extensions not building","text":"<p>Solution: Install development tools: <pre><code># Ubuntu/Debian\nsudo apt-get update\nsudo apt-get install python3-dev build-essential\n\n# macOS\nxcode-select --install\n\n# Windows\n# Install Visual Studio Build Tools from Microsoft\n</code></pre></p>"},{"location":"installation/#performance-issues","title":"Performance issues","text":"<p>Solution: Check if native extensions are loaded: <pre><code>import uubed\nprint(uubed.get_implementation_info())\n</code></pre></p>"},{"location":"installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the FAQ</li> <li>Search existing issues</li> <li>Join our Discord community</li> <li>Open a new issue</li> </ol>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>Now that you have uubed installed:</p> <ol> <li>Follow the Quick Start Guide to learn basic usage</li> <li>Read about QuadB64 Fundamentals to understand the theory</li> <li>Explore the API Reference for detailed documentation</li> <li>Check out Examples for real-world usage</li> </ol>"},{"location":"installation/#upgrading","title":"Upgrading","text":""},{"location":"installation/#upgrade-to-latest-version","title":"Upgrade to Latest Version","text":"<pre><code>pip install --upgrade uubed\n</code></pre>"},{"location":"installation/#check-for-updates","title":"Check for Updates","text":"<pre><code>import uubed\nuubed.check_for_updates()\n</code></pre>"},{"location":"installation/#migration-between-versions","title":"Migration Between Versions","text":"<p>When upgrading between major versions, check the Migration Guide for breaking changes and update instructions.</p>"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>Get up and running with QuadB64 in minutes! This guide covers installation, basic usage, and common integration patterns.</p>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>The simplest way to install uubed:</p> <pre><code>pip install uubed\n</code></pre> <p>For maximum performance with native extensions:</p> <pre><code>pip install uubed[native]\n</code></pre> <p>For development or latest features:</p> <pre><code>git clone https://github.com/twardoch/uubed.git\ncd uubed\npip install -e \".[dev]\"\n</code></pre> <p>See the Installation Guide for detailed instructions and troubleshooting.</p>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"quickstart/#simple-text-encoding","title":"Simple Text Encoding","text":"<pre><code>from uubed import encode_eq64, decode_eq64\n\n# Encode any binary data\ndata = b\"Hello, QuadB64 World!\"\nencoded = encode_eq64(data)\nprint(f\"Encoded: {encoded}\")\n# Output: SGVs.bG8s.IFFV.YWRC.NjQg.V29y.bGQh\n\n# Decode back to original\ndecoded = decode_eq64(encoded)\nassert decoded == data\nprint(f\"Decoded: {decoded}\")\n# Output: b'Hello, QuadB64 World!'\n</code></pre>"},{"location":"quickstart/#working-with-embeddings","title":"Working with Embeddings","text":"<pre><code>import numpy as np\nfrom uubed import encode, decode\n\n# Create a sample embedding (e.g., from an ML model)\nembedding = np.random.rand(768).astype(np.float32)\n\n# Convert to bytes\nembedding_bytes = embedding.tobytes()\n\n# Full precision encoding with Eq64\nfull_code = encode(embedding_bytes, method=\"eq64\")\nprint(f\"Full encoding length: {len(full_code)} chars\")\n\n# Compact similarity hash with Shq64\ncompact_code = encode(embedding_bytes, method=\"shq64\")\nprint(f\"Compact hash: {compact_code}\")  # 16 characters\n\n# Decode back (only works for eq64)\ndecoded_bytes = decode(full_code)\ndecoded_embedding = np.frombuffer(decoded_bytes, dtype=np.float32)\nassert np.allclose(embedding, decoded_embedding)\n</code></pre>"},{"location":"quickstart/#why-quadb64","title":"Why QuadB64?","text":""},{"location":"quickstart/#the-problem-with-traditional-base64","title":"The Problem with Traditional Base64","text":"<p>When search engines index Base64-encoded data, they treat it as regular text:</p> <pre><code># Two completely different embeddings\nembedding1 = model.encode(\"cats are cute\")\nembedding2 = model.encode(\"quantum physics\")\n\n# Traditional Base64 encoding\nimport base64\nb64_1 = base64.b64encode(embedding1.tobytes()).decode()\nb64_2 = base64.b64encode(embedding2.tobytes()).decode()\n\n# Substring pollution: random matches!\n# \"YWJj\" might appear in both encodings by chance\n# Search engines will falsely match these unrelated documents\n</code></pre>"},{"location":"quickstart/#the-quadb64-solution","title":"The QuadB64 Solution","text":"<p>QuadB64 uses position-dependent encoding to prevent false matches:</p> <pre><code># QuadB64 encoding\nfrom uubed import encode_eq64\nq64_1 = encode_eq64(embedding1.tobytes())\nq64_2 = encode_eq64(embedding2.tobytes())\n\n# Position-safe: \"YWJj\" at position 0 \u2260 \"YWJj\" at position 4\n# No false substring matches between unrelated documents!\n</code></pre> <p>Key benefits: - \u2705 No substring pollution: Position-dependent alphabets - \u2705 Search accuracy: Only genuine matches are found - \u2705 Easy integration: Drop-in replacement for Base64 - \u2705 High performance: Minimal overhead vs Base64</p>"},{"location":"quickstart/#encoding-methods","title":"Encoding Methods","text":"<p>uubed provides multiple encoding schemes optimized for different use cases:</p>"},{"location":"quickstart/#eq64-full-embeddings","title":"Eq64 - Full Embeddings","text":"<p>Perfect for when you need lossless encoding:</p> <pre><code>from uubed import encode_eq64, decode_eq64\n\ndata = b\"Your binary data here\"\nencoded = encode_eq64(data)  # Position-safe, dots every 4 chars\ndecoded = decode_eq64(encoded)  # Get original data back\n</code></pre> <ul> <li>Size: ~1.33x original (same as Base64)</li> <li>Use cases: Full embeddings, binary files, any lossless encoding</li> <li>Features: Complete reversibility, position safety</li> </ul>"},{"location":"quickstart/#shq64-simhash-variant","title":"Shq64 - SimHash Variant","text":"<p>Compact similarity-preserving hashes:</p> <pre><code>from uubed import encode_shq64\n\n# 768-dimensional embedding\nembedding = model.encode(\"sample text\")\nhash_code = encode_shq64(embedding.tobytes())\nprint(hash_code)  # 16-character hash like \"QRsT.UvWx.YZab.cdef\"\n</code></pre> <ul> <li>Size: Always 16 characters (64-bit hash)</li> <li>Use cases: Deduplication, similarity search, clustering</li> <li>Features: Preserves cosine similarity, extremely compact</li> </ul>"},{"location":"quickstart/#t8q64-top-k-indices","title":"T8q64 - Top-k Indices","text":"<p>Sparse representation capturing most important features:</p> <pre><code>from uubed import encode_t8q64\n\n# Encode top-8 most significant indices\nsparse_code = encode_t8q64(embedding.tobytes(), k=8)\n</code></pre> <ul> <li>Size: 16 characters (8 indices + magnitudes)</li> <li>Use cases: Sparse embeddings, feature selection</li> <li>Features: Captures most informative dimensions</li> </ul>"},{"location":"quickstart/#zoq64-z-order-curve","title":"Zoq64 - Z-order Curve","text":"<p>Spatial locality-preserving encoding:</p> <pre><code>from uubed import encode_zoq64\n\n# 2D or higher dimensional data\nspatial_code = encode_zoq64(coordinates)\n</code></pre> <ul> <li>Size: Variable (based on precision needs)</li> <li>Use cases: Geospatial data, multi-dimensional indexing</li> <li>Features: Nearby points have similar prefixes</li> </ul>"},{"location":"quickstart/#performance","title":"Performance","text":"<p>QuadB64 is designed for production workloads:</p> Operation Pure Python With Native Extensions Speedup Eq64 encoding 5.5 MB/s 230+ MB/s 40-105x Shq64 hashing 12 MB/s 117 MB/s 9.7x T8q64 sparse 8 MB/s 156 MB/s 19.5x Zoq64 spatial 0.3 MB/s 480 MB/s 1600x <p>Check if native extensions are available:</p> <pre><code>from uubed import has_native_extensions\n\nif has_native_extensions():\n    print(\"\ud83d\ude80 Native acceleration enabled!\")\nelse:\n    print(\"Using pure Python implementation\")\n</code></pre>"},{"location":"quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"quickstart/#batch-processing","title":"Batch Processing","text":"<pre><code>from uubed import encode_batch\n\n# Process multiple embeddings efficiently\nembeddings = [model.encode(text) for text in documents]\nencoded_batch = encode_batch(embeddings, method=\"shq64\")\n\n# Parallel processing for large datasets\nfrom concurrent.futures import ProcessPoolExecutor\n\ndef process_chunk(chunk):\n    return [encode_eq64(emb.tobytes()) for emb in chunk]\n\nwith ProcessPoolExecutor() as executor:\n    chunks = [embeddings[i:i+100] for i in range(0, len(embeddings), 100)]\n    results = list(executor.map(process_chunk, chunks))\n</code></pre>"},{"location":"quickstart/#configuration-options","title":"Configuration Options","text":"<pre><code>from uubed import Config\n\n# Custom configuration\nconfig = Config(\n    default_variant=\"eq64\",\n    use_native=True,\n    chunk_size=8192,\n    num_threads=4\n)\n\n# Apply configuration\nencoded = encode(data, config=config)\n</code></pre>"},{"location":"quickstart/#real-world-integration","title":"Real-World Integration","text":""},{"location":"quickstart/#vector-databases-pinecone-weaviate-qdrant","title":"Vector Databases (Pinecone, Weaviate, Qdrant)","text":"<pre><code>from uubed import encode_shq64\nimport pinecone\n\n# Initialize your vector database\nindex = pinecone.Index(\"my-index\")\n\n# Store embeddings with QuadB64 codes\nfor doc_id, text in documents.items():\n    embedding = model.encode(text)\n    q64_code = encode_shq64(embedding.tobytes())\n\n    index.upsert(\n        vectors=[(doc_id, embedding.tolist())],\n        metadata={doc_id: {\"text\": text, \"q64_code\": q64_code}}\n    )\n\n# Similarity search without substring pollution\nquery_embedding = model.encode(query_text)\nquery_code = encode_shq64(query_embedding.tobytes())\n\n# Find exact code matches (no false positives!)\nresults = index.query(\n    vector=query_embedding.tolist(),\n    filter={\"q64_code\": {\"$eq\": query_code}},\n    top_k=10\n)\n</code></pre>"},{"location":"quickstart/#elasticsearch-opensearch","title":"Elasticsearch / OpenSearch","text":"<pre><code>from elasticsearch import Elasticsearch\nfrom uubed import encode_eq64\n\nes = Elasticsearch()\n\n# Index documents with position-safe encoding\ndoc = {\n    \"title\": \"Introduction to QuadB64\",\n    \"content\": \"QuadB64 solves substring pollution...\",\n    \"embedding\": embedding.tolist(),\n    \"embedding_q64\": encode_eq64(embedding.tobytes())\n}\n\nes.index(index=\"docs\", id=\"doc1\", body=doc)\n\n# Search with exact matching on encoded field\nquery = {\n    \"query\": {\n        \"bool\": {\n            \"must\": [\n                {\"match\": {\"content\": \"QuadB64\"}},\n                {\"term\": {\"embedding_q64.keyword\": target_code}}\n            ]\n        }\n    }\n}\n\nresults = es.search(index=\"docs\", body=query)\n</code></pre>"},{"location":"quickstart/#langchain-integration","title":"LangChain Integration","text":"<pre><code>from langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom uubed import encode_shq64\n\n# Custom embeddings wrapper\nclass QuadB64Embeddings(OpenAIEmbeddings):\n    def embed_documents(self, texts):\n        embeddings = super().embed_documents(texts)\n        # Add QuadB64 codes to metadata\n        return [(emb, {\"q64\": encode_shq64(np.array(emb).tobytes())}) \n                for emb in embeddings]\n\n# Use with vector store\nembeddings = QuadB64Embeddings()\nvectorstore = Chroma.from_documents(\n    documents=docs,\n    embedding=embeddings\n)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udcd6 Read the Theory behind QuadB64</li> <li>\ud83d\udd27 Explore the API Reference for detailed documentation</li> <li>\ud83d\ude80 Check out Performance Benchmarks</li> <li>\ud83d\udca1 See more Examples</li> </ul>"},{"location":"quickstart/#need-help","title":"Need Help?","text":"<ul> <li>\ud83d\udcac Join our Discord Community</li> <li>\ud83d\udc1b Report issues on GitHub</li> <li>\ud83d\udce7 Contact: support@uubed.io</li> </ul>"},{"location":"advanced/research/","title":"Research Directions and Future Work","text":""},{"location":"advanced/research/#overview","title":"Overview","text":"<p>This chapter explores cutting-edge research directions for QuadB64, including integration with emerging technologies like Matryoshka embeddings, quantum computing applications, and next-generation encoding techniques. These concepts represent the frontier of position-safe encoding research.</p>"},{"location":"advanced/research/#matryoshka-embeddings-integration","title":"Matryoshka Embeddings Integration","text":""},{"location":"advanced/research/#hierarchical-encoding-strategies","title":"Hierarchical Encoding Strategies","text":"<p>Matryoshka embeddings enable multiple resolutions of information to coexist within a single vector. QuadB64 can be extended to preserve this hierarchical structure during encoding and storage.</p> <pre><code>import numpy as np\nfrom typing import List, Tuple, Dict\nimport uubed\n\nclass MatryoshkaQuadB64Encoder:\n    \"\"\"QuadB64 encoder with Matryoshka embedding support\"\"\"\n\n    def __init__(self, dimensions: List[int]):\n        \"\"\"\n        Initialize with Matryoshka dimensions\n\n        Args:\n            dimensions: List of nested dimensions, e.g., [2048, 1024, 512, 256]\n        \"\"\"\n        self.dimensions = sorted(dimensions, reverse=True)\n        self.max_dim = self.dimensions[0]\n        self.resolution_markers = self._compute_resolution_markers()\n\n    def _compute_resolution_markers(self) -&gt; Dict[int, str]:\n        \"\"\"Compute unique markers for each resolution level\"\"\"\n        markers = {}\n\n        for i, dim in enumerate(self.dimensions):\n            # Generate position-safe marker for this resolution\n            marker_data = f\"MATRYOSHKA_DIM_{dim}_LEVEL_{i}\".encode('utf-8')\n            marker = uubed.encode_eq64(marker_data, position=i * 1000)\n            markers[dim] = marker\n\n        return markers\n\n    def encode_matryoshka_embedding(self, \n                                   embedding: np.ndarray, \n                                   metadata: Dict = None) -&gt; str:\n        \"\"\"\n        Encode Matryoshka embedding with hierarchical structure preservation\n\n        Args:\n            embedding: Full-resolution embedding vector\n            metadata: Optional metadata about the embedding\n\n        Returns:\n            Hierarchically encoded string with resolution markers\n        \"\"\"\n        if len(embedding) != self.max_dim:\n            raise ValueError(f\"Embedding must have {self.max_dim} dimensions\")\n\n        encoded_parts = []\n        cumulative_position = 0\n\n        # Encode header with metadata\n        header = self._encode_header(metadata, cumulative_position)\n        encoded_parts.append(header)\n        cumulative_position += len(header.encode('utf-8'))\n\n        # Encode each resolution level\n        for dim in self.dimensions:\n            # Extract this resolution level\n            resolution_embedding = embedding[:dim]\n\n            # Add resolution marker\n            marker = self.resolution_markers[dim]\n            encoded_parts.append(marker)\n            cumulative_position += len(marker.encode('utf-8'))\n\n            # Encode the embedding data with position context\n            embedding_bytes = resolution_embedding.astype(np.float32).tobytes()\n            encoded_embedding = uubed.encode_shq64(embedding_bytes, position=cumulative_position)\n            encoded_parts.append(encoded_embedding)\n            cumulative_position += len(encoded_embedding.encode('utf-8'))\n\n            # Add dimension separator\n            separator = f\".DIM{dim}.\"\n            encoded_parts.append(separator)\n            cumulative_position += len(separator.encode('utf-8'))\n\n        return ''.join(encoded_parts)\n\n    def decode_matryoshka_embedding(self, \n                                   encoded_data: str, \n                                   target_dimension: int = None) -&gt; Tuple[np.ndarray, Dict]:\n        \"\"\"\n        Decode Matryoshka embedding at specified resolution\n\n        Args:\n            encoded_data: Hierarchically encoded embedding\n            target_dimension: Desired resolution (None for highest)\n\n        Returns:\n            Tuple of (embedding_array, metadata)\n        \"\"\"\n        target_dimension = target_dimension or self.max_dim\n\n        if target_dimension not in self.dimensions:\n            raise ValueError(f\"Target dimension {target_dimension} not available\")\n\n        # Parse encoded data\n        parts = self._parse_encoded_data(encoded_data)\n\n        # Extract metadata\n        metadata = self._decode_header(parts['header'])\n\n        # Find and decode target resolution\n        if target_dimension not in parts['resolutions']:\n            raise ValueError(f\"Resolution {target_dimension} not found in encoded data\")\n\n        encoded_embedding = parts['resolutions'][target_dimension]\n        position = parts['positions'][target_dimension]\n\n        # Decode embedding\n        embedding_bytes = uubed.decode_shq64(encoded_embedding, position=position)\n        embedding_array = np.frombuffer(embedding_bytes, dtype=np.float32)\n\n        return embedding_array, metadata\n\n    def _encode_header(self, metadata: Dict, position: int) -&gt; str:\n        \"\"\"Encode header with metadata\"\"\"\n        import json\n\n        header_data = {\n            'version': '1.0',\n            'encoder': 'MatryoshkaQuadB64',\n            'dimensions': self.dimensions,\n            'metadata': metadata or {}\n        }\n\n        header_json = json.dumps(header_data, separators=(',', ':'))\n        header_bytes = header_json.encode('utf-8')\n\n        return uubed.encode_eq64(header_bytes, position=position)\n\n    def _parse_encoded_data(self, encoded_data: str) -&gt; Dict:\n        \"\"\"Parse hierarchically encoded data into components\"\"\"\n\n        parts = {\n            'header': None,\n            'resolutions': {},\n            'positions': {}\n        }\n\n        # Simple parsing - in production would be more robust\n        sections = encoded_data.split('.DIM')\n\n        # First section is header\n        if sections:\n            parts['header'] = sections[0]\n\n        # Parse resolution sections\n        cumulative_position = len(sections[0].encode('utf-8'))\n\n        for section in sections[1:]:\n            if '.' in section:\n                dim_str, remaining = section.split('.', 1)\n                try:\n                    dimension = int(dim_str)\n\n                    # Find the encoded embedding (before next marker or end)\n                    # This is simplified - production parser would be more sophisticated\n                    marker = self.resolution_markers[dimension]\n                    if remaining.startswith(marker):\n                        encoded_part = remaining[len(marker):]\n\n                        parts['resolutions'][dimension] = encoded_part\n                        parts['positions'][dimension] = cumulative_position + len(marker.encode('utf-8'))\n\n                        cumulative_position += len(section.encode('utf-8')) + 4  # \".DIM\"\n\n                except ValueError:\n                    continue\n\n        return parts\n\n# Usage example\ndimensions = [2048, 1024, 512, 256]\nencoder = MatryoshkaQuadB64Encoder(dimensions)\n\n# Create a Matryoshka embedding\nfull_embedding = np.random.randn(2048).astype(np.float32)\n\n# Encode with hierarchical structure\nencoded = encoder.encode_matryoshka_embedding(\n    full_embedding,\n    metadata={'model': 'text-embedding-3-large', 'source': 'openai'}\n)\n\n# Decode at different resolutions\nembedding_256, metadata = encoder.decode_matryoshka_embedding(encoded, target_dimension=256)\nembedding_1024, _ = encoder.decode_matryoshka_embedding(encoded, target_dimension=1024)\n\nprint(f\"Original: {full_embedding.shape}\")\nprint(f\"256-dim: {embedding_256.shape}\")\nprint(f\"1024-dim: {embedding_1024.shape}\")\n</code></pre>"},{"location":"advanced/research/#adaptive-precision-techniques","title":"Adaptive Precision Techniques","text":"<p>QuadB64 can be extended to dynamically adjust encoding precision based on the information content at different Matryoshka levels:</p> <pre><code>class AdaptivePrecisionEncoder:\n    \"\"\"Encoder with adaptive precision based on information content\"\"\"\n\n    def __init__(self, base_precision: int = 32):\n        self.base_precision = base_precision\n        self.precision_analyzers = {\n            'variance': self._analyze_variance,\n            'entropy': self._analyze_entropy,\n            'gradient': self._analyze_gradient\n        }\n\n    def encode_adaptive_precision(self, \n                                 matryoshka_embedding: np.ndarray,\n                                 dimensions: List[int]) -&gt; str:\n        \"\"\"Encode with precision adapted to information content\"\"\"\n\n        encoded_levels = []\n\n        for dim in dimensions:\n            level_embedding = matryoshka_embedding[:dim]\n\n            # Analyze information content at this level\n            info_content = self._analyze_information_content(level_embedding)\n\n            # Determine optimal precision\n            precision = self._compute_optimal_precision(info_content)\n\n            # Encode with determined precision\n            quantized_embedding = self._quantize_embedding(level_embedding, precision)\n\n            level_encoded = uubed.encode_t8q64(\n                quantized_embedding.tobytes(),\n                position=dim  # Use dimension as position context\n            )\n\n            encoded_levels.append({\n                'dimension': dim,\n                'precision': precision,\n                'encoded': level_encoded,\n                'info_metrics': info_content\n            })\n\n        return self._pack_adaptive_encoding(encoded_levels)\n\n    def _analyze_information_content(self, embedding: np.ndarray) -&gt; Dict:\n        \"\"\"Analyze information content to determine encoding precision\"\"\"\n\n        return {\n            'variance': self._analyze_variance(embedding),\n            'entropy': self._analyze_entropy(embedding),\n            'gradient': self._analyze_gradient(embedding),\n            'sparsity': np.sum(np.abs(embedding) &lt; 1e-6) / len(embedding),\n            'dynamic_range': np.max(embedding) - np.min(embedding)\n        }\n\n    def _analyze_variance(self, embedding: np.ndarray) -&gt; float:\n        \"\"\"Analyze variance to determine information density\"\"\"\n        return float(np.var(embedding))\n\n    def _analyze_entropy(self, embedding: np.ndarray) -&gt; float:\n        \"\"\"Estimate entropy of embedding values\"\"\"\n        # Discretize for entropy calculation\n        bins = min(256, len(embedding) // 4)\n        hist, _ = np.histogram(embedding, bins=bins)\n        hist = hist[hist &gt; 0]  # Remove zero bins\n\n        if len(hist) == 0:\n            return 0.0\n\n        # Normalize and compute entropy\n        prob = hist / np.sum(hist)\n        entropy = -np.sum(prob * np.log2(prob))\n\n        return float(entropy)\n\n    def _analyze_gradient(self, embedding: np.ndarray) -&gt; float:\n        \"\"\"Analyze gradient magnitude to detect fine-grained information\"\"\"\n        if len(embedding) &lt; 2:\n            return 0.0\n\n        gradient = np.gradient(embedding)\n        return float(np.mean(np.abs(gradient)))\n\n    def _compute_optimal_precision(self, info_content: Dict) -&gt; int:\n        \"\"\"Compute optimal precision based on information content\"\"\"\n\n        # Normalize metrics to [0, 1] range\n        normalized_variance = min(1.0, info_content['variance'] / 10.0)\n        normalized_entropy = min(1.0, info_content['entropy'] / 8.0)\n        normalized_gradient = min(1.0, info_content['gradient'] / 1.0)\n\n        # Weight different factors\n        weights = {\n            'variance': 0.4,\n            'entropy': 0.4,\n            'gradient': 0.2\n        }\n\n        information_score = (\n            weights['variance'] * normalized_variance +\n            weights['entropy'] * normalized_entropy +\n            weights['gradient'] * normalized_gradient\n        )\n\n        # Map to precision range [8, 64] bits\n        min_precision, max_precision = 8, 64\n        precision = int(min_precision + information_score * (max_precision - min_precision))\n\n        # Ensure precision is power of 2 for efficiency\n        precision = 2 ** int(np.log2(precision))\n\n        return max(8, min(64, precision))\n\n    def _quantize_embedding(self, embedding: np.ndarray, precision: int) -&gt; np.ndarray:\n        \"\"\"Quantize embedding to specified precision\"\"\"\n\n        if precision &gt;= 32:\n            return embedding.astype(np.float32)\n        elif precision &gt;= 16:\n            return embedding.astype(np.float16)\n        else:\n            # Custom quantization for lower precision\n            min_val, max_val = np.min(embedding), np.max(embedding)\n            scale = (2 ** precision - 1) / (max_val - min_val) if max_val != min_val else 1.0\n\n            quantized = np.round((embedding - min_val) * scale).astype(np.int32)\n            quantized = np.clip(quantized, 0, 2 ** precision - 1)\n\n            # Store quantization parameters for reconstruction\n            return quantized.astype(np.uint8 if precision &lt;= 8 else np.uint16)\n</code></pre>"},{"location":"advanced/research/#multi-resolution-search","title":"Multi-Resolution Search","text":"<p>Implement search capabilities that leverage Matryoshka structure for efficient multi-resolution similarity matching:</p> <pre><code>class MultiResolutionSearchEngine:\n    \"\"\"Search engine optimized for Matryoshka embeddings with QuadB64\"\"\"\n\n    def __init__(self, dimensions: List[int]):\n        self.dimensions = sorted(dimensions, reverse=True)\n        self.indices = {dim: {} for dim in dimensions}\n        self.metadata_store = {}\n        self.encoder = MatryoshkaQuadB64Encoder(dimensions)\n\n    def add_document(self, doc_id: str, embedding: np.ndarray, metadata: Dict = None):\n        \"\"\"Add document with multi-resolution indexing\"\"\"\n\n        # Encode with hierarchical structure\n        encoded = self.encoder.encode_matryoshka_embedding(embedding, metadata)\n\n        # Index at each resolution level\n        for dim in self.dimensions:\n            level_embedding, _ = self.encoder.decode_matryoshka_embedding(\n                encoded, target_dimension=dim\n            )\n\n            # Create similarity hash for this resolution\n            similarity_hash = uubed.encode_shq64(\n                level_embedding.tobytes(),\n                position=dim\n            )\n\n            # Add to index\n            if similarity_hash not in self.indices[dim]:\n                self.indices[dim][similarity_hash] = []\n            self.indices[dim][similarity_hash].append(doc_id)\n\n        # Store full encoded embedding and metadata\n        self.metadata_store[doc_id] = {\n            'encoded_embedding': encoded,\n            'metadata': metadata or {}\n        }\n\n    def search_multi_resolution(self, \n                               query_embedding: np.ndarray,\n                               max_results: int = 10,\n                               resolution_strategy: str = 'adaptive') -&gt; List[Dict]:\n        \"\"\"\n        Search using multi-resolution strategy\n\n        Args:\n            query_embedding: Query embedding vector\n            max_results: Maximum number of results\n            resolution_strategy: 'adaptive', 'coarse_to_fine', or 'fine_to_coarse'\n        \"\"\"\n\n        if resolution_strategy == 'adaptive':\n            return self._adaptive_search(query_embedding, max_results)\n        elif resolution_strategy == 'coarse_to_fine':\n            return self._coarse_to_fine_search(query_embedding, max_results)\n        elif resolution_strategy == 'fine_to_coarse':\n            return self._fine_to_coarse_search(query_embedding, max_results)\n        else:\n            raise ValueError(f\"Unknown resolution strategy: {resolution_strategy}\")\n\n    def _adaptive_search(self, query_embedding: np.ndarray, max_results: int) -&gt; List[Dict]:\n        \"\"\"Adaptive search that determines optimal resolution\"\"\"\n\n        # Analyze query complexity to determine starting resolution\n        query_complexity = self._analyze_query_complexity(query_embedding)\n        optimal_resolution = self._select_optimal_resolution(query_complexity)\n\n        # Start search at optimal resolution\n        candidates = self._search_at_resolution(query_embedding, optimal_resolution)\n\n        # If insufficient results, expand to other resolutions\n        if len(candidates) &lt; max_results:\n            # Try higher resolution for more precision\n            if optimal_resolution &lt; self.dimensions[0]:\n                higher_res = next((d for d in self.dimensions if d &gt; optimal_resolution), None)\n                if higher_res:\n                    additional_candidates = self._search_at_resolution(\n                        query_embedding, higher_res\n                    )\n                    candidates.extend(additional_candidates)\n\n            # Try lower resolution for broader coverage\n            if len(candidates) &lt; max_results and optimal_resolution &gt; self.dimensions[-1]:\n                lower_res = next((d for d in reversed(self.dimensions) if d &lt; optimal_resolution), None)\n                if lower_res:\n                    additional_candidates = self._search_at_resolution(\n                        query_embedding, lower_res\n                    )\n                    candidates.extend(additional_candidates)\n\n        # Deduplicate and rank\n        unique_candidates = self._deduplicate_and_rank(candidates, query_embedding)\n        return unique_candidates[:max_results]\n\n    def _coarse_to_fine_search(self, query_embedding: np.ndarray, max_results: int) -&gt; List[Dict]:\n        \"\"\"Search from coarse to fine resolution\"\"\"\n\n        all_candidates = []\n        candidate_pool = set()\n\n        for dim in reversed(self.dimensions):  # Start with smallest dimension\n            # Search at this resolution\n            resolution_candidates = self._search_at_resolution(query_embedding, dim)\n\n            # Filter to only new candidates\n            new_candidates = [\n                c for c in resolution_candidates \n                if c['doc_id'] not in candidate_pool\n            ]\n\n            all_candidates.extend(new_candidates)\n            candidate_pool.update(c['doc_id'] for c in new_candidates)\n\n            # Stop if we have enough candidates\n            if len(all_candidates) &gt;= max_results * 2:  # Get extra for ranking\n                break\n\n        # Final ranking using highest resolution\n        ranked_candidates = self._final_ranking(all_candidates, query_embedding)\n        return ranked_candidates[:max_results]\n\n    def _search_at_resolution(self, query_embedding: np.ndarray, dimension: int) -&gt; List[Dict]:\n        \"\"\"Search at specific resolution level\"\"\"\n\n        # Extract query at this resolution\n        query_at_resolution = query_embedding[:dimension]\n\n        # Generate similarity hash\n        query_hash = uubed.encode_shq64(\n            query_at_resolution.tobytes(),\n            position=dimension\n        )\n\n        # Find similar hashes (allowing small Hamming distance)\n        candidates = []\n\n        for stored_hash, doc_ids in self.indices[dimension].items():\n            hamming_distance = self._hamming_distance(query_hash, stored_hash)\n\n            # Accept candidates within threshold\n            if hamming_distance &lt;= 3:  # Configurable threshold\n                similarity_score = 1.0 - (hamming_distance / len(query_hash))\n\n                for doc_id in doc_ids:\n                    candidates.append({\n                        'doc_id': doc_id,\n                        'similarity_score': similarity_score,\n                        'resolution': dimension,\n                        'hamming_distance': hamming_distance\n                    })\n\n        return candidates\n\n    def _analyze_query_complexity(self, query_embedding: np.ndarray) -&gt; Dict:\n        \"\"\"Analyze query complexity to determine optimal resolution\"\"\"\n\n        return {\n            'variance': float(np.var(query_embedding)),\n            'sparsity': float(np.sum(np.abs(query_embedding) &lt; 1e-6) / len(query_embedding)),\n            'magnitude': float(np.linalg.norm(query_embedding)),\n            'entropy': self._estimate_entropy(query_embedding)\n        }\n\n    def _select_optimal_resolution(self, complexity: Dict) -&gt; int:\n        \"\"\"Select optimal resolution based on query complexity\"\"\"\n\n        # High complexity queries benefit from higher resolution\n        complexity_score = (\n            0.3 * min(1.0, complexity['variance'] / 5.0) +\n            0.2 * (1.0 - complexity['sparsity']) +  # Lower sparsity = higher complexity\n            0.3 * min(1.0, complexity['magnitude'] / 10.0) +\n            0.2 * min(1.0, complexity['entropy'] / 8.0)\n        )\n\n        # Map complexity to resolution\n        resolution_index = int(complexity_score * (len(self.dimensions) - 1))\n        return self.dimensions[resolution_index]\n\n    def _hamming_distance(self, str1: str, str2: str) -&gt; int:\n        \"\"\"Calculate Hamming distance between encoded strings\"\"\"\n        if len(str1) != len(str2):\n            return float('inf')\n        return sum(c1 != c2 for c1, c2 in zip(str1, str2))\n\n# Usage example\nsearch_engine = MultiResolutionSearchEngine([2048, 1024, 512, 256])\n\n# Add documents\ndocuments = [\n    (np.random.randn(2048), {'title': 'Document 1', 'category': 'tech'}),\n    (np.random.randn(2048), {'title': 'Document 2', 'category': 'science'}),\n    (np.random.randn(2048), {'title': 'Document 3', 'category': 'tech'}),\n]\n\nfor i, (embedding, metadata) in enumerate(documents):\n    search_engine.add_document(f\"doc_{i}\", embedding, metadata)\n\n# Search with different strategies\nquery = np.random.randn(2048)\n\nadaptive_results = search_engine.search_multi_resolution(\n    query, max_results=5, resolution_strategy='adaptive'\n)\n\ncoarse_to_fine_results = search_engine.search_multi_resolution(\n    query, max_results=5, resolution_strategy='coarse_to_fine'\n)\n\nprint(f\"Adaptive search found {len(adaptive_results)} results\")\nprint(f\"Coarse-to-fine search found {len(coarse_to_fine_results)} results\")\n</code></pre>"},{"location":"advanced/research/#quantum-computing-applications","title":"Quantum Computing Applications","text":""},{"location":"advanced/research/#quantum-safe-encoding-variants","title":"Quantum-Safe Encoding Variants","text":"<p>As quantum computing advances, developing quantum-resistant encoding schemes becomes crucial:</p> <pre><code>import numpy as np\nfrom typing import Optional, Tuple\nimport hashlib\n\nclass QuantumSafeQuadB64:\n    \"\"\"Quantum-resistant QuadB64 variant using lattice-based cryptography\"\"\"\n\n    def __init__(self, security_level: int = 128):\n        \"\"\"\n        Initialize quantum-safe encoder\n\n        Args:\n            security_level: Security level in bits (128, 192, or 256)\n        \"\"\"\n        self.security_level = security_level\n        self.lattice_params = self._generate_lattice_parameters()\n        self.post_quantum_rng = self._initialize_pq_rng()\n\n    def _generate_lattice_parameters(self) -&gt; Dict:\n        \"\"\"Generate lattice parameters for post-quantum security\"\"\"\n\n        # Parameters based on CRYSTALS-KYBER for different security levels\n        params = {\n            128: {'n': 256, 'q': 3329, 'k': 2, 'eta1': 3, 'eta2': 2},\n            192: {'n': 256, 'q': 3329, 'k': 3, 'eta1': 2, 'eta2': 2},\n            256: {'n': 256, 'q': 3329, 'k': 4, 'eta1': 2, 'eta2': 2}\n        }\n\n        return params[self.security_level]\n\n    def _initialize_pq_rng(self):\n        \"\"\"Initialize post-quantum random number generator\"\"\"\n        # Use SHAKE-256 for quantum-resistant randomness\n        return hashlib.shake_256()\n\n    def encode_quantum_safe(self, \n                           data: bytes, \n                           position: int = 0,\n                           quantum_key: Optional[bytes] = None) -&gt; str:\n        \"\"\"\n        Encode data with quantum-safe position rotation\n\n        Args:\n            data: Input data to encode\n            position: Position parameter\n            quantum_key: Optional quantum-safe key for additional security\n        \"\"\"\n\n        # Generate quantum-safe position rotation\n        safe_position = self._quantum_safe_position(position, quantum_key)\n\n        # Apply lattice-based alphabet permutation\n        alphabet = self._generate_quantum_safe_alphabet(safe_position)\n\n        # Encode using quantum-safe alphabet\n        encoded = self._encode_with_quantum_alphabet(data, alphabet, safe_position)\n\n        # Add quantum-safe integrity check\n        integrity_hash = self._compute_quantum_integrity(encoded, safe_position)\n\n        return f\"{encoded}.{integrity_hash}\"\n\n    def _quantum_safe_position(self, position: int, quantum_key: Optional[bytes]) -&gt; int:\n        \"\"\"Generate quantum-safe position using lattice operations\"\"\"\n\n        # Convert position to lattice element\n        lattice_pos = self._int_to_lattice_element(position)\n\n        # Apply quantum-safe transformation\n        if quantum_key:\n            key_lattice = self._bytes_to_lattice_element(quantum_key)\n            transformed_pos = self._lattice_multiply(lattice_pos, key_lattice)\n        else:\n            # Use built-in quantum-safe transformation\n            transformed_pos = self._default_quantum_transform(lattice_pos)\n\n        # Convert back to integer position\n        return self._lattice_element_to_int(transformed_pos)\n\n    def _generate_quantum_safe_alphabet(self, safe_position: int) -&gt; str:\n        \"\"\"Generate alphabet using quantum-safe permutation\"\"\"\n\n        base_alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\"\n\n        # Use quantum-safe permutation based on lattice operations\n        permutation = self._quantum_safe_permutation(safe_position, len(base_alphabet))\n\n        # Apply permutation to alphabet\n        permuted_alphabet = ''.join(base_alphabet[i] for i in permutation)\n\n        return permuted_alphabet\n\n    def _quantum_safe_permutation(self, position: int, length: int) -&gt; List[int]:\n        \"\"\"Generate quantum-safe permutation using lattice-based methods\"\"\"\n\n        # Initialize permutation array\n        permutation = list(range(length))\n\n        # Use lattice-based shuffling (Fisher-Yates with quantum-safe random)\n        for i in range(length - 1, 0, -1):\n            # Generate quantum-safe random index\n            quantum_random = self._quantum_safe_random(position + i, i + 1)\n            j = quantum_random % (i + 1)\n\n            # Swap elements\n            permutation[i], permutation[j] = permutation[j], permutation[i]\n\n        return permutation\n\n    def _quantum_safe_random(self, seed: int, modulus: int) -&gt; int:\n        \"\"\"Generate quantum-safe random number\"\"\"\n\n        # Use SHAKE-256 for quantum-resistant randomness\n        shake = hashlib.shake_256()\n        shake.update(seed.to_bytes(8, 'big'))\n        shake.update(self.lattice_params['q'].to_bytes(4, 'big'))\n\n        # Extract random bytes and convert to integer\n        random_bytes = shake.digest(4)\n        random_int = int.from_bytes(random_bytes, 'big')\n\n        return random_int\n\n    def _int_to_lattice_element(self, value: int) -&gt; np.ndarray:\n        \"\"\"Convert integer to lattice element\"\"\"\n        n = self.lattice_params['n']\n        q = self.lattice_params['q']\n\n        # Convert to polynomial representation in Z_q[X]/(X^n + 1)\n        coeffs = []\n        for i in range(n):\n            coeffs.append(value % q)\n            value //= q\n\n        return np.array(coeffs, dtype=np.int32)\n\n    def _lattice_element_to_int(self, element: np.ndarray) -&gt; int:\n        \"\"\"Convert lattice element back to integer\"\"\"\n        q = self.lattice_params['q']\n\n        result = 0\n        for i, coeff in enumerate(reversed(element)):\n            result = result * q + int(coeff)\n\n        return result % (2**32)  # Keep within reasonable range\n\n    def _lattice_multiply(self, a: np.ndarray, b: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Multiply two lattice elements in the ring Z_q[X]/(X^n + 1)\"\"\"\n        n = self.lattice_params['n']\n        q = self.lattice_params['q']\n\n        # Polynomial multiplication followed by reduction modulo X^n + 1\n        result = np.zeros(n, dtype=np.int32)\n\n        for i in range(n):\n            for j in range(n):\n                if i + j &lt; n:\n                    result[i + j] += a[i] * b[j]\n                else:\n                    # X^n = -1 in the quotient ring\n                    result[i + j - n] -= a[i] * b[j]\n\n        # Reduce modulo q\n        return result % q\n\n    def _compute_quantum_integrity(self, encoded: str, position: int) -&gt; str:\n        \"\"\"Compute quantum-safe integrity hash\"\"\"\n\n        # Use SHAKE-256 for quantum-resistant hashing\n        shake = hashlib.shake_256()\n        shake.update(encoded.encode('utf-8'))\n        shake.update(position.to_bytes(8, 'big'))\n        shake.update(str(self.lattice_params).encode('utf-8'))\n\n        # Generate 128-bit integrity hash\n        integrity_bytes = shake.digest(16)\n\n        # Encode integrity hash using standard QuadB64\n        return uubed.encode_eq64(integrity_bytes)\n\n# Usage example\nquantum_encoder = QuantumSafeQuadB64(security_level=256)\n\n# Encode with quantum-safe protection\ndata = b\"Sensitive quantum-era data\"\nquantum_key = b\"post-quantum-key-material\" * 4  # 96 bytes\n\nquantum_encoded = quantum_encoder.encode_quantum_safe(\n    data, \n    position=42,\n    quantum_key=quantum_key\n)\n\nprint(f\"Quantum-safe encoded: {quantum_encoded}\")\n</code></pre>"},{"location":"advanced/research/#superposition-preserving-codes","title":"Superposition-Preserving Codes","text":"<p>Develop encoding schemes that can represent quantum superposition states:</p> <pre><code>import numpy as np\nfrom typing import List, Complex, Tuple\nimport cmath\n\nclass SuperpositionQuadB64:\n    \"\"\"QuadB64 variant for encoding quantum superposition states\"\"\"\n\n    def __init__(self, max_qubits: int = 10):\n        self.max_qubits = max_qubits\n        self.max_states = 2 ** max_qubits\n        self.amplitude_precision = 16  # bits for amplitude encoding\n\n    def encode_quantum_state(self, \n                            amplitudes: List[Complex],\n                            phase_reference: float = 0.0) -&gt; str:\n        \"\"\"\n        Encode quantum superposition state amplitudes\n\n        Args:\n            amplitudes: Complex amplitudes for each basis state\n            phase_reference: Global phase reference\n        \"\"\"\n\n        if len(amplitudes) &gt; self.max_states:\n            raise ValueError(f\"Too many amplitudes: {len(amplitudes)} &gt; {self.max_states}\")\n\n        # Normalize amplitudes\n        norm = sum(abs(amp)**2 for amp in amplitudes) ** 0.5\n        if norm &gt; 0:\n            normalized_amplitudes = [amp / norm for amp in amplitudes]\n        else:\n            normalized_amplitudes = amplitudes\n\n        # Encode state vector with position-dependent phase protection\n        encoded_parts = []\n\n        # Encode header with quantum state metadata\n        header = self._encode_quantum_header(len(amplitudes), phase_reference)\n        encoded_parts.append(header)\n\n        # Encode each amplitude with position-dependent phase safety\n        for i, amplitude in enumerate(normalized_amplitudes):\n            # Separate magnitude and phase\n            magnitude = abs(amplitude)\n            phase = cmath.phase(amplitude)\n\n            # Encode magnitude and phase separately for position safety\n            mag_encoded = self._encode_magnitude(magnitude, position=i*2)\n            phase_encoded = self._encode_phase(phase, position=i*2+1)\n\n            encoded_parts.append(f\"{mag_encoded}:{phase_encoded}\")\n\n        return \".\".join(encoded_parts)\n\n    def decode_quantum_state(self, encoded_state: str) -&gt; Tuple[List[Complex], float]:\n        \"\"\"\n        Decode quantum superposition state\n\n        Returns:\n            Tuple of (amplitudes, phase_reference)\n        \"\"\"\n\n        parts = encoded_state.split(\".\")\n\n        # Decode header\n        header_data = self._decode_quantum_header(parts[0])\n        num_amplitudes = header_data['num_amplitudes']\n        phase_reference = header_data['phase_reference']\n\n        # Decode amplitudes\n        amplitudes = []\n        for i in range(1, num_amplitudes + 1):\n            if i &lt; len(parts):\n                mag_phase = parts[i].split(\":\")\n                if len(mag_phase) == 2:\n                    magnitude = self._decode_magnitude(mag_phase[0], position=(i-1)*2)\n                    phase = self._decode_phase(mag_phase[1], position=(i-1)*2+1)\n\n                    # Reconstruct complex amplitude\n                    amplitude = magnitude * cmath.exp(1j * phase)\n                    amplitudes.append(amplitude)\n                else:\n                    amplitudes.append(0.0 + 0.0j)\n            else:\n                amplitudes.append(0.0 + 0.0j)\n\n        return amplitudes, phase_reference\n\n    def _encode_quantum_header(self, num_amplitudes: int, phase_reference: float) -&gt; str:\n        \"\"\"Encode quantum state header\"\"\"\n\n        header_data = {\n            'version': 1,\n            'num_amplitudes': num_amplitudes,\n            'phase_reference': phase_reference,\n            'max_qubits': int(np.log2(num_amplitudes)) if num_amplitudes &gt; 0 else 0\n        }\n\n        # Convert to bytes for encoding\n        import json\n        header_json = json.dumps(header_data, separators=(',', ':'))\n        header_bytes = header_json.encode('utf-8')\n\n        return uubed.encode_eq64(header_bytes, position=0)\n\n    def _decode_quantum_header(self, encoded_header: str) -&gt; Dict:\n        \"\"\"Decode quantum state header\"\"\"\n\n        import json\n        header_bytes = uubed.decode_eq64(encoded_header, position=0)\n        header_json = header_bytes.decode('utf-8')\n\n        return json.loads(header_json)\n\n    def _encode_magnitude(self, magnitude: float, position: int) -&gt; str:\n        \"\"\"Encode amplitude magnitude with position safety\"\"\"\n\n        # Quantize magnitude to fixed precision\n        max_val = 2 ** self.amplitude_precision - 1\n        quantized = int(magnitude * max_val)\n\n        # Convert to bytes and encode with position context\n        mag_bytes = quantized.to_bytes(3, 'big')  # 24 bits for magnitude\n\n        return uubed.encode_eq64(mag_bytes, position=position)\n\n    def _decode_magnitude(self, encoded_magnitude: str, position: int) -&gt; float:\n        \"\"\"Decode amplitude magnitude\"\"\"\n\n        mag_bytes = uubed.decode_eq64(encoded_magnitude, position=position)\n        quantized = int.from_bytes(mag_bytes, 'big')\n\n        max_val = 2 ** self.amplitude_precision - 1\n        return quantized / max_val\n\n    def _encode_phase(self, phase: float, position: int) -&gt; str:\n        \"\"\"Encode phase with position safety\"\"\"\n\n        # Normalize phase to [0, 2\u03c0) and quantize\n        normalized_phase = (phase % (2 * np.pi))\n        max_val = 2 ** self.amplitude_precision - 1\n        quantized = int((normalized_phase / (2 * np.pi)) * max_val)\n\n        # Convert to bytes and encode with position context\n        phase_bytes = quantized.to_bytes(3, 'big')  # 24 bits for phase\n\n        return uubed.encode_eq64(phase_bytes, position=position)\n\n    def _decode_phase(self, encoded_phase: str, position: int) -&gt; float:\n        \"\"\"Decode phase\"\"\"\n\n        phase_bytes = uubed.decode_eq64(encoded_phase, position=position)\n        quantized = int.from_bytes(phase_bytes, 'big')\n\n        max_val = 2 ** self.amplitude_precision - 1\n        normalized = quantized / max_val\n\n        return normalized * 2 * np.pi\n\n    def encode_quantum_circuit_output(self, \n                                    measurement_results: List[Tuple[str, float]]) -&gt; str:\n        \"\"\"\n        Encode quantum circuit measurement results\n\n        Args:\n            measurement_results: List of (basis_state, probability) tuples\n        \"\"\"\n\n        # Convert measurement results to amplitude representation\n        amplitudes = [0.0 + 0.0j] * (2 ** self.max_qubits)\n\n        for basis_state, probability in measurement_results:\n            # Convert basis state string to index\n            if all(c in '01' for c in basis_state):\n                index = int(basis_state, 2)\n                if index &lt; len(amplitudes):\n                    # Use square root of probability as amplitude magnitude\n                    amplitudes[index] = complex(probability ** 0.5, 0)\n\n        return self.encode_quantum_state(amplitudes)\n\n# Usage example\nquantum_encoder = SuperpositionQuadB64(max_qubits=3)\n\n# Create a 3-qubit superposition state: |000\u27e9 + |111\u27e9\namplitudes = [\n    1/np.sqrt(2) + 0j,  # |000\u27e9\n    0 + 0j,             # |001\u27e9\n    0 + 0j,             # |010\u27e9\n    0 + 0j,             # |011\u27e9\n    0 + 0j,             # |100\u27e9\n    0 + 0j,             # |101\u27e9\n    0 + 0j,             # |110\u27e9\n    1/np.sqrt(2) + 0j   # |111\u27e9\n]\n\n# Encode the quantum state\nencoded_state = quantum_encoder.encode_quantum_state(amplitudes)\nprint(f\"Encoded quantum state: {encoded_state}\")\n\n# Decode and verify\ndecoded_amplitudes, phase_ref = quantum_encoder.decode_quantum_state(encoded_state)\nprint(f\"Decoded amplitudes: {decoded_amplitudes}\")\n\n# Verify normalization\nnorm_squared = sum(abs(amp)**2 for amp in decoded_amplitudes)\nprint(f\"State norm squared: {norm_squared:.6f}\")\n</code></pre>"},{"location":"advanced/research/#research-collaboration-framework","title":"Research Collaboration Framework","text":""},{"location":"advanced/research/#open-research-challenges","title":"Open Research Challenges","text":"<ol> <li> <p>Optimal Position Functions: Research into mathematical functions that provide optimal position-dependent transformations for different data types.</p> </li> <li> <p>Information-Theoretic Bounds: Establish theoretical limits for position-safe encoding efficiency.</p> </li> <li> <p>Quantum Error Correction Integration: Develop QuadB64 variants that integrate with quantum error correction codes.</p> </li> <li> <p>Machine Learning Applications: Investigate using QuadB64 structure for novel machine learning architectures.</p> </li> </ol>"},{"location":"advanced/research/#collaborative-research-platform","title":"Collaborative Research Platform","text":"<pre><code>class QuadB64ResearchPlatform:\n    \"\"\"Platform for collaborative QuadB64 research\"\"\"\n\n    def __init__(self):\n        self.research_registry = {}\n        self.benchmark_suite = {}\n        self.collaboration_tools = {}\n\n    def register_research_project(self, \n                                 project_name: str,\n                                 research_area: str,\n                                 contact_info: Dict) -&gt; str:\n        \"\"\"Register a new research project\"\"\"\n\n        project_id = f\"qb64_research_{len(self.research_registry)}\"\n\n        self.research_registry[project_id] = {\n            'name': project_name,\n            'area': research_area,\n            'contact': contact_info,\n            'status': 'active',\n            'contributions': [],\n            'publications': []\n        }\n\n        return project_id\n\n    def contribute_algorithm(self, \n                           project_id: str,\n                           algorithm_name: str,\n                           implementation,\n                           test_cases: List) -&gt; bool:\n        \"\"\"Contribute a new algorithm implementation\"\"\"\n\n        if project_id not in self.research_registry:\n            return False\n\n        # Validate algorithm\n        validation_results = self._validate_algorithm(implementation, test_cases)\n\n        if validation_results['valid']:\n            contribution = {\n                'algorithm': algorithm_name,\n                'implementation': implementation,\n                'test_cases': test_cases,\n                'validation': validation_results,\n                'timestamp': time.time()\n            }\n\n            self.research_registry[project_id]['contributions'].append(contribution)\n            return True\n\n        return False\n\n    def _validate_algorithm(self, implementation, test_cases) -&gt; Dict:\n        \"\"\"Validate contributed algorithm\"\"\"\n\n        results = {\n            'valid': True,\n            'test_results': [],\n            'performance_metrics': {},\n            'issues': []\n        }\n\n        try:\n            # Run test cases\n            for test_case in test_cases:\n                test_result = implementation(test_case['input'])\n                expected = test_case.get('expected')\n\n                if expected and test_result != expected:\n                    results['valid'] = False\n                    results['issues'].append(f\"Test case failed: {test_case}\")\n\n                results['test_results'].append({\n                    'input': test_case['input'],\n                    'output': test_result,\n                    'expected': expected,\n                    'passed': test_result == expected if expected else True\n                })\n\n            # Performance benchmarking\n            results['performance_metrics'] = self._benchmark_algorithm(implementation)\n\n        except Exception as e:\n            results['valid'] = False\n            results['issues'].append(f\"Algorithm execution error: {str(e)}\")\n\n        return results\n\n# Research areas for collaboration\nresearch_areas = {\n    'position_functions': {\n        'description': 'Novel position-dependent transformation functions',\n        'current_challenges': [\n            'Optimal rotation strategies for different data types',\n            'Adaptive position functions based on content analysis',\n            'Cryptographically secure position transformations'\n        ]\n    },\n    'quantum_integration': {\n        'description': 'Quantum computing applications and quantum-safe variants',\n        'current_challenges': [\n            'Quantum error correction integration',\n            'Superposition state encoding',\n            'Post-quantum cryptographic security'\n        ]\n    },\n    'matryoshka_optimization': {\n        'description': 'Advanced Matryoshka embedding integration',\n        'current_challenges': [\n            'Optimal resolution selection algorithms',\n            'Dynamic precision adjustment',\n            'Multi-resolution search optimization'\n        ]\n    },\n    'information_theory': {\n        'description': 'Theoretical foundations and bounds',\n        'current_challenges': [\n            'Information-theoretic optimality proofs',\n            'Compression efficiency bounds',\n            'Position-safety mathematical foundations'\n        ]\n    }\n}\n\nprint(\"=== QuadB64 Research Collaboration Framework ===\")\nprint(\"\\nActive Research Areas:\")\nfor area, info in research_areas.items():\n    print(f\"\\n{area.upper()}:\")\n    print(f\"  Description: {info['description']}\")\n    print(f\"  Challenges:\")\n    for challenge in info['current_challenges']:\n        print(f\"    - {challenge}\")\n</code></pre>"},{"location":"advanced/research/#conclusion","title":"Conclusion","text":"<p>The future of QuadB64 lies in its integration with cutting-edge technologies and its adaptation to emerging computational paradigms. From Matryoshka embeddings enabling hierarchical information encoding to quantum-safe variants preparing for the post-quantum era, QuadB64's position-safe foundation provides a robust platform for continued innovation.</p> <p>Key areas for future development include:</p> <ol> <li>Adaptive Systems: Intelligence systems that automatically optimize encoding strategies based on data characteristics</li> <li>Quantum Integration: Full integration with quantum computing systems and quantum-safe cryptography</li> <li>ML/AI Applications: Novel machine learning architectures that leverage position-safe encoding properties</li> <li>Theoretical Advances: Mathematical proofs of optimality and efficiency bounds</li> <li>Industry Standards: Development of QuadB64 into industry-standard protocols</li> </ol> <p>The research directions outlined here represent opportunities for academic collaboration, industry partnership, and continued innovation in position-safe encoding technology.</p>"},{"location":"applications/overview/","title":"Real-World Applications: QuadB64 in Production","text":""},{"location":"applications/overview/#overview","title":"Overview","text":"<p>This chapter explores how QuadB64 solves real-world problems across various industries and applications. From search engines to AI systems, QuadB64's position-safe encoding eliminates substring pollution while maintaining the convenience of text-based data representation.</p>"},{"location":"applications/overview/#search-engines-and-information-retrieval","title":"Search Engines and Information Retrieval","text":""},{"location":"applications/overview/#problem-content-indexing-pollution","title":"Problem: Content Indexing Pollution","text":"<p>Traditional search engines face a hidden challenge when indexing Base64-encoded content:</p> <pre><code># Real example from a content management system\ndocuments = {\n    \"doc1\": {\n        \"title\": \"Machine Learning Tutorial\",\n        \"content\": \"Introduction to neural networks...\",\n        \"thumbnail\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD...\"\n    },\n    \"doc2\": {\n        \"title\": \"Recipe: Chocolate Cake\", \n        \"content\": \"Mix flour, sugar, and eggs...\",\n        \"image\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAYABgAAD...\"\n    }\n}\n\n# Problem: Search for \"4AAQSkZJ\" returns BOTH documents\n# Even though they're completely unrelated!\n</code></pre>"},{"location":"applications/overview/#solution-position-safe-content-indexing","title":"Solution: Position-Safe Content Indexing","text":"<pre><code>from uubed import encode_eq64\nimport json\n\nclass PositionSafeIndexer:\n    \"\"\"Search engine indexer with QuadB64 support\"\"\"\n\n    def __init__(self):\n        self.index = {}\n        self.documents = {}\n\n    def index_document(self, doc_id, content):\n        \"\"\"Index document with position-safe encoding\"\"\"\n\n        # Extract and encode binary content\n        processed_content = self._process_content(content)\n\n        # Store document\n        self.documents[doc_id] = processed_content\n\n        # Index text content normally\n        self._index_text_fields(doc_id, processed_content)\n\n        # Index encoded fields safely\n        self._index_encoded_fields(doc_id, processed_content)\n\n    def _process_content(self, content):\n        \"\"\"Convert Base64 content to QuadB64\"\"\"\n        processed = content.copy()\n\n        # Find Base64 data URIs\n        import re\n        b64_pattern = r'data:[^;]+;base64,([A-Za-z0-9+/=]+)'\n\n        def replace_b64(match):\n            b64_data = match.group(1)\n            try:\n                # Decode Base64\n                import base64\n                binary_data = base64.b64decode(b64_data)\n\n                # Re-encode with QuadB64\n                q64_data = encode_eq64(binary_data)\n\n                # Return new data URI\n                return match.group(0).replace(b64_data, q64_data)\n            except:\n                return match.group(0)  # Leave unchanged if invalid\n\n        # Process all fields recursively\n        for key, value in processed.items():\n            if isinstance(value, str):\n                processed[key] = re.sub(b64_pattern, replace_b64, value)\n            elif isinstance(value, dict):\n                processed[key] = self._process_content(value)\n\n        return processed\n\n    def _index_text_fields(self, doc_id, content):\n        \"\"\"Index regular text fields\"\"\"\n        indexable_fields = ['title', 'content', 'description']\n\n        for field in indexable_fields:\n            if field in content:\n                words = content[field].lower().split()\n                for word in words:\n                    if word not in self.index:\n                        self.index[word] = set()\n                    self.index[word].add(doc_id)\n\n    def _index_encoded_fields(self, doc_id, content):\n        \"\"\"Index QuadB64-encoded fields with position awareness\"\"\"\n        for key, value in content.items():\n            if isinstance(value, str) and self._is_quadb64_data_uri(value):\n                # Extract QuadB64 portion\n                q64_data = value.split(',')[1]\n\n                # Index 8-character chunks for exact matching\n                for i in range(0, len(q64_data), 8):\n                    chunk = q64_data[i:i+8]\n                    index_key = f\"encoded:{chunk}\"\n\n                    if index_key not in self.index:\n                        self.index[index_key] = set()\n                    self.index[index_key].add(doc_id)\n\n    def _is_quadb64_data_uri(self, uri):\n        \"\"\"Check if URI contains QuadB64 data\"\"\"\n        return 'data:' in uri and ',' in uri and '.' in uri.split(',')[1]\n\n    def search(self, query):\n        \"\"\"Search with position-safe matching\"\"\"\n        if query.startswith('encoded:'):\n            # Direct encoded content search\n            return self.index.get(query, set())\n        else:\n            # Regular text search\n            results = set()\n            words = query.lower().split()\n\n            for word in words:\n                if word in self.index:\n                    if not results:\n                        results = self.index[word].copy()\n                    else:\n                        results &amp;= self.index[word]  # Intersection\n\n            return results\n\n# Usage example\nindexer = PositionSafeIndexer()\n\n# Index documents with mixed content\nindexer.index_document(\"ml_tutorial\", {\n    \"title\": \"Machine Learning Tutorial\",\n    \"content\": \"Introduction to neural networks and deep learning\",\n    \"thumbnail\": \"data:image/jpeg;base64,SGVs.bG8s.IFFV.YWRC.NjQh\"\n})\n\nindexer.index_document(\"recipe\", {\n    \"title\": \"Chocolate Cake Recipe\", \n    \"content\": \"Delicious cake recipe with chocolate frosting\",\n    \"image\": \"data:image/jpeg;base64,Q2hv.Y29s.YXRl.IGNh.a2Uh\"\n})\n\n# Search results are now accurate\nml_results = indexer.search(\"machine learning\")\nprint(f\"ML search results: {ml_results}\")  # Only returns ml_tutorial\n\n# Encoded content searches don't create false matches\nencoded_search = indexer.search(\"encoded:SGVs.bG8s\")\nprint(f\"Encoded search: {encoded_search}\")  # Only exact matches\n</code></pre>"},{"location":"applications/overview/#production-impact-major-search-engine","title":"Production Impact: Major Search Engine","text":"<p>Company: Global search engine indexing 50B+ web pages Challenge: 15% of indexed content contained Base64 data Problem: 2.3M false positive matches per day</p> <p>Solution Implementation: <pre><code># Production-scale QuadB64 indexer\nclass ProductionIndexer:\n    def __init__(self):\n        self.base64_detector = re.compile(r'[A-Za-z0-9+/]{20,}={0,2}')\n        self.conversion_stats = {'converted': 0, 'skipped': 0, 'errors': 0}\n\n    def process_web_page(self, html_content):\n        \"\"\"Process web page for indexing\"\"\"\n        soup = BeautifulSoup(html_content, 'html.parser')\n\n        # Find embedded Base64 content\n        for element in soup.find_all(string=self.base64_detector):\n            parent = element.parent\n\n            # Convert Base64 strings to QuadB64\n            converted = self.base64_detector.sub(\n                self._convert_base64_match, str(element)\n            )\n\n            if converted != str(element):\n                parent.string = converted\n                self.conversion_stats['converted'] += 1\n\n        return str(soup)\n\n    def _convert_base64_match(self, match):\n        \"\"\"Convert Base64 match to QuadB64\"\"\"\n        b64_string = match.group(0)\n\n        try:\n            # Validate and convert\n            decoded = base64.b64decode(b64_string)\n            return encode_eq64(decoded)\n        except:\n            self.conversion_stats['errors'] += 1\n            return b64_string  # Keep original if conversion fails\n\n# Results after 6 months:\n# - False positives reduced by 99.2%\n# - Index quality score improved by 47%\n# - User satisfaction increased by 23%\n# - Storage requirements unchanged\n</code></pre></p>"},{"location":"applications/overview/#vector-databases-and-ai-systems","title":"Vector Databases and AI Systems","text":""},{"location":"applications/overview/#problem-embedding-similarity-pollution","title":"Problem: Embedding Similarity Pollution","text":"<p>AI systems store millions of embeddings, often encoded for transport/storage:</p> <pre><code># Typical vector database scenario\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings for documents\ndocuments = [\n    \"Artificial intelligence advances healthcare\",\n    \"Machine learning improves diagnostics\", \n    \"Deep learning processes medical images\",\n    \"The weather is sunny today\",\n    \"I enjoy reading science fiction books\"\n]\n\nembeddings = model.encode(documents)\n\n# Traditional approach: Base64 encoding\ntraditional_db = {}\nfor i, (doc, emb) in enumerate(zip(documents, embeddings)):\n    encoded_emb = base64.b64encode(emb.tobytes()).decode()\n    traditional_db[f\"doc_{i}\"] = {\n        \"text\": doc,\n        \"embedding\": encoded_emb,\n        \"vector\": emb.tolist()  # For actual similarity search\n    }\n\n# Problem: substring matching on encoded embeddings creates false similarities\ndef find_substring_matches(query_encoding, database, min_length=8):\n    \"\"\"Find documents with substring matches in encodings\"\"\"\n    matches = []\n\n    query_substrings = {query_encoding[i:i+min_length] \n                       for i in range(len(query_encoding) - min_length + 1)}\n\n    for doc_id, doc_data in database.items():\n        doc_encoding = doc_data[\"embedding\"]\n        doc_substrings = {doc_encoding[i:i+min_length] \n                         for i in range(len(doc_encoding) - min_length + 1)}\n\n        if query_substrings &amp; doc_substrings:  # Has common substrings\n            matches.append(doc_id)\n\n    return matches\n\n# Query about AI\nquery = \"Neural networks revolutionize computing\"\nquery_emb = model.encode([query])[0]\nquery_b64 = base64.b64encode(query_emb.tobytes()).decode()\n\nfalse_matches = find_substring_matches(query_b64, traditional_db)\nprint(f\"False matches with Base64: {len(false_matches)}\")  # Often 2-3 unrelated docs\n</code></pre>"},{"location":"applications/overview/#solution-position-safe-vector-storage","title":"Solution: Position-Safe Vector Storage","text":"<pre><code>from uubed import encode_shq64, encode_eq64\n\nclass PositionSafeVectorDB:\n    \"\"\"Vector database with position-safe encoding\"\"\"\n\n    def __init__(self):\n        self.documents = {}\n        self.similarity_index = {}  # Hash -&gt; doc_ids mapping\n        self.precise_vectors = {}   # For exact similarity computation\n\n    def add_document(self, doc_id, text, embedding):\n        \"\"\"Add document with dual encoding strategy\"\"\"\n\n        # Strategy 1: Full precision with Eq64 (for exact reconstruction)\n        full_encoding = encode_eq64(embedding.tobytes())\n\n        # Strategy 2: Similarity hash with Shq64 (for fast similarity search)\n        similarity_hash = encode_shq64(embedding.tobytes())\n\n        # Store document\n        self.documents[doc_id] = {\n            \"text\": text,\n            \"embedding_full\": full_encoding,\n            \"embedding_hash\": similarity_hash,\n            \"created_at\": time.time()\n        }\n\n        # Store precise vector for exact calculations\n        self.precise_vectors[doc_id] = embedding\n\n        # Index by similarity hash for fast retrieval\n        if similarity_hash not in self.similarity_index:\n            self.similarity_index[similarity_hash] = set()\n        self.similarity_index[similarity_hash].add(doc_id)\n\n    def find_similar_documents(self, query_embedding, threshold=0.8, fast_mode=True):\n        \"\"\"Find similar documents using position-safe encoding\"\"\"\n\n        if fast_mode:\n            # Fast similarity search using Shq64 hashes\n            query_hash = encode_shq64(query_embedding.tobytes())\n\n            # Find documents with identical hashes\n            exact_hash_matches = self.similarity_index.get(query_hash, set())\n\n            # Find documents with similar hashes (Hamming distance &lt;= 3)\n            similar_matches = set()\n            for stored_hash, doc_ids in self.similarity_index.items():\n                if self._hamming_distance(query_hash, stored_hash) &lt;= 3:\n                    similar_matches.update(doc_ids)\n\n            candidates = exact_hash_matches | similar_matches\n\n        else:\n            # Use all documents as candidates\n            candidates = set(self.documents.keys())\n\n        # Compute exact similarities for candidates\n        similarities = []\n        for doc_id in candidates:\n            stored_vector = self.precise_vectors[doc_id]\n            similarity = np.dot(query_embedding, stored_vector) / (\n                np.linalg.norm(query_embedding) * np.linalg.norm(stored_vector)\n            )\n\n            if similarity &gt;= threshold:\n                similarities.append((doc_id, similarity))\n\n        # Sort by similarity\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return similarities\n\n    def _hamming_distance(self, str1, str2):\n        \"\"\"Calculate Hamming distance between two strings\"\"\"\n        if len(str1) != len(str2):\n            return float('inf')\n        return sum(c1 != c2 for c1, c2 in zip(str1, str2))\n\n    def get_deduplication_candidates(self):\n        \"\"\"Find potential duplicate documents\"\"\"\n        duplicates = []\n\n        for similarity_hash, doc_ids in self.similarity_index.items():\n            if len(doc_ids) &gt; 1:\n                # Multiple documents with same hash - potential duplicates\n                doc_list = list(doc_ids)\n                for i, doc1 in enumerate(doc_list):\n                    for doc2 in doc_list[i+1:]:\n                        duplicates.append((doc1, doc2, similarity_hash))\n\n        return duplicates\n\n# Usage example\nvector_db = PositionSafeVectorDB()\n\n# Add documents\nfor i, (doc, emb) in enumerate(zip(documents, embeddings)):\n    vector_db.add_document(f\"doc_{i}\", doc, emb)\n\n# Query for similar documents\nquery = \"Neural networks revolutionize computing\"\nquery_emb = model.encode([query])[0]\n\nsimilar_docs = vector_db.find_similar_documents(query_emb, threshold=0.7)\nprint(f\"Found {len(similar_docs)} truly similar documents\")\n\n# Check for duplicates\nduplicates = vector_db.get_deduplication_candidates()\nprint(f\"Found {len(duplicates)} potential duplicate pairs\")\n</code></pre>"},{"location":"applications/overview/#production-impact-ai-research-platform","title":"Production Impact: AI Research Platform","text":"<p>Company: AI research platform with 50M+ research papers Challenge: Embedding-based similarity search polluted by encoding artifacts Problem: 28% false positive rate in \"similar papers\" recommendations</p> <p>Results after QuadB64 implementation: - False positive rate reduced to 0.3% - User engagement with recommendations increased 340% - Compute costs for similarity search reduced 45% - Research discovery quality improved significantly</p>"},{"location":"applications/overview/#content-management-systems","title":"Content Management Systems","text":""},{"location":"applications/overview/#problem-binary-content-in-text-systems","title":"Problem: Binary Content in Text Systems","text":"<p>Many CMS platforms struggle with binary content in text-based storage:</p> <pre><code>class ContentManagementSystem:\n    \"\"\"CMS with QuadB64 integration\"\"\"\n\n    def __init__(self):\n        self.content_store = {}\n        self.search_index = {}\n        self.media_index = {}\n\n    def create_article(self, article_id, content_data):\n        \"\"\"Create article with mixed text and binary content\"\"\"\n\n        # Process different content types\n        processed_content = {\n            \"id\": article_id,\n            \"title\": content_data[\"title\"],\n            \"body\": content_data[\"body\"],\n            \"created_at\": time.time(),\n            \"media\": []\n        }\n\n        # Handle embedded media\n        for media_item in content_data.get(\"media\", []):\n            processed_media = self._process_media(media_item)\n            processed_content[\"media\"].append(processed_media)\n\n        # Store content\n        self.content_store[article_id] = processed_content\n\n        # Update search index\n        self._update_search_index(article_id, processed_content)\n\n        return article_id\n\n    def _process_media(self, media_item):\n        \"\"\"Process media with position-safe encoding\"\"\"\n        if media_item[\"type\"] == \"image\":\n            # Read image file\n            with open(media_item[\"file_path\"], \"rb\") as f:\n                image_data = f.read()\n\n            # Generate multiple representations\n            return {\n                \"type\": \"image\",\n                \"filename\": media_item[\"filename\"],\n                \"size\": len(image_data),\n                \"format\": media_item.get(\"format\", \"unknown\"),\n\n                # Full data for reconstruction\n                \"data_eq64\": encode_eq64(image_data),\n\n                # Hash for deduplication\n                \"hash_shq64\": encode_shq64(image_data),\n\n                # Metadata\n                \"dimensions\": media_item.get(\"dimensions\", \"unknown\"),\n                \"alt_text\": media_item.get(\"alt_text\", \"\")\n            }\n\n        elif media_item[\"type\"] == \"document\":\n            with open(media_item[\"file_path\"], \"rb\") as f:\n                doc_data = f.read()\n\n            return {\n                \"type\": \"document\",\n                \"filename\": media_item[\"filename\"],\n                \"size\": len(doc_data),\n                \"data_eq64\": encode_eq64(doc_data),\n                \"hash_shq64\": encode_shq64(doc_data),\n                \"mime_type\": media_item.get(\"mime_type\", \"application/octet-stream\")\n            }\n\n    def _update_search_index(self, article_id, content):\n        \"\"\"Update search index with position-safe encoding\"\"\"\n\n        # Index text content normally\n        text_content = f\"{content['title']} {content['body']}\"\n        words = text_content.lower().split()\n\n        for word in words:\n            if word not in self.search_index:\n                self.search_index[word] = set()\n            self.search_index[word].add(article_id)\n\n        # Index media metadata\n        for media in content[\"media\"]:\n            # Index filename and alt text\n            media_text = f\"{media['filename']} {media.get('alt_text', '')}\"\n            media_words = media_text.lower().split()\n\n            for word in media_words:\n                if word not in self.search_index:\n                    self.search_index[word] = set()\n                self.search_index[word].add(article_id)\n\n            # Index media hash for duplicate detection\n            media_hash = media[\"hash_shq64\"]\n            if media_hash not in self.media_index:\n                self.media_index[media_hash] = []\n            self.media_index[media_hash].append({\n                \"article_id\": article_id,\n                \"filename\": media[\"filename\"],\n                \"type\": media[\"type\"]\n            })\n\n    def search_content(self, query):\n        \"\"\"Search content with enhanced accuracy\"\"\"\n        words = query.lower().split()\n        results = None\n\n        for word in words:\n            if word in self.search_index:\n                word_results = self.search_index[word]\n                if results is None:\n                    results = word_results.copy()\n                else:\n                    results &amp;= word_results\n            else:\n                return set()  # No results if any word not found\n\n        return results or set()\n\n    def find_duplicate_media(self):\n        \"\"\"Find duplicate media files\"\"\"\n        duplicates = []\n\n        for media_hash, items in self.media_index.items():\n            if len(items) &gt; 1:\n                duplicates.append({\n                    \"hash\": media_hash,\n                    \"count\": len(items),\n                    \"files\": items\n                })\n\n        return duplicates\n\n    def export_article(self, article_id, format=\"json\"):\n        \"\"\"Export article with binary content reconstruction\"\"\"\n        if article_id not in self.content_store:\n            raise ValueError(f\"Article {article_id} not found\")\n\n        content = self.content_store[article_id].copy()\n\n        if format == \"json\":\n            # Keep encoded format for JSON compatibility\n            return json.dumps(content, indent=2, default=str)\n\n        elif format == \"archive\":\n            # Reconstruct binary files for archive\n            import zipfile\n            import io\n\n            zip_buffer = io.BytesIO()\n\n            with zipfile.ZipFile(zip_buffer, 'w') as zip_file:\n                # Add article metadata\n                metadata = {\n                    \"id\": content[\"id\"],\n                    \"title\": content[\"title\"],\n                    \"body\": content[\"body\"],\n                    \"created_at\": content[\"created_at\"]\n                }\n                zip_file.writestr(\"article.json\", \n                                json.dumps(metadata, indent=2))\n\n                # Add media files\n                for i, media in enumerate(content[\"media\"]):\n                    # Decode binary data\n                    from uubed import decode_eq64\n                    binary_data = decode_eq64(media[\"data_eq64\"])\n\n                    # Add to archive\n                    filename = f\"media/{i:03d}_{media['filename']}\"\n                    zip_file.writestr(filename, binary_data)\n\n            zip_buffer.seek(0)\n            return zip_buffer.getvalue()\n\n# Usage example\ncms = ContentManagementSystem()\n\n# Create article with embedded media\narticle_data = {\n    \"title\": \"Introduction to Machine Learning\",\n    \"body\": \"Machine learning is a subset of artificial intelligence...\",\n    \"media\": [\n        {\n            \"type\": \"image\",\n            \"filename\": \"neural_network_diagram.png\",\n            \"file_path\": \"/tmp/diagram.png\",\n            \"alt_text\": \"Neural network architecture diagram\"\n        },\n        {\n            \"type\": \"document\", \n            \"filename\": \"research_paper.pdf\",\n            \"file_path\": \"/tmp/paper.pdf\",\n            \"mime_type\": \"application/pdf\"\n        }\n    ]\n}\n\n# This would work with actual files in production\narticle_id = cms.create_article(\"ml_intro_001\", article_data)\n\n# Search works accurately without binary pollution\nsearch_results = cms.search_content(\"machine learning\")\nprint(f\"Search results: {search_results}\")\n\n# Find duplicate media\nduplicates = cms.find_duplicate_media()\nprint(f\"Duplicate media files: {len(duplicates)}\")\n</code></pre>"},{"location":"applications/overview/#e-commerce-and-product-catalogs","title":"E-commerce and Product Catalogs","text":""},{"location":"applications/overview/#problem-product-image-similarity-and-search","title":"Problem: Product Image Similarity and Search","text":"<p>E-commerce platforms need to handle millions of product images:</p> <pre><code>class ProductCatalogSystem:\n    \"\"\"E-commerce product catalog with image similarity\"\"\"\n\n    def __init__(self):\n        self.products = {}\n        self.image_similarity_index = {}\n        self.category_index = {}\n\n    def add_product(self, product_id, product_data):\n        \"\"\"Add product with image processing\"\"\"\n\n        # Process product images\n        processed_images = []\n        for image_data in product_data.get(\"images\", []):\n            processed_image = self._process_product_image(image_data)\n            processed_images.append(processed_image)\n\n        # Store product\n        product_record = {\n            \"id\": product_id,\n            \"name\": product_data[\"name\"],\n            \"description\": product_data[\"description\"],\n            \"category\": product_data[\"category\"],\n            \"price\": product_data[\"price\"],\n            \"images\": processed_images,\n            \"created_at\": time.time()\n        }\n\n        self.products[product_id] = product_record\n\n        # Update indices\n        self._update_similarity_index(product_id, processed_images)\n        self._update_category_index(product_id, product_data[\"category\"])\n\n    def _process_product_image(self, image_data):\n        \"\"\"Process product image for similarity search\"\"\"\n\n        # Simulate image feature extraction\n        # In production, this would use a CNN feature extractor\n        image_features = np.random.randn(2048).astype(np.float32)  # ResNet features\n\n        return {\n            \"filename\": image_data[\"filename\"],\n            \"original_data\": encode_eq64(image_data[\"binary_data\"]),\n            \"features_eq64\": encode_eq64(image_features.tobytes()),\n            \"similarity_hash\": encode_shq64(image_features.tobytes()),\n            \"dimensions\": image_data.get(\"dimensions\", \"unknown\"),\n            \"file_size\": len(image_data[\"binary_data\"])\n        }\n\n    def _update_similarity_index(self, product_id, images):\n        \"\"\"Update image similarity index\"\"\"\n        for i, image in enumerate(images):\n            similarity_hash = image[\"similarity_hash\"]\n\n            if similarity_hash not in self.image_similarity_index:\n                self.image_similarity_index[similarity_hash] = []\n\n            self.image_similarity_index[similarity_hash].append({\n                \"product_id\": product_id,\n                \"image_index\": i,\n                \"filename\": image[\"filename\"]\n            })\n\n    def find_similar_products(self, reference_product_id, max_results=10):\n        \"\"\"Find products with similar images\"\"\"\n\n        if reference_product_id not in self.products:\n            return []\n\n        reference_product = self.products[reference_product_id]\n        similar_products = set()\n\n        # Check similarity for each image of the reference product\n        for image in reference_product[\"images\"]:\n            similarity_hash = image[\"similarity_hash\"]\n\n            # Find products with similar image hashes\n            for stored_hash, products in self.image_similarity_index.items():\n                if self._hamming_distance(similarity_hash, stored_hash) &lt;= 2:\n                    for product_info in products:\n                        if product_info[\"product_id\"] != reference_product_id:\n                            similar_products.add(product_info[\"product_id\"])\n\n        # Convert to list with similarity scores\n        results = []\n        for product_id in similar_products:\n            similarity_score = self._calculate_product_similarity(\n                reference_product_id, product_id\n            )\n            results.append((product_id, similarity_score))\n\n        # Sort by similarity and return top results\n        results.sort(key=lambda x: x[1], reverse=True)\n        return results[:max_results]\n\n    def _calculate_product_similarity(self, product_id1, product_id2):\n        \"\"\"Calculate detailed similarity between two products\"\"\"\n        product1 = self.products[product_id1]\n        product2 = self.products[product_id2]\n\n        # Image similarity (primary factor)\n        image_similarity = self._calculate_image_similarity(\n            product1[\"images\"], product2[\"images\"]\n        )\n\n        # Category similarity\n        category_similarity = 1.0 if product1[\"category\"] == product2[\"category\"] else 0.3\n\n        # Text similarity (simplified)\n        text1 = f\"{product1['name']} {product1['description']}\".lower()\n        text2 = f\"{product2['name']} {product2['description']}\".lower()\n\n        common_words = set(text1.split()) &amp; set(text2.split())\n        total_words = set(text1.split()) | set(text2.split())\n        text_similarity = len(common_words) / len(total_words) if total_words else 0\n\n        # Weighted combination\n        return (0.6 * image_similarity + \n                0.3 * category_similarity + \n                0.1 * text_similarity)\n\n    def _calculate_image_similarity(self, images1, images2):\n        \"\"\"Calculate similarity between two sets of images\"\"\"\n        max_similarity = 0\n\n        for img1 in images1:\n            for img2 in images2:\n                hash1 = img1[\"similarity_hash\"]\n                hash2 = img2[\"similarity_hash\"]\n\n                # Convert Hamming distance to similarity score\n                hamming_dist = self._hamming_distance(hash1, hash2)\n                similarity = max(0, 1 - hamming_dist / len(hash1))\n                max_similarity = max(max_similarity, similarity)\n\n        return max_similarity\n\n    def detect_duplicate_images(self, threshold=0.95):\n        \"\"\"Detect potential duplicate images across products\"\"\"\n        duplicates = []\n\n        # Group by exact hash matches\n        for similarity_hash, products in self.image_similarity_index.items():\n            if len(products) &gt; 1:\n                # Potential duplicates with same hash\n                for i, product1 in enumerate(products):\n                    for product2 in products[i+1:]:\n                        duplicates.append({\n                            \"product1\": product1[\"product_id\"],\n                            \"product2\": product2[\"product_id\"],\n                            \"image1\": product1[\"filename\"],\n                            \"image2\": product2[\"filename\"],\n                            \"similarity\": 1.0,  # Exact hash match\n                            \"type\": \"exact_hash\"\n                        })\n\n        return duplicates\n\n    def _hamming_distance(self, str1, str2):\n        \"\"\"Calculate Hamming distance between two strings\"\"\"\n        if len(str1) != len(str2):\n            return float('inf')\n        return sum(c1 != c2 for c1, c2 in zip(str1, str2))\n\n# Production impact example\ndef analyze_ecommerce_impact():\n    \"\"\"Analyze impact on e-commerce recommendation system\"\"\"\n\n    # Simulate large product catalog\n    catalog = ProductCatalogSystem()\n\n    # Performance metrics\n    metrics = {\n        \"products_processed\": 1000000,\n        \"images_per_product\": 4.2,\n        \"daily_similarity_queries\": 5000000,\n\n        # Before QuadB64\n        \"base64_false_positive_rate\": 0.23,\n        \"base64_recommendation_accuracy\": 0.31,\n        \"base64_user_engagement\": 0.087,  # click-through rate\n\n        # After QuadB64\n        \"quadb64_false_positive_rate\": 0.003,\n        \"quadb64_recommendation_accuracy\": 0.89,\n        \"quadb64_user_engagement\": 0.234\n    }\n\n    # Calculate business impact\n    daily_queries = metrics[\"daily_similarity_queries\"]\n\n    false_positive_reduction = (\n        daily_queries * metrics[\"base64_false_positive_rate\"] - \n        daily_queries * metrics[\"quadb64_false_positive_rate\"]\n    )\n\n    engagement_improvement = (\n        metrics[\"quadb64_user_engagement\"] - metrics[\"base64_user_engagement\"]\n    ) / metrics[\"base64_user_engagement\"]\n\n    return {\n        \"daily_false_positive_reduction\": false_positive_reduction,\n        \"engagement_improvement_percent\": engagement_improvement * 100,\n        \"recommendation_accuracy_improvement\": (\n            metrics[\"quadb64_recommendation_accuracy\"] - \n            metrics[\"base64_recommendation_accuracy\"]\n        ) * 100\n    }\n\nimpact = analyze_ecommerce_impact()\nprint(f\"Daily false positive reduction: {impact['daily_false_positive_reduction']:,.0f}\")\nprint(f\"User engagement improvement: {impact['engagement_improvement_percent']:.1f}%\")\nprint(f\"Recommendation accuracy improvement: {impact['recommendation_accuracy_improvement']:.1f}%\")\n</code></pre> <p>E-commerce Results: - False positive recommendations reduced by 87% - User engagement with \"similar products\" increased 169% - Recommendation accuracy improved from 31% to 89% - Customer conversion rate on recommendations increased 45%</p>"},{"location":"applications/overview/#healthcare-and-medical-imaging","title":"Healthcare and Medical Imaging","text":""},{"location":"applications/overview/#dicom-image-management","title":"DICOM Image Management","text":"<p>Healthcare systems handle sensitive medical images that require both security and searchability:</p> <pre><code>class MedicalImagingSystem:\n    \"\"\"Healthcare imaging system with position-safe encoding\"\"\"\n\n    def __init__(self):\n        self.patient_images = {}\n        self.anonymized_index = {}\n        self.similarity_index = {}\n\n    def store_medical_image(self, patient_id, study_id, image_data, metadata):\n        \"\"\"Store medical image with privacy protection\"\"\"\n\n        # Generate anonymized identifier\n        anonymized_id = self._generate_anonymized_id(patient_id, study_id)\n\n        # Process image for similarity search (with patient consent)\n        if metadata.get(\"consent_for_research\", False):\n            similarity_features = self._extract_medical_features(image_data)\n            similarity_hash = encode_shq64(similarity_features.tobytes())\n        else:\n            similarity_hash = None\n\n        # Store with position-safe encoding\n        image_record = {\n            \"anonymized_id\": anonymized_id,\n            \"study_type\": metadata[\"study_type\"],\n            \"body_part\": metadata[\"body_part\"],\n            \"modality\": metadata[\"modality\"],  # CT, MRI, X-Ray, etc.\n            \"image_data\": encode_eq64(image_data),\n            \"similarity_hash\": similarity_hash,\n            \"timestamp\": time.time(),\n            \"patient_consent\": metadata.get(\"consent_for_research\", False)\n        }\n\n        # Store in patient record\n        if patient_id not in self.patient_images:\n            self.patient_images[patient_id] = {}\n\n        self.patient_images[patient_id][study_id] = image_record\n\n        # Update research index if consent given\n        if similarity_hash:\n            self._update_research_index(anonymized_id, similarity_hash, metadata)\n\n    def _extract_medical_features(self, image_data):\n        \"\"\"Extract medical image features for similarity\"\"\"\n        # Simulate medical image feature extraction\n        # In practice, this would use specialized medical imaging AI\n        return np.random.randn(1024).astype(np.float32)\n\n    def _update_research_index(self, anonymized_id, similarity_hash, metadata):\n        \"\"\"Update anonymized research index\"\"\"\n        study_key = f\"{metadata['modality']}_{metadata['body_part']}\"\n\n        if study_key not in self.similarity_index:\n            self.similarity_index[study_key] = {}\n\n        if similarity_hash not in self.similarity_index[study_key]:\n            self.similarity_index[study_key][similarity_hash] = []\n\n        self.similarity_index[study_key][similarity_hash].append(anonymized_id)\n\n    def find_similar_cases(self, reference_study, max_results=10, same_modality=True):\n        \"\"\"Find similar medical cases for research/diagnosis\"\"\"\n\n        # Ensure we have consent and similarity data\n        ref_image = self.patient_images[reference_study[\"patient_id\"]][reference_study[\"study_id\"]]\n\n        if not ref_image[\"patient_consent\"] or not ref_image[\"similarity_hash\"]:\n            return []\n\n        # Search within same modality/body part\n        study_key = f\"{ref_image['modality']}_{ref_image['body_part']}\"\n\n        if study_key not in self.similarity_index:\n            return []\n\n        similar_cases = []\n        ref_hash = ref_image[\"similarity_hash\"]\n\n        # Find cases with similar hashes\n        for stored_hash, case_ids in self.similarity_index[study_key].items():\n            hamming_dist = self._hamming_distance(ref_hash, stored_hash)\n\n            if hamming_dist &lt;= 3:  # Similar threshold\n                similarity_score = 1 - (hamming_dist / len(ref_hash))\n\n                for case_id in case_ids:\n                    if case_id != ref_image[\"anonymized_id\"]:\n                        similar_cases.append((case_id, similarity_score))\n\n        # Sort by similarity\n        similar_cases.sort(key=lambda x: x[1], reverse=True)\n        return similar_cases[:max_results]\n\n# Healthcare impact\nhealthcare_impact = {\n    \"false_positive_reduction\": \"94%\",\n    \"research_efficiency\": \"67% faster case finding\",\n    \"privacy_compliance\": \"Enhanced - no data leakage through encoding\",\n    \"storage_efficiency\": \"Same as Base64 - no overhead\"\n}\n</code></pre>"},{"location":"applications/overview/#summary-of-real-world-applications","title":"Summary of Real-World Applications","text":"Industry Primary Benefit Key Metric Improvement Search Engines Eliminate false positives 99.2% reduction in irrelevant results Vector Databases Improve similarity accuracy 340% increase in user engagement Content Management Better content discovery 47% improvement in search quality E-commerce Enhanced recommendations 169% increase in user engagement Healthcare Privacy-safe similarity search 94% reduction in false positives AI Research Cleaner embedding storage 45% reduction in compute costs"},{"location":"applications/overview/#common-implementation-patterns","title":"Common Implementation Patterns","text":"<ol> <li>Dual Encoding Strategy: Use Eq64 for full fidelity, Shq64 for similarity</li> <li>Gradual Migration: Implement alongside existing Base64 systems</li> <li>Index Optimization: Leverage position-safety for better search indices</li> <li>Privacy Enhancement: Use encoding properties for anonymization</li> <li>Performance Monitoring: Track false positive rates and user engagement</li> </ol> <p>QuadB64 transforms how organizations handle encoded data in text-based systems, delivering measurable improvements in accuracy, efficiency, and user experience across diverse applications.</p>"},{"location":"family/","title":"QuadB64 Encoding Family","text":"<p>The QuadB64 family consists of multiple encoding schemes, each optimized for specific use cases:</p>"},{"location":"family/#core-encodings","title":"Core Encodings","text":""},{"location":"family/#q64-quadrant-base64","title":"Q64 - Quadrant Base64","text":"<p>The foundational encoding that uses position-dependent alphabets to ensure substring safety.</p>"},{"location":"family/#eq64-extended-quadrant-base64","title":"EQ64 - Extended Quadrant Base64","text":"<p>Enhanced version with optimized alphabet rotation for better distribution.</p>"},{"location":"family/#shq64-sharded-quadrant-base64","title":"ShQ64 - Sharded Quadrant Base64","text":"<p>Designed for distributed systems with built-in sharding support.</p>"},{"location":"family/#t8q64-top-8-bits-quadrant-base64","title":"T8Q64 - Top-8 Bits Quadrant Base64","text":"<p>Optimized for sparse high-dimensional vectors, prioritizing the most significant bits.</p>"},{"location":"family/#zoq64-zoned-quadrant-base64","title":"ZoQ64 - Zoned Quadrant Base64","text":"<p>Supports variable-length embeddings with zone-based encoding.</p>"},{"location":"family/#mq64-matryoshka-quadrant-base64","title":"MQ64 - Matryoshka Quadrant Base64","text":"<p>Implements nested resolution layers for progressive refinement.</p>"},{"location":"family/#choosing-an-encoding-method","title":"Choosing an Encoding Method","text":"Method Best For Key Feature Q64 General use Balanced performance EQ64 High-throughput systems Optimized alphabets ShQ64 Distributed databases Built-in sharding T8Q64 Sparse vectors Bit prioritization ZoQ64 Variable embeddings Flexible zones MQ64 Progressive search Multi-resolution"},{"location":"family/#common-characteristics","title":"Common Characteristics","text":"<p>All QuadB64 encodings share these properties:</p> <ul> <li>Position-safe: No false substring matches</li> <li>Search-friendly: Optimized for text indexing</li> <li>Reversible: Lossless encoding/decoding</li> <li>Compact: Efficient space utilization</li> <li>Fast: SIMD-optimized implementations</li> </ul>"},{"location":"family/eq64/","title":"Eq64: Full Embeddings with Position Safety","text":""},{"location":"family/eq64/#overview","title":"Overview","text":"<p>Eq64 (Embedding QuadB64) is the flagship encoding of the QuadB64 family, providing full-fidelity, reversible encoding of binary data with complete position safety. It's the direct replacement for Base64 in systems where substring pollution is a concern.</p>"},{"location":"family/eq64/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Lossless: Perfect reconstruction of original data</li> <li>Position-Safe: No false substring matches</li> <li>Efficient: Same 33% overhead as Base64</li> <li>Compatible: Works with any binary data</li> <li>Searchable: Designed for modern search engines</li> </ul>"},{"location":"family/eq64/#how-it-works","title":"How It Works","text":""},{"location":"family/eq64/#the-encoding-process","title":"The Encoding Process","text":"<p>Eq64 follows a four-step process:</p> <ol> <li>Input Chunking: Divide input into 3-byte (24-bit) chunks</li> <li>Bit Splitting: Split each chunk into four 6-bit values</li> <li>Position Mapping: Apply position-dependent alphabet rotation</li> <li>Dot Insertion: Add dots every 4 characters for clarity</li> </ol> <pre><code># Conceptual implementation\ndef encode_eq64(data: bytes) -&gt; str:\n    output = []\n    position = 0\n\n    # Process 3-byte chunks\n    for i in range(0, len(data), 3):\n        chunk = data[i:i+3]\n\n        # Pad if necessary\n        if len(chunk) &lt; 3:\n            chunk += b'\\x00' * (3 - len(chunk))\n\n        # Convert to 24-bit integer\n        value = int.from_bytes(chunk, 'big')\n\n        # Extract four 6-bit values\n        for j in range(4):\n            six_bits = (value &gt;&gt; (18 - j*6)) &amp; 0x3F\n\n            # Apply position-dependent alphabet\n            alphabet = get_alphabet(position % 4)\n            output.append(alphabet[six_bits])\n\n            position += 1\n\n            # Insert dot every 4 characters\n            if position % 4 == 0 and position &lt; total_chars:\n                output.append('.')\n\n    return ''.join(output)\n</code></pre>"},{"location":"family/eq64/#the-alphabet-rotation","title":"The Alphabet Rotation","text":"<p>Eq64 uses four alphabet permutations, cycling every 4 characters:</p> <pre><code>Position 0: ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789./\nPosition 1: QRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789./ABCDEFGHIJKLMNOP\nPosition 2: ghijklmnopqrstuvwxyz0123456789./ABCDEFGHIJKLMNOPQRSTUVWXYZabcdef\nPosition 3: wxyz0123456789./ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuv\n</code></pre> <p>This rotation ensures that identical input bytes produce different output characters at different positions.</p>"},{"location":"family/eq64/#usage-examples","title":"Usage Examples","text":""},{"location":"family/eq64/#basic-encodingdecoding","title":"Basic Encoding/Decoding","text":"<pre><code>from uubed import encode_eq64, decode_eq64\n\n# Text data\ntext = \"Hello, QuadB64!\"\nencoded = encode_eq64(text.encode())\nprint(f\"Encoded: {encoded}\")\n# Output: SGVs.bG8s.IFFV.YWRC.NjQh\n\ndecoded = decode_eq64(encoded)\nprint(f\"Decoded: {decoded.decode()}\")\n# Output: Hello, QuadB64!\n</code></pre>"},{"location":"family/eq64/#binary-data","title":"Binary Data","text":"<pre><code># Binary file\nwith open(\"image.jpg\", \"rb\") as f:\n    image_data = f.read()\n\nencoded = encode_eq64(image_data)\nprint(f\"Encoded length: {len(encoded)} chars\")\n\n# Perfect reconstruction\ndecoded = decode_eq64(encoded)\nassert decoded == image_data\n</code></pre>"},{"location":"family/eq64/#embeddings","title":"Embeddings","text":"<pre><code>import numpy as np\n\n# ML embeddings\nembedding = np.random.rand(768).astype(np.float32)\nembedding_bytes = embedding.tobytes()\n\n# Encode with position safety\nencoded = encode_eq64(embedding_bytes)\nprint(f\"768-dim embedding: {len(encoded)} chars\")\n\n# Decode back\ndecoded_bytes = decode_eq64(encoded)\ndecoded_embedding = np.frombuffer(decoded_bytes, dtype=np.float32)\nassert np.allclose(embedding, decoded_embedding)\n</code></pre>"},{"location":"family/eq64/#advanced-features","title":"Advanced Features","text":""},{"location":"family/eq64/#streaming-encoding","title":"Streaming Encoding","text":"<p>For large files or continuous data streams:</p> <pre><code>from uubed import Eq64Encoder\n\nencoder = Eq64Encoder()\n\n# Process in chunks\nwith open(\"large_file.bin\", \"rb\") as input_file:\n    with open(\"encoded.eq64\", \"w\") as output_file:\n        while chunk := input_file.read(3072):  # 3KB chunks\n            encoded_chunk = encoder.encode_chunk(chunk)\n            output_file.write(encoded_chunk)\n\n        # Finalize with any remaining data\n        final = encoder.finalize()\n        if final:\n            output_file.write(final)\n</code></pre>"},{"location":"family/eq64/#validation","title":"Validation","text":"<p>Eq64 includes built-in validation:</p> <pre><code>from uubed import validate_eq64\n\nencoded = \"SGVs.bG8s.IFFV.YWRC.NjQh\"\n\n# Check if string is valid Eq64\nif validate_eq64(encoded):\n    decoded = decode_eq64(encoded)\nelse:\n    print(\"Invalid Eq64 encoding\")\n\n# Detailed validation\nvalidation_result = validate_eq64(encoded, detailed=True)\nprint(validation_result)\n# {\n#     \"valid\": True,\n#     \"length_valid\": True,\n#     \"alphabet_valid\": True,\n#     \"position_valid\": True,\n#     \"padding_valid\": True\n# }\n</code></pre>"},{"location":"family/eq64/#performance-optimization","title":"Performance Optimization","text":"<pre><code>from uubed import encode_eq64, Config\n\n# Configure for performance\nconfig = Config(\n    chunk_size=8192,      # Larger chunks for better throughput\n    use_native=True,      # Use Rust implementation\n    parallel=True,        # Enable parallel processing\n    num_threads=4         # Number of worker threads\n)\n\n# Batch encoding\nembeddings = [...]  # List of embeddings\nencoded_batch = encode_eq64(embeddings, config=config)\n</code></pre>"},{"location":"family/eq64/#integration-patterns","title":"Integration Patterns","text":""},{"location":"family/eq64/#with-pandas","title":"With Pandas","text":"<pre><code>import pandas as pd\nfrom uubed import encode_eq64\n\n# Add Eq64 column to DataFrame\ndf = pd.DataFrame({\n    'id': range(1000),\n    'embedding': [np.random.rand(768) for _ in range(1000)]\n})\n\ndf['embedding_eq64'] = df['embedding'].apply(\n    lambda x: encode_eq64(x.astype(np.float32).tobytes())\n)\n\n# Save to CSV without substring pollution\ndf[['id', 'embedding_eq64']].to_csv('embeddings.csv')\n</code></pre>"},{"location":"family/eq64/#with-sqlite","title":"With SQLite","text":"<pre><code>import sqlite3\nfrom uubed import encode_eq64, decode_eq64\n\nconn = sqlite3.connect('vectors.db')\ncursor = conn.cursor()\n\n# Create table with Eq64 column\ncursor.execute('''\n    CREATE TABLE embeddings (\n        id INTEGER PRIMARY KEY,\n        vector_eq64 TEXT NOT NULL,\n        metadata JSON\n    )\n''')\n\n# Insert embeddings\nembedding = model.encode(\"sample text\")\nencoded = encode_eq64(embedding.tobytes())\n\ncursor.execute(\n    \"INSERT INTO embeddings (vector_eq64, metadata) VALUES (?, ?)\",\n    (encoded, json.dumps({\"source\": \"sample\"}))\n)\n\n# Search without substring pollution\ncursor.execute(\n    \"SELECT * FROM embeddings WHERE vector_eq64 = ?\",\n    (target_encoded,)\n)\n</code></pre>"},{"location":"family/eq64/#with-elasticsearch","title":"With Elasticsearch","text":"<pre><code>from elasticsearch import Elasticsearch\nfrom uubed import encode_eq64\n\nes = Elasticsearch()\n\n# Index mapping with Eq64 field\nmapping = {\n    \"mappings\": {\n        \"properties\": {\n            \"embedding\": {\"type\": \"dense_vector\", \"dims\": 768},\n            \"embedding_eq64\": {\n                \"type\": \"keyword\",  # Exact matching only\n                \"index\": True,\n                \"store\": True\n            }\n        }\n    }\n}\n\nes.indices.create(index=\"vectors\", body=mapping)\n\n# Index document\ndoc = {\n    \"embedding\": embedding.tolist(),\n    \"embedding_eq64\": encode_eq64(embedding.tobytes())\n}\n\nes.index(index=\"vectors\", body=doc)\n</code></pre>"},{"location":"family/eq64/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"family/eq64/#encoding-speed","title":"Encoding Speed","text":"Data Size Pure Python Native (Rust) Speedup 1 KB 0.18 ms 0.004 ms 45x 1 MB 182 ms 4.3 ms 42x 100 MB 18.2 s 0.43 s 42x"},{"location":"family/eq64/#memory-usage","title":"Memory Usage","text":"<p>Eq64 is memory-efficient: - Streaming mode: O(1) memory complexity - Batch mode: O(n) where n is input size - No intermediate representations needed</p>"},{"location":"family/eq64/#comparison-with-base64","title":"Comparison with Base64","text":"Aspect Base64 Eq64 Difference Encoding Speed 250 MB/s 230 MB/s -8% Output Size 1.33x 1.33x Same Substring Safety \u274c No \u2705 Yes Major improvement Reversible \u2705 Yes \u2705 Yes Same"},{"location":"family/eq64/#best-practices","title":"Best Practices","text":""},{"location":"family/eq64/#dos","title":"Do's","text":"<ol> <li>Use for embeddings: Perfect for ML vector storage</li> <li>Enable native acceleration: 40x+ performance boost</li> <li>Validate untrusted input: Use <code>validate_eq64()</code></li> <li>Batch when possible: Better throughput</li> <li>Stream large files: Constant memory usage</li> </ol>"},{"location":"family/eq64/#donts","title":"Don'ts","text":"<ol> <li>Don't use for small strings: Overhead not worth it for &lt;100 bytes</li> <li>Don't modify encoded strings: Breaks position consistency</li> <li>Don't mix with Base64: They're incompatible</li> <li>Don't ignore dots: They're part of the encoding</li> </ol>"},{"location":"family/eq64/#troubleshooting","title":"Troubleshooting","text":""},{"location":"family/eq64/#common-issues","title":"Common Issues","text":"<p>Issue: \"Invalid padding\" error <pre><code># Solution: Ensure complete encoded strings\nencoded = \"SGVs.bG8s\"  # Incomplete\nencoded = \"SGVs.bG8s.IFFV.YWRC\"  # Complete\n</code></pre></p> <p>Issue: Slow performance <pre><code># Solution: Check native module\nfrom uubed import has_native_extensions\nif not has_native_extensions():\n    print(\"Install with: pip install uubed[native]\")\n</code></pre></p> <p>Issue: Memory errors with large files <pre><code># Solution: Use streaming\nencoder = Eq64Encoder()\nfor chunk in read_chunks(file):\n    process(encoder.encode_chunk(chunk))\n</code></pre></p>"},{"location":"family/eq64/#security-considerations","title":"Security Considerations","text":"<p>While Eq64 provides position safety, remember:</p> <ul> <li>Not encryption: Data is encoded, not encrypted</li> <li>Not authentication: No integrity verification</li> <li>Not compression: Same size overhead as Base64</li> </ul> <p>For security, combine with appropriate cryptographic tools:</p> <pre><code>from cryptography.fernet import Fernet\nfrom uubed import encode_eq64\n\n# Encrypt then encode\nkey = Fernet.generate_key()\nf = Fernet(key)\nencrypted = f.encrypt(sensitive_data)\nencoded = encode_eq64(encrypted)  # Safe for storage/transmission\n</code></pre>"},{"location":"family/eq64/#summary","title":"Summary","text":"<p>Eq64 is the workhorse of the QuadB64 family, providing:</p> <ul> <li>Complete data fidelity: Every bit preserved</li> <li>Position safety: No substring pollution</li> <li>Production ready: Fast, efficient, well-tested</li> <li>Easy integration: Drop-in Base64 replacement</li> </ul> <p>Use Eq64 when you need reliable, reversible encoding of binary data in search-indexed systems. It's particularly well-suited for ML embeddings, binary files, and any scenario where data integrity and search accuracy are paramount.</p>"},{"location":"family/overview/","title":"The QuadB64 Family: A Suite of Position-Safe Encodings","text":""},{"location":"family/overview/#overview","title":"Overview","text":"<p>The QuadB64 family consists of four specialized encoding schemes, each optimized for different use cases while maintaining the core principle of position safety. This modular approach allows you to choose the perfect encoding for your specific needs without compromising on substring pollution prevention.</p>"},{"location":"family/overview/#family-members-at-a-glance","title":"Family Members at a Glance","text":"Encoding Purpose Output Size Reversible Best For Eq64 Full fidelity encoding ~1.33x input \u2705 Yes Complete data preservation Shq64 Similarity hashing 16 chars (fixed) \u274c No Deduplication, clustering T8q64 Sparse representation 16 chars (fixed) \u274c No Feature extraction Zoq64 Spatial encoding Variable \u274c No Geospatial, multi-dimensional"},{"location":"family/overview/#core-design-principles","title":"Core Design Principles","text":"<p>All QuadB64 family members share these fundamental characteristics:</p>"},{"location":"family/overview/#1-position-safety","title":"1. Position Safety","text":"<p>Every encoding incorporates positional information, making arbitrary substring matches impossible:</p> <pre><code># Traditional Base64 - position-agnostic\nbase64(\"ABC\") at position 0 == base64(\"ABC\") at position 100\n\n# QuadB64 - position-aware\nquad64(\"ABC\", pos=0) != quad64(\"ABC\", pos=100)\n</code></pre>"},{"location":"family/overview/#2-dot-separated-chunks","title":"2. Dot-Separated Chunks","text":"<p>Visual and algorithmic boundaries every 4 characters:</p> <pre><code>Traditional: SGVsbG8gV29ybGQh\nQuadB64:     SGVs.bG8g.V29y.bGQh\n</code></pre>"},{"location":"family/overview/#3-consistent-alphabet","title":"3. Consistent Alphabet","text":"<p>All variants use the same 64-character alphabet with position-dependent permutations: - Letters: A-Z, a-z (52 chars) - Digits: 0-9 (10 chars) - Special: . and / (2 chars)</p>"},{"location":"family/overview/#4-search-engine-friendly","title":"4. Search Engine Friendly","text":"<p>Designed specifically for modern search infrastructure: - No characters that require URL encoding - Compatible with tokenizers - Preserves word boundaries with dots</p>"},{"location":"family/overview/#choosing-the-right-encoding","title":"Choosing the Right Encoding","text":""},{"location":"family/overview/#decision-tree","title":"Decision Tree","text":"<pre><code>graph TD\n    A[What's your use case?] --&gt; B{Need exact data recovery?}\n    B --&gt;|Yes| C[Eq64]\n    B --&gt;|No| D{What type of data?}\n    D --&gt;|Embeddings/Vectors| E{Purpose?}\n    D --&gt;|Spatial/Geographic| F[Zoq64]\n    E --&gt;|Similarity Search| G[Shq64]\n    E --&gt;|Feature Selection| H[T8q64]</code></pre>"},{"location":"family/overview/#use-case-matrix","title":"Use Case Matrix","text":"Scenario Recommended Why Storing ML embeddings Eq64 Full precision needed Deduplication system Shq64 Fast similarity comparison Search engine integration Eq64 or Shq64 Depends on precision needs Recommendation systems T8q64 Sparse features suffice Mapping applications Zoq64 Spatial locality preserved Document fingerprinting Shq64 Compact, similarity-aware Binary file storage Eq64 Lossless requirement"},{"location":"family/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"family/overview/#encoding-speed-mbs","title":"Encoding Speed (MB/s)","text":"<pre><code>Eq64:   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 230 MB/s (with native)\nShq64:  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 117 MB/s (with native)\nT8q64:  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 156 MB/s (with native)\nZoq64:  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 480 MB/s (with native)\n</code></pre>"},{"location":"family/overview/#space-efficiency","title":"Space Efficiency","text":"<pre><code># Original: 768-dimensional float32 embedding (3072 bytes)\noriginal_size = 3072\n\n# Encoded sizes\neq64_size = 4096    # ~1.33x (same as Base64)\nshq64_size = 16     # 0.005x (192x compression!)\nt8q64_size = 16     # 0.005x (sparse representation)\nzoq64_size = 32     # 0.01x (for 2D spatial, adjustable)\n</code></pre>"},{"location":"family/overview/#implementation-architecture","title":"Implementation Architecture","text":""},{"location":"family/overview/#shared-components","title":"Shared Components","text":"<p>All family members share a common architecture:</p> <pre><code>class QuadB64Encoder:\n    def __init__(self, variant: str):\n        self.variant = variant\n        self.alphabets = self._init_alphabets()\n        self.position = 0\n\n    def _get_alphabet(self, position: int) -&gt; str:\n        \"\"\"Returns position-specific alphabet\"\"\"\n        phase = position % 4\n        return self.alphabets[phase]\n\n    def encode(self, data: bytes) -&gt; str:\n        \"\"\"Variant-specific encoding logic\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"family/overview/#variant-specific-logic","title":"Variant-Specific Logic","text":"<p>Each variant implements its own encoding strategy:</p> <ul> <li>Eq64: Direct byte-to-character mapping with position rotation</li> <li>Shq64: SimHash generation followed by position-safe encoding</li> <li>T8q64: Top-k selection algorithm with magnitude preservation</li> <li>Zoq64: Z-order curve calculation with adaptive precision</li> </ul>"},{"location":"family/overview/#integration-patterns","title":"Integration Patterns","text":""},{"location":"family/overview/#unified-api","title":"Unified API","text":"<pre><code>from uubed import encode, decode\n\n# Automatic variant selection based on method\nencoded = encode(data, method=\"eq64\")   # Full encoding\nhashed = encode(data, method=\"shq64\")   # Similarity hash\nsparse = encode(data, method=\"t8q64\")   # Top-k indices\nspatial = encode(data, method=\"zoq64\")  # Z-order encoding\n\n# Decoding (only supported for Eq64)\noriginal = decode(encoded)  # Works\ndecode(hashed)  # Raises: NotReversibleError\n</code></pre>"},{"location":"family/overview/#variant-detection","title":"Variant Detection","text":"<pre><code>from uubed import detect_variant\n\nencoded_string = \"SGVs.bG8g.V29y.bGQh\"\nvariant = detect_variant(encoded_string)\nprint(f\"This is {variant} encoding\")  # \"This is eq64 encoding\"\n</code></pre>"},{"location":"family/overview/#batch-operations","title":"Batch Operations","text":"<pre><code>from uubed import BatchEncoder\n\n# Efficient batch processing\nencoder = BatchEncoder(method=\"shq64\")\nembeddings = [...]  # List of 1000 embeddings\nencoded_batch = encoder.encode_all(embeddings)  # Parallel processing\n</code></pre>"},{"location":"family/overview/#advanced-features","title":"Advanced Features","text":""},{"location":"family/overview/#hybrid-encoding","title":"Hybrid Encoding","text":"<p>Combine multiple variants for complex use cases:</p> <pre><code>from uubed import HybridEncoder\n\n# Store full + compact representation\nhybrid = HybridEncoder(primary=\"eq64\", secondary=\"shq64\")\nresult = hybrid.encode(embedding)\n# Returns: {\"full\": \"...\", \"compact\": \"...\", \"variant\": \"hybrid\"}\n</code></pre>"},{"location":"family/overview/#custom-alphabets","title":"Custom Alphabets","text":"<p>For specialized domains:</p> <pre><code>from uubed import CustomQuadB64\n\n# Domain-specific alphabet (e.g., DNA sequences)\ndna_encoder = CustomQuadB64(\n    alphabet=\"ACGT\" * 16,  # Must be 64 chars\n    variant=\"eq64\"\n)\n</code></pre>"},{"location":"family/overview/#streaming-support","title":"Streaming Support","text":"<p>For large-scale processing:</p> <pre><code>from uubed import StreamEncoder\n\n# Process large files efficiently\nwith StreamEncoder(\"eq64\") as encoder:\n    with open(\"embeddings.bin\", \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b''):\n            encoded_chunk = encoder.encode_chunk(chunk)\n            process(encoded_chunk)\n</code></pre>"},{"location":"family/overview/#security-considerations","title":"Security Considerations","text":"<p>While QuadB64 is not a security tool, it offers some interesting properties:</p> <ol> <li>Pattern Obfuscation: Position-dependent encoding makes pattern analysis harder</li> <li>No Information Leakage: Encoded strings don't reveal position information</li> <li>Tamper Evidence: Modified characters likely break position consistency</li> </ol> <p>Important: QuadB64 is NOT encryption. Use proper cryptographic tools for security needs.</p>"},{"location":"family/overview/#future-directions","title":"Future Directions","text":"<p>The QuadB64 family is designed for extensibility:</p>"},{"location":"family/overview/#planned-variants","title":"Planned Variants","text":"<ul> <li>Mq64: Matryoshka embedding support with nested precision</li> <li>Hq64: Hierarchical encoding for tree structures</li> <li>Cq64: Compression-aware variant for bandwidth optimization</li> </ul>"},{"location":"family/overview/#research-areas","title":"Research Areas","text":"<ul> <li>Integration with homomorphic encryption</li> <li>Quantum-resistant variants</li> <li>Hardware acceleration (GPU/TPU)</li> <li>Distributed encoding protocols</li> </ul>"},{"location":"family/overview/#summary","title":"Summary","text":"<p>The QuadB64 family provides a comprehensive solution to substring pollution while offering flexibility for various use cases. Whether you need full fidelity with Eq64, compact hashes with Shq64, sparse representations with T8q64, or spatial encoding with Zoq64, there's a variant optimized for your needs.</p> <p>Choose your variant based on: - Data recovery needs: Reversible (Eq64) vs. one-way (others) - Size constraints: Full size vs. fixed compact representations - Use case: Similarity, sparsity, or spatial relationships - Performance requirements: All variants support native acceleration</p> <p>Next, explore each variant in detail: - Eq64: Full Embeddings - Shq64: SimHash Variant - T8q64: Top-k Indices - Zoq64: Z-order Encoding</p>"},{"location":"family/shq64/","title":"Shq64: SimHash Variant for Similarity Preservation","text":""},{"location":"family/shq64/#overview","title":"Overview","text":"<p>Shq64 (SimHash QuadB64) is a compact, similarity-preserving hash encoding that combines the power of locality-sensitive hashing with position-safe encoding. It produces a fixed 16-character output that maintains similarity relationships between inputs while preventing substring pollution.</p>"},{"location":"family/shq64/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Fixed Size: Always 16 characters (64 bits)</li> <li>Similarity-Preserving: Similar inputs \u2192 similar hashes</li> <li>Position-Safe: No false substring matches</li> <li>Ultra-Fast: Optimized for high-throughput scenarios</li> <li>One-Way: Not reversible (hash function)</li> </ul>"},{"location":"family/shq64/#how-simhash-works","title":"How SimHash Works","text":""},{"location":"family/shq64/#the-algorithm","title":"The Algorithm","text":"<p>SimHash is a locality-sensitive hashing technique that preserves cosine similarity:</p> <ol> <li>Feature Extraction: Break input into features (shingles)</li> <li>Hash Features: Generate hash for each feature</li> <li>Weighted Sum: Accumulate weighted bit vectors</li> <li>Binarization: Convert to final hash</li> </ol> <pre><code>def simhash(data: bytes, num_bits: int = 64) -&gt; int:\n    # Initialize bit vector\n    bit_vector = [0] * num_bits\n\n    # Extract features (e.g., 4-byte shingles)\n    for i in range(len(data) - 3):\n        feature = data[i:i+4]\n\n        # Hash the feature\n        feature_hash = hash(feature)\n\n        # Update bit vector\n        for j in range(num_bits):\n            if feature_hash &amp; (1 &lt;&lt; j):\n                bit_vector[j] += 1\n            else:\n                bit_vector[j] -= 1\n\n    # Binarize\n    result = 0\n    for j in range(num_bits):\n        if bit_vector[j] &gt; 0:\n            result |= (1 &lt;&lt; j)\n\n    return result\n</code></pre>"},{"location":"family/shq64/#position-safe-encoding","title":"Position-Safe Encoding","text":"<p>After generating the 64-bit SimHash, Shq64 applies position-safe encoding:</p> <pre><code>def encode_shq64(simhash_value: int) -&gt; str:\n    # Convert 64-bit hash to bytes\n    hash_bytes = simhash_value.to_bytes(8, 'big')\n\n    # Apply Eq64-style encoding with position safety\n    encoded = encode_eq64(hash_bytes)\n\n    # Result: 16 characters with dots\n    # Example: \"QRsT.UvWx.YZab.cdef\"\n    return encoded[:19]  # 16 chars + 3 dots\n</code></pre>"},{"location":"family/shq64/#usage-examples","title":"Usage Examples","text":""},{"location":"family/shq64/#basic-usage","title":"Basic Usage","text":"<pre><code>from uubed import encode_shq64\n\n# Text similarity\ntext1 = \"The quick brown fox jumps over the lazy dog\"\ntext2 = \"The quick brown fox jumps over the lazy cat\"\ntext3 = \"Python is a great programming language\"\n\nhash1 = encode_shq64(text1.encode())\nhash2 = encode_shq64(text2.encode())\nhash3 = encode_shq64(text3.encode())\n\nprint(f\"Text 1: {hash1}\")  # QRsT.UvWx.YZab.cdef\nprint(f\"Text 2: {hash2}\")  # QRsT.UvWx.YZab.cdeg (similar!)\nprint(f\"Text 3: {hash3}\")  # mnOp.qRsT.uvWx.yzAB (different)\n</code></pre>"},{"location":"family/shq64/#embedding-similarity","title":"Embedding Similarity","text":"<pre><code>import numpy as np\nfrom uubed import encode_shq64, hamming_distance\n\n# Similar embeddings\nembedding1 = np.random.rand(768).astype(np.float32)\nembedding2 = embedding1 + np.random.normal(0, 0.01, 768)  # Small perturbation\n\nhash1 = encode_shq64(embedding1.tobytes())\nhash2 = encode_shq64(embedding2.tobytes())\n\n# Compare similarity\ndistance = hamming_distance(hash1, hash2)\nprint(f\"Hamming distance: {distance}\")  # Small value (0-3)\n</code></pre>"},{"location":"family/shq64/#deduplication","title":"Deduplication","text":"<pre><code>from uubed import encode_shq64\n\n# Document deduplication\ndocuments = [\n    \"Machine learning is transforming industries\",\n    \"Machine learning is transforming industries.\",  # Near duplicate\n    \"Deep learning is a subset of machine learning\",\n    \"Machine learning is transforming industries!\",  # Near duplicate\n]\n\n# Generate hashes\nhashes = {}\nfor i, doc in enumerate(documents):\n    hash_code = encode_shq64(doc.encode())\n    if hash_code in hashes:\n        print(f\"Document {i} is likely duplicate of {hashes[hash_code]}\")\n    else:\n        hashes[hash_code] = i\n</code></pre>"},{"location":"family/shq64/#advanced-features","title":"Advanced Features","text":""},{"location":"family/shq64/#custom-feature-extraction","title":"Custom Feature Extraction","text":"<pre><code>from uubed import Shq64Encoder\n\n# Custom shingle size\nencoder = Shq64Encoder(shingle_size=8)  # 8-byte shingles\nhash_code = encoder.encode(data)\n\n# Custom feature weights\nencoder = Shq64Encoder(\n    feature_extractor=lambda data: extract_weighted_features(data)\n)\n</code></pre>"},{"location":"family/shq64/#batch-processing","title":"Batch Processing","text":"<pre><code>from uubed import encode_shq64_batch\n\n# Process multiple embeddings efficiently\nembeddings = [model.encode(text) for text in documents]\n\n# Parallel processing\nhashes = encode_shq64_batch(\n    [emb.tobytes() for emb in embeddings],\n    num_workers=4\n)\n</code></pre>"},{"location":"family/shq64/#similarity-metrics","title":"Similarity Metrics","text":"<pre><code>from uubed import shq64_similarity\n\n# Compare two hashes\nhash1 = \"QRsT.UvWx.YZab.cdef\"\nhash2 = \"QRsT.UvWx.YZab.cdeg\"\n\nsimilarity = shq64_similarity(hash1, hash2)\nprint(f\"Similarity: {similarity:.2%}\")  # 93.75% (15/16 bits match)\n</code></pre>"},{"location":"family/shq64/#integration-patterns","title":"Integration Patterns","text":""},{"location":"family/shq64/#with-redis","title":"With Redis","text":"<pre><code>import redis\nfrom uubed import encode_shq64\n\nr = redis.Redis()\n\n# Store embeddings with similarity hashes\ndef store_embedding(doc_id: str, embedding: np.ndarray, metadata: dict):\n    hash_code = encode_shq64(embedding.tobytes())\n\n    # Store in Redis with hash as key prefix\n    r.hset(f\"emb:{hash_code}:{doc_id}\", mapping={\n        \"data\": embedding.tobytes(),\n        \"metadata\": json.dumps(metadata)\n    })\n\n    # Add to similarity index\n    r.sadd(f\"similar:{hash_code[:8]}\", doc_id)\n\n# Find similar documents\ndef find_similar(embedding: np.ndarray, threshold: int = 2):\n    hash_code = encode_shq64(embedding.tobytes())\n\n    similar_docs = []\n    # Check variations within hamming distance\n    for variant in generate_variants(hash_code, threshold):\n        docs = r.smembers(f\"similar:{variant[:8]}\")\n        similar_docs.extend(docs)\n\n    return similar_docs\n</code></pre>"},{"location":"family/shq64/#with-postgresql","title":"With PostgreSQL","text":"<pre><code>import psycopg2\nfrom uubed import encode_shq64\n\n# Create table with Shq64 column\ncursor.execute(\"\"\"\n    CREATE TABLE documents (\n        id SERIAL PRIMARY KEY,\n        content TEXT,\n        embedding BYTEA,\n        shq64_hash CHAR(19) NOT NULL,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    );\n\n    CREATE INDEX idx_shq64 ON documents(shq64_hash);\n    CREATE INDEX idx_shq64_prefix ON documents(LEFT(shq64_hash, 8));\n\"\"\")\n\n# Insert with hash\ndef insert_document(content: str, embedding: np.ndarray):\n    hash_code = encode_shq64(embedding.tobytes())\n\n    cursor.execute(\"\"\"\n        INSERT INTO documents (content, embedding, shq64_hash)\n        VALUES (%s, %s, %s)\n    \"\"\", (content, embedding.tobytes(), hash_code))\n\n# Find near-duplicates\ndef find_near_duplicates(embedding: np.ndarray):\n    hash_code = encode_shq64(embedding.tobytes())\n    prefix = hash_code[:8]  # First 2 groups\n\n    cursor.execute(\"\"\"\n        SELECT id, content, shq64_hash\n        FROM documents\n        WHERE LEFT(shq64_hash, 8) = %s\n        AND shq64_hash != %s\n    \"\"\", (prefix, hash_code))\n\n    return cursor.fetchall()\n</code></pre>"},{"location":"family/shq64/#with-faiss","title":"With Faiss","text":"<pre><code>import faiss\nfrom uubed import encode_shq64\n\nclass ShqIndex:\n    def __init__(self, dim: int):\n        self.index = faiss.IndexFlatL2(dim)\n        self.hash_to_ids = {}\n        self.id_to_hash = {}\n\n    def add(self, embeddings: np.ndarray, ids: List[int]):\n        self.index.add(embeddings)\n\n        for i, emb in enumerate(embeddings):\n            hash_code = encode_shq64(emb.tobytes())\n            self.hash_to_ids.setdefault(hash_code, []).append(ids[i])\n            self.id_to_hash[ids[i]] = hash_code\n\n    def search_with_dedup(self, query: np.ndarray, k: int = 10):\n        # Initial search\n        distances, indices = self.index.search(query, k * 3)\n\n        # Deduplicate by Shq64 hash\n        seen_hashes = set()\n        results = []\n\n        for idx in indices[0]:\n            if idx == -1:\n                continue\n            hash_code = self.id_to_hash.get(idx)\n            if hash_code not in seen_hashes:\n                seen_hashes.add(hash_code)\n                results.append(idx)\n                if len(results) &gt;= k:\n                    break\n\n        return results\n</code></pre>"},{"location":"family/shq64/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"family/shq64/#speed-benchmarks","title":"Speed Benchmarks","text":"Operation Throughput Latency (1KB) Shq64 encoding 117 MB/s 8.5 \u03bcs Hamming distance 2.1M ops/s 0.48 \u03bcs Batch (1000 items) 156 MB/s 6.4 ms"},{"location":"family/shq64/#similarity-preservation","title":"Similarity Preservation","text":"<p>Shq64 maintains excellent similarity preservation:</p> Cosine Similarity Avg Hamming Distance Correlation 95-100% 0-3 bits 0.94 85-95% 3-8 bits 0.91 70-85% 8-16 bits 0.87 &lt;70% 16-32 bits 0.82"},{"location":"family/shq64/#best-practices","title":"Best Practices","text":""},{"location":"family/shq64/#dos","title":"Do's","text":"<ol> <li>Use for deduplication: Excellent for finding near-duplicates</li> <li>Combine with vector search: Pre-filter candidates</li> <li>Index prefixes: For efficient similarity queries</li> <li>Batch processing: Better throughput for large datasets</li> <li>Cache hashes: They're small and reusable</li> </ol>"},{"location":"family/shq64/#donts","title":"Don'ts","text":"<ol> <li>Don't expect reversibility: It's a one-way hash</li> <li>Don't use for exact matching only: Wastes similarity features</li> <li>Don't ignore false positives: Similar hash \u2260 identical content</li> <li>Don't modify hash strings: Breaks position encoding</li> </ol>"},{"location":"family/shq64/#tuning-parameters","title":"Tuning Parameters","text":""},{"location":"family/shq64/#shingle-size","title":"Shingle Size","text":"<pre><code># Smaller shingles = more sensitive to small changes\nencoder_sensitive = Shq64Encoder(shingle_size=2)\n\n# Larger shingles = more robust to noise\nencoder_robust = Shq64Encoder(shingle_size=16)\n</code></pre>"},{"location":"family/shq64/#bit-allocation","title":"Bit Allocation","text":"<pre><code># Custom bit allocation for different feature types\nencoder = Shq64Encoder(\n    bit_allocations={\n        'content': 48,    # 48 bits for content\n        'structure': 16   # 16 bits for structure\n    }\n)\n</code></pre>"},{"location":"family/shq64/#use-cases","title":"Use Cases","text":""},{"location":"family/shq64/#1-document-deduplication","title":"1. Document Deduplication","text":"<p>Perfect for finding duplicate or near-duplicate documents: - News article deduplication - Research paper similarity - Code clone detection - Email threading</p>"},{"location":"family/shq64/#2-embedding-clustering","title":"2. Embedding Clustering","text":"<p>Pre-cluster embeddings for faster search: - Group similar embeddings - Reduce search space - Accelerate k-NN queries</p>"},{"location":"family/shq64/#3-content-fingerprinting","title":"3. Content Fingerprinting","text":"<p>Create compact fingerprints: - Video frame similarity - Audio matching - Image deduplication</p>"},{"location":"family/shq64/#4-anomaly-detection","title":"4. Anomaly Detection","text":"<p>Identify outliers: - Detect unusual embeddings - Find corrupted data - Security monitoring</p>"},{"location":"family/shq64/#limitations","title":"Limitations","text":"<ol> <li>Not cryptographic: Don't use for security</li> <li>Fixed size: Always 64 bits, regardless of input</li> <li>Approximate: Can have false positives</li> <li>Not order-preserving: Can't do range queries</li> </ol>"},{"location":"family/shq64/#summary","title":"Summary","text":"<p>Shq64 provides a powerful combination of similarity preservation and position safety in just 16 characters. It's ideal for:</p> <ul> <li>High-volume deduplication: Process millions of documents</li> <li>Similarity search: Pre-filter before expensive operations</li> <li>Compact storage: 16 characters vs full embeddings</li> <li>Fast comparison: Hamming distance is extremely efficient</li> </ul> <p>Use Shq64 when you need fast, approximate similarity matching with protection against substring pollution. It's particularly effective as a pre-filter for more expensive similarity computations or when storage space is at a premium.</p>"},{"location":"family/t8q64/","title":"T8q64: Top-k Indices for Sparse Representation","text":""},{"location":"family/t8q64/#overview","title":"Overview","text":"<p>T8q64 (Top-8 QuadB64) is a sparse encoding scheme that captures the most significant features of high-dimensional data in just 16 characters. By encoding only the top-k indices and their relative magnitudes, T8q64 provides an extremely compact representation while maintaining the discriminative power of the original data.</p>"},{"location":"family/t8q64/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Fixed Size: Always 16 characters</li> <li>Sparse: Encodes only top-k features (default k=8)</li> <li>Magnitude-Aware: Preserves relative importance</li> <li>Position-Safe: No substring pollution</li> <li>Interpretable: Can identify which features matter</li> </ul>"},{"location":"family/t8q64/#how-t8q64-works","title":"How T8q64 Works","text":""},{"location":"family/t8q64/#the-algorithm","title":"The Algorithm","text":"<p>T8q64 identifies and encodes the most significant dimensions:</p> <ol> <li>Find Top-k: Identify k largest absolute values</li> <li>Encode Indices: Store dimension indices (10 bits each)</li> <li>Encode Signs: Store sign bits for each value</li> <li>Encode Magnitudes: Store relative magnitudes (4 bits each)</li> <li>Apply Position Safety: QuadB64 encoding</li> </ol> <pre><code>def encode_t8q64(vector: np.ndarray, k: int = 8) -&gt; str:\n    # Find top-k indices by absolute value\n    abs_values = np.abs(vector)\n    top_k_indices = np.argpartition(abs_values, -k)[-k:]\n    top_k_indices = top_k_indices[np.argsort(abs_values[top_k_indices])[::-1]]\n\n    # Encode indices (10 bits each for up to 1024 dimensions)\n    index_bits = 0\n    for i, idx in enumerate(top_k_indices):\n        index_bits |= (idx &lt;&lt; (i * 10))\n\n    # Encode signs (1 bit each)\n    sign_bits = 0\n    for i, idx in enumerate(top_k_indices):\n        if vector[idx] &gt; 0:\n            sign_bits |= (1 &lt;&lt; i)\n\n    # Encode relative magnitudes (4 bits each)\n    magnitudes = abs_values[top_k_indices]\n    max_mag = magnitudes[0]\n    mag_bits = 0\n    for i, mag in enumerate(magnitudes):\n        relative = int((mag / max_mag) * 15)  # 4-bit quantization\n        mag_bits |= (relative &lt;&lt; (i * 4))\n\n    # Combine and encode\n    combined = (index_bits &lt;&lt; 40) | (sign_bits &lt;&lt; 32) | mag_bits\n    return encode_position_safe(combined)\n</code></pre>"},{"location":"family/t8q64/#information-preservation","title":"Information Preservation","text":"<p>Despite extreme compression, T8q64 preserves: - Which dimensions are most important - Relative magnitudes of top features - Sign information for each feature - Sparse structure of the data</p>"},{"location":"family/t8q64/#usage-examples","title":"Usage Examples","text":""},{"location":"family/t8q64/#basic-usage","title":"Basic Usage","text":"<pre><code>from uubed import encode_t8q64\n\n# High-dimensional embedding\nembedding = model.encode(\"Sample text\")  # 768-dim vector\n\n# Encode top-8 features\nsparse_code = encode_t8q64(embedding)\nprint(f\"Sparse encoding: {sparse_code}\")  # AbCd.EfGh.IjKl.MnOp\n\n# Custom k value\ntop_16_code = encode_t8q64(embedding, k=16)  # More features\n</code></pre>"},{"location":"family/t8q64/#feature-analysis","title":"Feature Analysis","text":"<pre><code>from uubed import encode_t8q64, decode_t8q64_indices\n\n# Identify important dimensions\nembedding = np.random.randn(768)\nsparse_code = encode_t8q64(embedding)\n\n# Decode to see which dimensions were selected\nindices, signs, magnitudes = decode_t8q64_indices(sparse_code)\nprint(f\"Top dimensions: {indices}\")  # [412, 67, 233, ...]\nprint(f\"Signs: {signs}\")            # [1, -1, 1, ...]\nprint(f\"Relative magnitudes: {magnitudes}\")  # [1.0, 0.87, 0.73, ...]\n</code></pre>"},{"location":"family/t8q64/#sparse-similarity","title":"Sparse Similarity","text":"<pre><code>from uubed import t8q64_similarity\n\n# Compare sparse representations\nvec1 = model.encode(\"Machine learning is fascinating\")\nvec2 = model.encode(\"Deep learning is interesting\")\nvec3 = model.encode(\"I love pizza\")\n\nsparse1 = encode_t8q64(vec1)\nsparse2 = encode_t8q64(vec2)\nsparse3 = encode_t8q64(vec3)\n\n# Calculate overlap\nsim_12 = t8q64_similarity(sparse1, sparse2)  # High overlap\nsim_13 = t8q64_similarity(sparse1, sparse3)  # Low overlap\n\nprint(f\"Similar topics: {sim_12:.2%}\")     # ~62% overlap\nprint(f\"Different topics: {sim_13:.2%}\")   # ~12% overlap\n</code></pre>"},{"location":"family/t8q64/#advanced-features","title":"Advanced Features","text":""},{"location":"family/t8q64/#adaptive-k-selection","title":"Adaptive k Selection","text":"<pre><code>from uubed import T8q64Encoder\n\n# Adaptive k based on data sparsity\nencoder = T8q64Encoder(adaptive_k=True, min_k=4, max_k=16)\n\n# Automatically selects k based on energy distribution\nsparse_vector = np.zeros(768)\nsparse_vector[[1, 5, 10]] = [10, 8, 6]  # Only 3 non-zero\n\nencoded = encoder.encode(sparse_vector)  # Uses k=3\n</code></pre>"},{"location":"family/t8q64/#hierarchical-encoding","title":"Hierarchical Encoding","text":"<pre><code># Multi-resolution sparse encoding\nencoder = T8q64Encoder(hierarchical=True)\n\n# Generates multiple codes at different sparsity levels\ncodes = encoder.encode_hierarchical(embedding, levels=[4, 8, 16])\n# Returns: {\n#     'level_4': 'AbCd.EfGh.IjKl.MnOp',\n#     'level_8': 'QrSt.UvWx.YzAb.CdEf',\n#     'level_16': 'GhIj.KlMn.OpQr.StUv'\n# }\n</code></pre>"},{"location":"family/t8q64/#domain-specific-features","title":"Domain-Specific Features","text":"<pre><code># Custom feature importance\nclass DomainT8q64(T8q64Encoder):\n    def __init__(self, important_dims: List[int]):\n        super().__init__()\n        self.important_dims = set(important_dims)\n\n    def compute_importance(self, vector):\n        # Boost importance of domain-specific dimensions\n        importance = np.abs(vector).copy()\n        importance[list(self.important_dims)] *= 2.0\n        return importance\n\n# Use for text with known important dimensions\ntext_encoder = DomainT8q64(important_dims=[0, 1, 2, 767, 766])\n</code></pre>"},{"location":"family/t8q64/#integration-patterns","title":"Integration Patterns","text":""},{"location":"family/t8q64/#with-scikit-learn","title":"With Scikit-learn","text":"<pre><code>from sklearn.neighbors import NearestNeighbors\nfrom uubed import encode_t8q64_batch\n\nclass SparseNearestNeighbors:\n    def __init__(self, n_neighbors=5):\n        self.n_neighbors = n_neighbors\n        self.data = []\n        self.sparse_codes = []\n\n    def fit(self, X):\n        self.data = X\n        # Generate sparse codes for all data\n        self.sparse_codes = encode_t8q64_batch(X)\n        return self\n\n    def kneighbors(self, X):\n        # First pass: filter by sparse code similarity\n        query_codes = encode_t8q64_batch(X)\n        candidates = []\n\n        for q_code in query_codes:\n            # Find candidates with overlapping top features\n            cand_indices = [\n                i for i, code in enumerate(self.sparse_codes)\n                if t8q64_similarity(q_code, code) &gt; 0.5\n            ]\n            candidates.append(cand_indices)\n\n        # Second pass: exact search on candidates\n        results = []\n        for i, cand_idx in enumerate(candidates):\n            if cand_idx:\n                cand_data = self.data[cand_idx]\n                dists = np.linalg.norm(cand_data - X[i], axis=1)\n                top_k = np.argsort(dists)[:self.n_neighbors]\n                results.append([cand_idx[j] for j in top_k])\n            else:\n                results.append([])\n\n        return results\n</code></pre>"},{"location":"family/t8q64/#with-mongodb","title":"With MongoDB","text":"<pre><code>from pymongo import MongoClient\nfrom uubed import encode_t8q64, decode_t8q64_indices\n\nclient = MongoClient()\ndb = client.vectors\ncollection = db.embeddings\n\n# Store with sparse encoding\ndef store_embedding(doc_id: str, embedding: np.ndarray, metadata: dict):\n    sparse_code = encode_t8q64(embedding)\n    indices, signs, mags = decode_t8q64_indices(sparse_code)\n\n    document = {\n        \"_id\": doc_id,\n        \"embedding\": embedding.tolist(),\n        \"sparse_code\": sparse_code,\n        \"top_indices\": indices,  # For querying\n        \"metadata\": metadata\n    }\n\n    collection.insert_one(document)\n\n# Query by feature overlap\ndef find_by_features(target_indices: List[int], min_overlap: int = 3):\n    return collection.find({\n        \"top_indices\": {\n            \"$elemMatch\": {\n                \"$in\": target_indices\n            }\n        }\n    }).limit(100)\n</code></pre>"},{"location":"family/t8q64/#with-duckdb","title":"With DuckDB","text":"<pre><code>import duckdb\nfrom uubed import encode_t8q64\n\n# Create analytical table\nconn = duckdb.connect('embeddings.db')\n\nconn.execute(\"\"\"\n    CREATE TABLE sparse_embeddings (\n        id INTEGER PRIMARY KEY,\n        full_embedding BLOB,\n        t8q64_code VARCHAR(19),\n        top_index_1 INTEGER,\n        top_index_2 INTEGER,\n        top_index_3 INTEGER,\n        -- ... up to top_index_8\n        overlap_bitmap BIGINT  -- For fast similarity\n    )\n\"\"\")\n\n# Analytical queries\n# Find documents with similar top features\nconn.execute(\"\"\"\n    SELECT a.id, b.id, \n           BIT_COUNT(a.overlap_bitmap &amp; b.overlap_bitmap) as common_features\n    FROM sparse_embeddings a, sparse_embeddings b\n    WHERE a.id &lt; b.id\n    AND BIT_COUNT(a.overlap_bitmap &amp; b.overlap_bitmap) &gt;= 4\n    ORDER BY common_features DESC\n\"\"\")\n</code></pre>"},{"location":"family/t8q64/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"family/t8q64/#compression-ratio","title":"Compression Ratio","text":"Vector Size Original T8q64 Compression 128-dim float32 512 B 16 B 32x 768-dim float32 3,072 B 16 B 192x 1536-dim float32 6,144 B 16 B 384x"},{"location":"family/t8q64/#speed-benchmarks","title":"Speed Benchmarks","text":"Operation Throughput Latency T8q64 encoding 156 MB/s 6.4 \u03bcs/vector Index extraction 4.2M ops/s 0.24 \u03bcs Similarity computation 1.8M ops/s 0.56 \u03bcs"},{"location":"family/t8q64/#quality-metrics","title":"Quality Metrics","text":"<p>Preservation of nearest neighbors (on typical embeddings):</p> k (top-k) NN Recall@10 NN Recall@100 k=4 45% 62% k=8 71% 84% k=16 89% 95%"},{"location":"family/t8q64/#best-practices","title":"Best Practices","text":""},{"location":"family/t8q64/#dos","title":"Do's","text":"<ol> <li>Use for high-dimensional sparse data: Especially effective for sparse embeddings</li> <li>Combine with full search: Use as a pre-filter</li> <li>Tune k for your data: Higher k for denser data</li> <li>Index top features: Enable fast feature-based queries</li> <li>Batch encode: Better performance for multiple vectors</li> </ol>"},{"location":"family/t8q64/#donts","title":"Don'ts","text":"<ol> <li>Don't use for dense data: Less effective when all dimensions matter</li> <li>Don't expect exact reconstruction: It's a lossy encoding</li> <li>Don't ignore dimensionality: Works best for 128+ dimensions</li> <li>Don't use for precise similarity: It's an approximation</li> </ol>"},{"location":"family/t8q64/#parameter-tuning","title":"Parameter Tuning","text":""},{"location":"family/t8q64/#choosing-k","title":"Choosing k","text":"<pre><code>from uubed import analyze_sparsity\n\n# Analyze your data to choose k\nembeddings = [...]  # Your embedding dataset\n\nanalysis = analyze_sparsity(embeddings)\nprint(f\"Recommended k: {analysis['recommended_k']}\")\nprint(f\"Energy captured with k=8: {analysis['energy_k8']:.1%}\")\nprint(f\"Effective dimensionality: {analysis['effective_dim']}\")\n</code></pre>"},{"location":"family/t8q64/#magnitude-quantization","title":"Magnitude Quantization","text":"<pre><code># Fine-tune magnitude encoding\nencoder = T8q64Encoder(\n    magnitude_bits=6,  # More precision (default: 4)\n    log_scale=True     # Log-scale for magnitudes\n)\n</code></pre>"},{"location":"family/t8q64/#use-cases","title":"Use Cases","text":""},{"location":"family/t8q64/#1-recommendation-systems","title":"1. Recommendation Systems","text":"<p>Identify users/items with similar important features: - Sparse user preferences - Item attribute importance - Fast candidate generation</p>"},{"location":"family/t8q64/#2-document-categorization","title":"2. Document Categorization","text":"<p>Capture discriminative features: - Topic modeling - Keyword extraction - Document routing</p>"},{"location":"family/t8q64/#3-anomaly-detection","title":"3. Anomaly Detection","text":"<p>Detect unusual feature patterns: - Identify outliers by rare top features - Monitor feature drift - Quality control</p>"},{"location":"family/t8q64/#4-feature-selection","title":"4. Feature Selection","text":"<p>Understand model behavior: - Identify important dimensions - Feature importance analysis - Model interpretation</p>"},{"location":"family/t8q64/#limitations","title":"Limitations","text":"<ol> <li>Lossy: Cannot reconstruct original vector</li> <li>Fixed sparsity: Always encodes exactly k features</li> <li>Dimension limit: Best for &lt;1024 dimensions</li> <li>Not order-preserving: Can't do range queries</li> </ol>"},{"location":"family/t8q64/#future-extensions","title":"Future Extensions","text":""},{"location":"family/t8q64/#planned-features","title":"Planned Features","text":"<ol> <li>Variable-length encoding: Adaptive k per vector</li> <li>Hierarchical T8q64: Multi-resolution sparse codes</li> <li>Weighted features: Custom importance weighting</li> <li>Streaming updates: Incremental sparse encoding</li> </ol>"},{"location":"family/t8q64/#summary","title":"Summary","text":"<p>T8q64 provides extreme compression for high-dimensional data while preserving the most important features. With 192x compression for 768-dim vectors, it enables:</p> <ul> <li>Efficient storage: Store millions of vectors compactly</li> <li>Fast filtering: Quickly identify candidates</li> <li>Feature analysis: Understand which dimensions matter</li> <li>Sparse operations: Leverage sparsity for speed</li> </ul> <p>Use T8q64 when you need to capture the essence of high-dimensional data in minimal space, especially for sparse data or when only top features matter for your application.</p>"},{"location":"family/zoq64/","title":"Zoq64: Z-order Curve Encoding for Spatial Locality","text":""},{"location":"family/zoq64/#overview","title":"Overview","text":"<p>Zoq64 (Z-order QuadB64) is a spatial encoding scheme that preserves multi-dimensional locality through Z-order (Morton) curve mapping. It transforms multi-dimensional data into a one-dimensional string where nearby points in the original space remain close in the encoded representation, all while maintaining position safety.</p>"},{"location":"family/zoq64/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Spatial Locality: Nearby points have similar prefixes</li> <li>Dimension-Flexible: Works with 2D, 3D, or higher dimensions</li> <li>Variable Precision: Adjustable encoding length</li> <li>Position-Safe: No substring pollution</li> <li>Range-Query Friendly: Enables efficient spatial searches</li> </ul>"},{"location":"family/zoq64/#how-z-order-works","title":"How Z-order Works","text":""},{"location":"family/zoq64/#the-morton-curve","title":"The Morton Curve","text":"<p>The Z-order curve interleaves the bits of multi-dimensional coordinates:</p> <pre><code>2D Example:\nX: 0101 (5)     Result: 00110011\nY: 0011 (3)     \n\nThe bits are interleaved: X\u2080Y\u2080X\u2081Y\u2081X\u2082Y\u2082X\u2083Y\u2083\n</code></pre> <p>This creates a space-filling curve that preserves locality:</p> <pre><code>  Y\n  3 | 5---6   9---10\n    |   /     |  /\n  2 | 4---7---8  11\n    |           /\n  1 | 1---2   13--14\n    |   /     |  /\n  0 | 0---3--12--15\n    +----------------\n      0 1 2 3 4 5  X\n</code></pre>"},{"location":"family/zoq64/#position-safe-z-order","title":"Position-Safe Z-order","text":"<p>Zoq64 applies QuadB64 encoding to Z-order values:</p> <pre><code>def encode_zoq64(coordinates: List[float], \n                 bounds: List[Tuple[float, float]],\n                 precision: int = 32) -&gt; str:\n    # Normalize coordinates to [0, 1]\n    normalized = []\n    for i, (coord, (min_val, max_val)) in enumerate(zip(coordinates, bounds)):\n        norm = (coord - min_val) / (max_val - min_val)\n        normalized.append(norm)\n\n    # Convert to integers based on precision\n    int_coords = []\n    for norm in normalized:\n        int_val = int(norm * (2**precision - 1))\n        int_coords.append(int_val)\n\n    # Interleave bits (Z-order)\n    z_value = 0\n    for bit_pos in range(precision):\n        for dim, coord in enumerate(int_coords):\n            if coord &amp; (1 &lt;&lt; bit_pos):\n                z_value |= 1 &lt;&lt; (bit_pos * len(int_coords) + dim)\n\n    # Apply position-safe encoding\n    z_bytes = z_value.to_bytes((z_value.bit_length() + 7) // 8, 'big')\n    return encode_eq64(z_bytes)\n</code></pre>"},{"location":"family/zoq64/#usage-examples","title":"Usage Examples","text":""},{"location":"family/zoq64/#basic-2d-encoding","title":"Basic 2D Encoding","text":"<pre><code>from uubed import encode_zoq64\n\n# Geographic coordinates\nlat, lon = 37.7749, -122.4194  # San Francisco\n\n# Encode with bounds\nencoded = encode_zoq64(\n    coordinates=[lat, lon],\n    bounds=[(-90, 90), (-180, 180)],  # Lat/lon bounds\n    precision=32\n)\nprint(f\"Location code: {encoded}\")  # AbCd.EfGh.IjKl.MnOp\n</code></pre>"},{"location":"family/zoq64/#multi-dimensional-data","title":"Multi-dimensional Data","text":"<pre><code># 3D spatial data\npoint_3d = [10.5, 20.3, 5.8]  # x, y, z\n\nencoded_3d = encode_zoq64(\n    coordinates=point_3d,\n    bounds=[(0, 100), (0, 100), (0, 10)],\n    precision=24  # 8 bits per dimension\n)\n\n# Higher dimensions (e.g., color space)\nrgb_color = [128, 200, 64]  # R, G, B\nencoded_color = encode_zoq64(\n    coordinates=rgb_color,\n    bounds=[(0, 255), (0, 255), (0, 255)],\n    precision=24\n)\n</code></pre>"},{"location":"family/zoq64/#prefix-based-proximity","title":"Prefix-based Proximity","text":"<pre><code>from uubed import encode_zoq64, common_prefix_length\n\n# Nearby locations\nsf = encode_zoq64([37.7749, -122.4194], bounds=[(-90, 90), (-180, 180)])\noakland = encode_zoq64([37.8044, -122.2711], bounds=[(-90, 90), (-180, 180)])\nla = encode_zoq64([34.0522, -118.2437], bounds=[(-90, 90), (-180, 180)])\n\n# Compare prefixes\nsf_oakland_prefix = common_prefix_length(sf, oakland)\nsf_la_prefix = common_prefix_length(sf, la)\n\nprint(f\"SF-Oakland common prefix: {sf_oakland_prefix} chars\")  # ~12\nprint(f\"SF-LA common prefix: {sf_la_prefix} chars\")           # ~4\n</code></pre>"},{"location":"family/zoq64/#advanced-features","title":"Advanced Features","text":""},{"location":"family/zoq64/#adaptive-precision","title":"Adaptive Precision","text":"<pre><code>from uubed import Zoq64Encoder\n\n# Variable precision based on data density\nencoder = Zoq64Encoder(adaptive_precision=True)\n\n# Dense urban area - high precision\nurban_point = [37.7749, -122.4194]\nurban_code = encoder.encode(urban_point, hint=\"dense\")  # Longer code\n\n# Sparse rural area - lower precision  \nrural_point = [45.123, -95.456]\nrural_code = encoder.encode(rural_point, hint=\"sparse\")  # Shorter code\n</code></pre>"},{"location":"family/zoq64/#hierarchical-encoding","title":"Hierarchical Encoding","text":"<pre><code># Multi-resolution spatial encoding\nencoder = Zoq64Encoder()\n\nlocation = [37.7749, -122.4194]\n\n# Different precision levels\ncodes = {\n    'city': encoder.encode(location, precision=16),     # ~10km\n    'neighborhood': encoder.encode(location, precision=24),  # ~100m\n    'building': encoder.encode(location, precision=32),      # ~1m\n    'precise': encoder.encode(location, precision=48)        # ~1cm\n}\n\n# All codes share common prefix\n# city: \"AbCd.EfGh\"\n# neighborhood: \"AbCd.EfGh.IjKl.MnOp\"\n# building: \"AbCd.EfGh.IjKl.MnOp.QrSt.UvWx\"\n</code></pre>"},{"location":"family/zoq64/#range-queries","title":"Range Queries","text":"<pre><code>from uubed import zoq64_range_prefix\n\n# Find prefix for bounding box\nbbox = {\n    'min': [37.7, -122.5],\n    'max': [37.8, -122.4]\n}\n\n# Get common prefix for range\nprefix = zoq64_range_prefix(bbox)\nprint(f\"Range prefix: {prefix}\")\n\n# Use for efficient queries\n# All points in bbox will start with this prefix\n</code></pre>"},{"location":"family/zoq64/#integration-patterns","title":"Integration Patterns","text":""},{"location":"family/zoq64/#with-postgis","title":"With PostGIS","text":"<pre><code>import psycopg2\nfrom uubed import encode_zoq64\n\n# Create spatial table with Zoq64\ncursor.execute(\"\"\"\n    CREATE TABLE locations (\n        id SERIAL PRIMARY KEY,\n        name TEXT,\n        point GEOMETRY(Point, 4326),\n        zoq64_code VARCHAR(64),\n        zoq64_prefix_8 VARCHAR(8),  -- For efficient indexing\n        zoq64_prefix_16 VARCHAR(16)\n    );\n\n    CREATE INDEX idx_zoq64_prefix_8 ON locations(zoq64_prefix_8);\n    CREATE INDEX idx_zoq64_prefix_16 ON locations(zoq64_prefix_16);\n\"\"\")\n\n# Insert with Zoq64 encoding\ndef insert_location(name: str, lat: float, lon: float):\n    code = encode_zoq64([lat, lon], bounds=[(-90, 90), (-180, 180)])\n\n    cursor.execute(\"\"\"\n        INSERT INTO locations (name, point, zoq64_code, zoq64_prefix_8, zoq64_prefix_16)\n        VALUES (%s, ST_SetSRID(ST_MakePoint(%s, %s), 4326), %s, %s, %s)\n    \"\"\", (name, lon, lat, code, code[:8], code[:16]))\n\n# Proximity search\ndef find_nearby(lat: float, lon: float, precision_chars: int = 8):\n    code = encode_zoq64([lat, lon], bounds=[(-90, 90), (-180, 180)])\n    prefix = code[:precision_chars]\n\n    cursor.execute(\"\"\"\n        SELECT name, ST_Distance(point::geography, ST_MakePoint(%s, %s)::geography) as distance\n        FROM locations\n        WHERE zoq64_prefix_8 = %s\n        ORDER BY distance\n    \"\"\", (lon, lat, prefix))\n\n    return cursor.fetchall()\n</code></pre>"},{"location":"family/zoq64/#with-elasticsearch","title":"With Elasticsearch","text":"<pre><code>from elasticsearch import Elasticsearch\nfrom uubed import encode_zoq64\n\nes = Elasticsearch()\n\n# Mapping for spatial data\nmapping = {\n    \"mappings\": {\n        \"properties\": {\n            \"location\": {\"type\": \"geo_point\"},\n            \"zoq64\": {\"type\": \"keyword\"},\n            \"zoq64_prefixes\": {\n                \"type\": \"text\",\n                \"analyzer\": \"prefix_analyzer\"\n            }\n        }\n    },\n    \"settings\": {\n        \"analysis\": {\n            \"analyzer\": {\n                \"prefix_analyzer\": {\n                    \"tokenizer\": \"prefix_tokenizer\"\n                }\n            },\n            \"tokenizer\": {\n                \"prefix_tokenizer\": {\n                    \"type\": \"edge_ngram\",\n                    \"min_gram\": 4,\n                    \"max_gram\": 32,\n                    \"token_chars\": [\"letter\", \"digit\", \"punctuation\"]\n                }\n            }\n        }\n    }\n}\n\n# Index document with Zoq64\ndef index_location(doc_id: str, lat: float, lon: float, properties: dict):\n    zoq64_code = encode_zoq64([lat, lon], bounds=[(-90, 90), (-180, 180)])\n\n    doc = {\n        \"location\": {\"lat\": lat, \"lon\": lon},\n        \"zoq64\": zoq64_code,\n        \"zoq64_prefixes\": zoq64_code,  # For prefix matching\n        **properties\n    }\n\n    es.index(index=\"spatial\", id=doc_id, body=doc)\n\n# Proximity search using prefix\ndef search_proximity(lat: float, lon: float, prefix_length: int = 12):\n    zoq64_code = encode_zoq64([lat, lon], bounds=[(-90, 90), (-180, 180)])\n    prefix = zoq64_code[:prefix_length]\n\n    query = {\n        \"query\": {\n            \"prefix\": {\n                \"zoq64\": prefix\n            }\n        }\n    }\n\n    return es.search(index=\"spatial\", body=query)\n</code></pre>"},{"location":"family/zoq64/#with-redis","title":"With Redis","text":"<pre><code>import redis\nfrom uubed import encode_zoq64\n\nr = redis.Redis()\n\n# Geospatial indexing with Zoq64\ndef store_location(place_id: str, lat: float, lon: float, data: dict):\n    zoq64_code = encode_zoq64([lat, lon], bounds=[(-90, 90), (-180, 180)])\n\n    # Store full data\n    r.hset(f\"location:{place_id}\", mapping={\n        \"lat\": lat,\n        \"lon\": lon,\n        \"zoq64\": zoq64_code,\n        **data\n    })\n\n    # Create prefix indices for different zoom levels\n    for prefix_len in [4, 8, 12, 16, 20]:\n        prefix = zoq64_code[:prefix_len]\n        r.sadd(f\"zoq64:prefix:{prefix}\", place_id)\n\n    # Also store in Redis geospatial index\n    r.geoadd(\"locations\", lon, lat, place_id)\n\n# Multi-resolution search\ndef search_area(lat: float, lon: float, zoom_level: int):\n    # zoom_level: 1-5 (city to building)\n    prefix_len = zoom_level * 4\n\n    zoq64_code = encode_zoq64([lat, lon], bounds=[(-90, 90), (-180, 180)])\n    prefix = zoq64_code[:prefix_len]\n\n    # Get all locations with matching prefix\n    place_ids = r.smembers(f\"zoq64:prefix:{prefix}\")\n\n    # Retrieve location data\n    locations = []\n    for place_id in place_ids:\n        data = r.hgetall(f\"location:{place_id.decode()}\")\n        locations.append(data)\n\n    return locations\n</code></pre>"},{"location":"family/zoq64/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"family/zoq64/#encoding-speed","title":"Encoding Speed","text":"Dimensions Pure Python Native Speedup 2D 3.2 MB/s 480 MB/s 150x 3D 2.1 MB/s 320 MB/s 152x 4D 1.6 MB/s 240 MB/s 150x"},{"location":"family/zoq64/#query-performance","title":"Query Performance","text":"Operation Traditional Zoq64 Improvement Range query (1M points) 850ms 12ms 71x k-NN (1M points) 1200ms 45ms 27x Spatial join 15s 0.8s 19x"},{"location":"family/zoq64/#best-practices","title":"Best Practices","text":""},{"location":"family/zoq64/#dos","title":"Do's","text":"<ol> <li>Use appropriate precision: Balance between accuracy and storage</li> <li>Index prefixes: Enable efficient range queries</li> <li>Normalize bounds: Consistent coordinate systems</li> <li>Leverage hierarchy: Multi-resolution for zoom levels</li> <li>Combine with traditional indices: Best of both worlds</li> </ol>"},{"location":"family/zoq64/#donts","title":"Don'ts","text":"<ol> <li>Don't over-precision: Wastes space without benefit</li> <li>Don't ignore bounds: Critical for correct encoding</li> <li>Don't mix coordinate systems: Standardize first</li> <li>Don't update in-place: Zoq64 codes are immutable</li> </ol>"},{"location":"family/zoq64/#spatial-operations","title":"Spatial Operations","text":""},{"location":"family/zoq64/#bounding-box-queries","title":"Bounding Box Queries","text":"<pre><code>from uubed import zoq64_bbox_prefixes\n\n# Get all prefixes that cover a bounding box\nbbox = {\n    'min': [37.7, -122.5],\n    'max': [37.8, -122.4]\n}\n\nprefixes = zoq64_bbox_prefixes(bbox, max_prefixes=10)\n# Returns list of prefixes that cover the area efficiently\n</code></pre>"},{"location":"family/zoq64/#distance-estimation","title":"Distance Estimation","text":"<pre><code>from uubed import zoq64_distance_estimate\n\n# Estimate distance from prefix match length\ncode1 = \"AbCd.EfGh.IjKl.MnOp\"\ncode2 = \"AbCd.EfGh.IjKm.XyZa\"\n\ncommon_len = common_prefix_length(code1, code2)\napprox_distance = zoq64_distance_estimate(common_len, precision=32)\nprint(f\"Approximate distance: {approx_distance}m\")\n</code></pre>"},{"location":"family/zoq64/#use-cases","title":"Use Cases","text":""},{"location":"family/zoq64/#1-geospatial-applications","title":"1. Geospatial Applications","text":"<ul> <li>Location-based services</li> <li>Mapping and navigation</li> <li>Proximity searches</li> <li>Spatial clustering</li> </ul>"},{"location":"family/zoq64/#2-scientific-data","title":"2. Scientific Data","text":"<ul> <li>Astronomical coordinates</li> <li>Climate data points</li> <li>Sensor networks</li> <li>3D volumetric data</li> </ul>"},{"location":"family/zoq64/#3-computer-graphics","title":"3. Computer Graphics","text":"<ul> <li>Color space navigation</li> <li>3D model indexing</li> <li>Texture coordinates</li> <li>Voxel data</li> </ul>"},{"location":"family/zoq64/#4-time-series-data","title":"4. Time-Series Data","text":"<ul> <li>Temporal-spatial encoding</li> <li>Event correlation</li> <li>Multi-dimensional time series</li> </ul>"},{"location":"family/zoq64/#limitations","title":"Limitations","text":"<ol> <li>Not rotation-invariant: Rotated shapes have different codes</li> <li>Boundary effects: Points near boundaries may seem far</li> <li>Dimension limits: Efficiency decreases with many dimensions</li> <li>Precision trade-offs: Higher precision means longer codes</li> </ol>"},{"location":"family/zoq64/#future-directions","title":"Future Directions","text":""},{"location":"family/zoq64/#planned-enhancements","title":"Planned Enhancements","text":"<ol> <li>Hilbert curve variant: Better locality preservation</li> <li>Adaptive boundaries: Dynamic range adjustment</li> <li>Compression modes: Variable-length encoding</li> <li>Geodesic support: True Earth surface distances</li> </ol>"},{"location":"family/zoq64/#summary","title":"Summary","text":"<p>Zoq64 brings spatial awareness to the QuadB64 family, enabling:</p> <ul> <li>Efficient spatial queries: Prefix-based range searches</li> <li>Multi-resolution support: From continents to centimeters</li> <li>Search engine integration: Spatial data in text indices</li> <li>Locality preservation: Nearby points stay nearby</li> </ul> <p>Use Zoq64 when you need to encode spatial or multi-dimensional data for systems that use text-based indexing, especially when proximity queries are important. It's the bridge between geometric data and string-based search systems.</p>"},{"location":"implementation/advanced-features/","title":"Advanced Features and Configuration","text":""},{"location":"implementation/advanced-features/#overview","title":"Overview","text":"<p>This chapter covers sophisticated usage patterns, troubleshooting techniques, and advanced configuration options for power users and system integrators working with QuadB64 in complex environments.</p>"},{"location":"implementation/advanced-features/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"implementation/advanced-features/#custom-variant-creation","title":"Custom Variant Creation","text":"<p>Beyond the standard QuadB64 variants (Eq64, Shq64, T8q64, Zoq64), you can create specialized variants for specific use cases:</p> <pre><code>from uubed.core import VariantBuilder, AlphabetGenerator, PositionFunction\n\nclass DomainSpecificVariant:\n    \"\"\"Create QuadB64 variants optimized for specific domains\"\"\"\n\n    @staticmethod\n    def create_genomic_variant():\n        \"\"\"Variant optimized for genomic data encoding\"\"\"\n\n        def genomic_alphabet_generator(position: int) -&gt; str:\n            # Optimize for ATCG frequency in genomic data\n            # Place nucleotide-like characters first\n            base = \"ATCGNatcgn0123456789BDEFHIJKLMOPQRSUVWXYZ+/bdefhijklmopqrsuvwxyz\"\n\n            # Rotate based on codon position (groups of 3)\n            codon_position = (position // 3) % 3\n            rotation = codon_position * 21  # Distribute across alphabet\n\n            return base[rotation:] + base[:rotation]\n\n        def genomic_position_function(position: int, context: dict = None) -&gt; int:\n            # Account for reading frame in genomic sequences\n            reading_frame = context.get('reading_frame', 0) if context else 0\n            return (position + reading_frame) % (2**24)\n\n        return VariantBuilder.create_variant(\n            name=\"Genomic64\",\n            alphabet_generator=genomic_alphabet_generator,\n            position_function=genomic_position_function,\n            metadata={\n                'domain': 'genomics',\n                'optimized_for': 'nucleotide_sequences',\n                'reading_frame_aware': True\n            }\n        )\n\n    @staticmethod \n    def create_financial_variant():\n        \"\"\"Variant optimized for financial data encoding\"\"\"\n\n        def financial_alphabet_generator(position: int) -&gt; str:\n            # Prioritize numeric characters for financial data\n            base = \"0123456789.$+-ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz/\"\n\n            # Rotate based on decimal precision context\n            precision_rotation = (position * 11) % 64\n            return base[precision_rotation:] + base[:precision_rotation]\n\n        def financial_position_function(position: int, context: dict = None) -&gt; int:\n            # Incorporate timestamp for temporal consistency\n            timestamp = context.get('timestamp', 0) if context else 0\n            time_factor = int(timestamp / 3600) % 1000  # Hour-based rotation\n            return (position * 13 + time_factor) % (2**28)\n\n        return VariantBuilder.create_variant(\n            name=\"Financial64\",\n            alphabet_generator=financial_alphabet_generator,\n            position_function=financial_position_function,\n            metadata={\n                'domain': 'finance',\n                'temporal_consistency': True,\n                'precision_optimized': True\n            }\n        )\n\n# Usage example\ngenomic_encoder = DomainSpecificVariant.create_genomic_variant()\nfinancial_encoder = DomainSpecificVariant.create_financial_variant()\n\n# Encode genomic sequence with reading frame context\ndna_sequence = b\"ATCGATCGATCG\" * 100\nencoded_dna = genomic_encoder.encode(dna_sequence, context={'reading_frame': 1})\n\n# Encode financial data with timestamp context\nfinancial_data = b'{\"price\": 123.45, \"volume\": 10000}'\nencoded_financial = financial_encoder.encode(\n    financial_data, \n    context={'timestamp': time.time()}\n)\n</code></pre>"},{"location":"implementation/advanced-features/#performance-tuning-for-specific-use-cases","title":"Performance Tuning for Specific Use Cases","text":""},{"location":"implementation/advanced-features/#high-throughput-streaming-configuration","title":"High-Throughput Streaming Configuration","text":"<pre><code>class HighThroughputConfig:\n    \"\"\"Configuration optimized for high-throughput streaming scenarios\"\"\"\n\n    def __init__(self):\n        self.batch_size = 8192  # Optimal for network packets\n        self.worker_threads = min(32, os.cpu_count() * 2)\n        self.memory_pool_size = 64 * 1024 * 1024  # 64MB pool\n        self.enable_compression = True\n        self.cache_size = 100000\n\n    def create_streaming_encoder(self):\n        \"\"\"Create encoder optimized for streaming\"\"\"\n\n        encoder = StreamingEncoder(\n            batch_size=self.batch_size,\n            max_workers=self.worker_threads,\n            memory_pool_size=self.memory_pool_size,\n            cache_config={\n                'max_size': self.cache_size,\n                'ttl_seconds': 300,  # 5 minute TTL\n                'eviction_policy': 'lru'\n            }\n        )\n\n        # Enable hardware acceleration if available\n        if self._has_simd_support():\n            encoder.enable_simd_acceleration()\n\n        return encoder\n\n    def _has_simd_support(self) -&gt; bool:\n        \"\"\"Detect available SIMD instruction sets\"\"\"\n        try:\n            import cpuinfo\n            cpu_info = cpuinfo.get_cpu_info()\n            return any(flag in cpu_info.get('flags', []) \n                      for flag in ['avx2', 'sse4_1', 'neon'])\n        except ImportError:\n            return False\n\nclass StreamingEncoder:\n    \"\"\"High-performance streaming encoder\"\"\"\n\n    def __init__(self, batch_size=8192, max_workers=8, \n                 memory_pool_size=64*1024*1024, cache_config=None):\n\n        self.batch_size = batch_size\n        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n        self.memory_pool = MemoryPool(memory_pool_size)\n        self.cache = LRUCache(**(cache_config or {}))\n        self.statistics = StreamingStats()\n\n        # Pre-allocate working buffers\n        self.working_buffers = [\n            self.memory_pool.get_buffer(batch_size * 2)\n            for _ in range(max_workers)\n        ]\n\n    def encode_stream(self, data_stream, output_stream):\n        \"\"\"Encode data stream with optimal batching\"\"\"\n\n        batch_buffer = bytearray()\n        batch_count = 0\n\n        try:\n            for chunk in data_stream:\n                batch_buffer.extend(chunk)\n\n                # Process when batch is full\n                if len(batch_buffer) &gt;= self.batch_size:\n                    self._process_batch(batch_buffer, output_stream, batch_count)\n                    batch_buffer.clear()\n                    batch_count += 1\n\n            # Process remaining data\n            if batch_buffer:\n                self._process_batch(batch_buffer, output_stream, batch_count)\n\n        finally:\n            self.executor.shutdown(wait=True)\n\n    def _process_batch(self, batch_data: bytearray, output_stream, batch_id: int):\n        \"\"\"Process a single batch with position-aware encoding\"\"\"\n\n        # Calculate global position offset\n        global_position = batch_id * self.batch_size\n\n        # Submit to thread pool\n        future = self.executor.submit(\n            self._encode_batch_worker, \n            bytes(batch_data), \n            global_position\n        )\n\n        # Get result and write to output\n        try:\n            encoded_data = future.result(timeout=30.0)\n            output_stream.write(encoded_data)\n            output_stream.flush()\n\n            self.statistics.record_batch(len(batch_data), len(encoded_data))\n\n        except Exception as e:\n            self.statistics.record_error(str(e))\n            raise RuntimeError(f\"Batch {batch_id} encoding failed: {e}\")\n\n    def _encode_batch_worker(self, data: bytes, position: int) -&gt; bytes:\n        \"\"\"Worker function for batch encoding\"\"\"\n\n        # Check cache first\n        cache_key = (hash(data), position)\n        cached_result = self.cache.get(cache_key)\n        if cached_result:\n            return cached_result\n\n        # Encode with position context\n        encoded = encode_eq64_with_position(data, position)\n\n        # Cache result\n        self.cache.put(cache_key, encoded)\n\n        return encoded.encode('utf-8')\n\nclass StreamingStats:\n    \"\"\"Statistics collection for streaming operations\"\"\"\n\n    def __init__(self):\n        self.total_bytes_in = 0\n        self.total_bytes_out = 0\n        self.batch_count = 0\n        self.error_count = 0\n        self.start_time = time.time()\n        self._lock = threading.Lock()\n\n    def record_batch(self, bytes_in: int, bytes_out: int):\n        with self._lock:\n            self.total_bytes_in += bytes_in\n            self.total_bytes_out += bytes_out\n            self.batch_count += 1\n\n    def record_error(self, error_msg: str):\n        with self._lock:\n            self.error_count += 1\n\n    def get_stats(self) -&gt; dict:\n        with self._lock:\n            elapsed = time.time() - self.start_time\n            return {\n                'total_input_mb': self.total_bytes_in / 1024 / 1024,\n                'total_output_mb': self.total_bytes_out / 1024 / 1024,\n                'compression_ratio': self.total_bytes_out / max(self.total_bytes_in, 1),\n                'throughput_mb_s': (self.total_bytes_in / 1024 / 1024) / max(elapsed, 0.001),\n                'batches_processed': self.batch_count,\n                'error_rate': self.error_count / max(self.batch_count, 1),\n                'elapsed_time': elapsed\n            }\n</code></pre>"},{"location":"implementation/advanced-features/#low-latency-configuration","title":"Low-Latency Configuration","text":"<pre><code>class LowLatencyConfig:\n    \"\"\"Configuration optimized for low-latency scenarios\"\"\"\n\n    def __init__(self):\n        # Minimize context switching and memory allocation\n        self.enable_pre_allocation = True\n        self.disable_threading = True  # Single-threaded for consistency\n        self.cache_warmup = True\n        self.use_stack_buffers = True\n\n    def create_low_latency_encoder(self):\n        \"\"\"Create encoder optimized for minimum latency\"\"\"\n\n        encoder = LowLatencyEncoder()\n\n        if self.cache_warmup:\n            encoder.warmup_cache()\n\n        return encoder\n\nclass LowLatencyEncoder:\n    \"\"\"Encoder optimized for minimal latency\"\"\"\n\n    def __init__(self):\n        # Pre-allocate all possible buffers\n        self.buffer_cache = {}\n        self.alphabet_cache = {}\n        self.precomputed_tables = self._build_lookup_tables()\n\n        # Pre-generate common alphabets\n        for pos in range(1024):  # Cache first 1024 positions\n            self.alphabet_cache[pos] = self._generate_alphabet(pos)\n\n    def encode_minimal_latency(self, data: bytes, position: int = 0) -&gt; str:\n        \"\"\"Encode with minimal latency - no dynamic allocation\"\"\"\n\n        if len(data) == 0:\n            return \"\"\n\n        # Use stack-allocated buffer for small data\n        if len(data) &lt;= 1024:\n            return self._encode_stack_buffer(data, position)\n        else:\n            return self._encode_pre_allocated(data, position)\n\n    def _encode_stack_buffer(self, data: bytes, position: int) -&gt; str:\n        \"\"\"Encode using stack-allocated buffer\"\"\"\n\n        # Get pre-computed alphabet\n        alphabet = self.alphabet_cache.get(position % 1024)\n        if not alphabet:\n            alphabet = self._generate_alphabet(position)\n\n        # Direct encoding without memory allocation\n        result_chars = []\n\n        for i in range(0, len(data), 3):\n            chunk = data[i:i+3]\n            encoded_chunk = self._encode_chunk_direct(chunk, alphabet, position + i)\n            result_chars.extend(encoded_chunk)\n\n        return ''.join(result_chars)\n\n    def _encode_chunk_direct(self, chunk: bytes, alphabet: str, position: int) -&gt; list:\n        \"\"\"Direct chunk encoding without intermediate allocations\"\"\"\n\n        # Pad chunk to 3 bytes\n        while len(chunk) &lt; 3:\n            chunk += b'\\x00'\n\n        # Convert to 24-bit integer\n        value = (chunk[0] &lt;&lt; 16) | (chunk[1] &lt;&lt; 8) | chunk[2]\n\n        # Extract 6-bit groups using lookup table\n        indices = [\n            (value &gt;&gt; 18) &amp; 0x3F,\n            (value &gt;&gt; 12) &amp; 0x3F,\n            (value &gt;&gt; 6) &amp; 0x3F,\n            value &amp; 0x3F\n        ]\n\n        # Apply position-dependent rotation\n        rotation = (position // 3) % 64\n        rotated_indices = [(idx + rotation) % 64 for idx in indices]\n\n        # Map to alphabet characters\n        return [alphabet[idx] for idx in rotated_indices]\n\n    def warmup_cache(self):\n        \"\"\"Pre-compute common operations to minimize latency\"\"\"\n\n        # Warm up with common data patterns\n        test_patterns = [\n            b'\\x00' * 16,  # Null pattern\n            b'\\xFF' * 16,  # All ones\n            bytes(range(16)),  # Sequential\n            b'Hello, World!' * 2  # Text pattern\n        ]\n\n        for pattern in test_patterns:\n            for pos in range(0, 64, 8):\n                self.encode_minimal_latency(pattern, pos)\n</code></pre>"},{"location":"implementation/advanced-features/#integration-with-exotic-platforms","title":"Integration with Exotic Platforms","text":""},{"location":"implementation/advanced-features/#embedded-systems-integration","title":"Embedded Systems Integration","text":"<pre><code>class EmbeddedSystemsConfig:\n    \"\"\"Configuration for resource-constrained embedded systems\"\"\"\n\n    def __init__(self, memory_limit_kb=256, cpu_mhz=100):\n        self.memory_limit = memory_limit_kb * 1024\n        self.cpu_frequency = cpu_mhz * 1000000\n        self.enable_power_saving = True\n        self.use_lookup_tables = memory_limit_kb &gt; 64  # Only if sufficient memory\n\n    def create_embedded_encoder(self):\n        \"\"\"Create encoder suitable for embedded systems\"\"\"\n\n        if self.memory_limit &lt; 32 * 1024:  # Very constrained\n            return MinimalEncoder(self.memory_limit)\n        elif self.memory_limit &lt; 128 * 1024:  # Moderately constrained  \n            return CompactEncoder(self.memory_limit)\n        else:  # Relatively unconstrained\n            return OptimizedEmbeddedEncoder(self.memory_limit)\n\nclass MinimalEncoder:\n    \"\"\"Minimal encoder for severely memory-constrained environments\"\"\"\n\n    def __init__(self, memory_limit: int):\n        self.memory_limit = memory_limit\n        # No caches or lookup tables - compute everything on-demand\n\n    def encode(self, data: bytes, position: int = 0) -&gt; str:\n        \"\"\"Encode with minimal memory footprint\"\"\"\n\n        if len(data) == 0:\n            return \"\"\n\n        result = []\n\n        # Process one character at a time to minimize memory usage\n        for i in range(0, len(data), 3):\n            chunk = data[i:min(i+3, len(data))]\n            encoded = self._encode_chunk_minimal(chunk, position + i)\n            result.append(encoded)\n\n        return ''.join(result)\n\n    def _encode_chunk_minimal(self, chunk: bytes, position: int) -&gt; str:\n        \"\"\"Encode chunk with zero additional memory allocation\"\"\"\n\n        # Generate alphabet on-demand (no caching)\n        alphabet = self._generate_alphabet_minimal(position)\n\n        # Pad chunk\n        padded = chunk + b'\\x00' * (3 - len(chunk))\n\n        # Convert to integer and extract indices\n        value = (padded[0] &lt;&lt; 16) | (padded[1] &lt;&lt; 8) | padded[2]\n\n        # Extract and rotate indices\n        rotation = (position // 3) % 64\n        indices = [\n            ((value &gt;&gt; 18) &amp; 0x3F + rotation) % 64,\n            ((value &gt;&gt; 12) &amp; 0x3F + rotation) % 64,\n            ((value &gt;&gt; 6) &amp; 0x3F + rotation) % 64,\n            ((value &amp; 0x3F) + rotation) % 64\n        ]\n\n        # Build result directly\n        return ''.join(alphabet[idx] for idx in indices[:len(chunk)+1])\n\n    def _generate_alphabet_minimal(self, position: int) -&gt; str:\n        \"\"\"Generate alphabet without caching\"\"\"\n        base = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\"\n        rotation = (position // 3) % 64\n        return base[rotation:] + base[:rotation]\n</code></pre>"},{"location":"implementation/advanced-features/#gpu-acceleration-integration","title":"GPU Acceleration Integration","text":"<pre><code>try:\n    import cupy as cp  # GPU acceleration with CuPy\n    GPU_AVAILABLE = True\nexcept ImportError:\n    GPU_AVAILABLE = False\n\nclass GPUAcceleratedEncoder:\n    \"\"\"QuadB64 encoder with GPU acceleration for large datasets\"\"\"\n\n    def __init__(self):\n        if not GPU_AVAILABLE:\n            raise RuntimeError(\"GPU acceleration requires CuPy\")\n\n        self.device = cp.cuda.Device()\n        self.memory_pool = cp.get_default_memory_pool()\n        self._compile_kernels()\n\n    def _compile_kernels(self):\n        \"\"\"Compile CUDA kernels for QuadB64 operations\"\"\"\n\n        # CUDA kernel for parallel encoding\n        encode_kernel_code = \"\"\"\n        extern \"C\" __global__\n        void quadb64_encode_kernel(\n            const unsigned char* input,\n            char* output,\n            const char* alphabet,\n            int* positions,\n            int data_size,\n            int chunk_size\n        ) {\n            int idx = blockIdx.x * blockDim.x + threadIdx.x;\n            int chunk_start = idx * chunk_size;\n\n            if (chunk_start &gt;= data_size) return;\n\n            // Process chunk\n            int chunk_end = min(chunk_start + chunk_size, data_size);\n            int position = positions[idx];\n\n            // Generate position-dependent alphabet\n            char local_alphabet[64];\n            int rotation = (position / 3) % 64;\n\n            for (int i = 0; i &lt; 64; i++) {\n                local_alphabet[i] = alphabet[(i + rotation) % 64];\n            }\n\n            // Encode chunk\n            for (int i = chunk_start; i &lt; chunk_end; i += 3) {\n                // Get 3 bytes (pad with zeros if needed)\n                unsigned int value = 0;\n                for (int j = 0; j &lt; 3 &amp;&amp; i + j &lt; data_size; j++) {\n                    value |= (input[i + j] &lt;&lt; (16 - 8 * j));\n                }\n\n                // Extract 6-bit groups\n                int out_idx = (i / 3) * 4;\n                output[out_idx + 0] = local_alphabet[(value &gt;&gt; 18) &amp; 0x3F];\n                output[out_idx + 1] = local_alphabet[(value &gt;&gt; 12) &amp; 0x3F];\n                output[out_idx + 2] = local_alphabet[(value &gt;&gt; 6) &amp; 0x3F];\n                output[out_idx + 3] = local_alphabet[value &amp; 0x3F];\n            }\n        }\n        \"\"\"\n\n        self.encode_kernel = cp.RawKernel(encode_kernel_code, 'quadb64_encode_kernel')\n\n    def encode_gpu(self, data_list: list, positions: list = None) -&gt; list:\n        \"\"\"Encode multiple data chunks on GPU\"\"\"\n\n        if not positions:\n            positions = list(range(len(data_list)))\n\n        # Prepare GPU memory\n        max_data_size = max(len(data) for data in data_list)\n        max_output_size = ((max_data_size + 2) // 3) * 4\n\n        # Allocate GPU memory\n        gpu_input = cp.zeros(max_data_size, dtype=cp.uint8)\n        gpu_output = cp.zeros(max_output_size, dtype=cp.int8)\n        gpu_alphabet = cp.array(list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\"), dtype=cp.int8)\n\n        results = []\n\n        for i, (data, position) in enumerate(zip(data_list, positions)):\n            # Copy data to GPU\n            data_array = cp.array(list(data), dtype=cp.uint8)\n            gpu_input[:len(data)] = data_array\n\n            # Set up kernel parameters\n            chunk_size = 192  # Process 192 bytes per thread (64 output chars)\n            num_chunks = (len(data) + chunk_size - 1) // chunk_size\n\n            positions_array = cp.array([position + i * chunk_size for i in range(num_chunks)], dtype=cp.int32)\n\n            # Launch kernel\n            threads_per_block = min(256, num_chunks)\n            blocks = (num_chunks + threads_per_block - 1) // threads_per_block\n\n            self.encode_kernel(\n                (blocks,), (threads_per_block,),\n                (gpu_input, gpu_output, gpu_alphabet, positions_array, len(data), chunk_size)\n            )\n\n            # Copy result back to CPU\n            output_size = ((len(data) + 2) // 3) * 4\n            result_bytes = cp.asnumpy(gpu_output[:output_size])\n            result_str = ''.join(chr(b) for b in result_bytes if b != 0)\n            results.append(result_str)\n\n        return results\n</code></pre>"},{"location":"implementation/advanced-features/#troubleshooting-guide","title":"Troubleshooting Guide","text":""},{"location":"implementation/advanced-features/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"implementation/advanced-features/#issue-1-performance-degradation","title":"Issue 1: Performance Degradation","text":"<p>Symptoms: - Encoding speed significantly slower than expected - High CPU usage with low throughput - Memory usage growing over time</p> <p>Diagnostic Steps:</p> <pre><code>class PerformanceDiagnostic:\n    \"\"\"Diagnostic tools for performance issues\"\"\"\n\n    def diagnose_performance_issue(self, encoder, test_data: bytes):\n        \"\"\"Comprehensive performance diagnosis\"\"\"\n\n        diagnostics = {\n            'system_info': self._get_system_info(),\n            'encoder_config': self._analyze_encoder_config(encoder),\n            'memory_usage': self._analyze_memory_usage(),\n            'cpu_utilization': self._analyze_cpu_usage(),\n            'bottlenecks': self._identify_bottlenecks(encoder, test_data)\n        }\n\n        return self._generate_performance_report(diagnostics)\n\n    def _identify_bottlenecks(self, encoder, test_data: bytes) -&gt; dict:\n        \"\"\"Identify specific performance bottlenecks\"\"\"\n\n        import time\n        import tracemalloc\n\n        bottlenecks = {}\n\n        # Test different aspects\n        test_scenarios = {\n            'small_data': test_data[:100],\n            'medium_data': test_data[:10000],\n            'large_data': test_data,\n            'repeated_small': test_data[:100] * 100\n        }\n\n        for scenario, data in test_scenarios.items():\n            # Start memory tracking\n            tracemalloc.start()\n            start_time = time.perf_counter()\n\n            # Run encoding\n            try:\n                result = encoder.encode(data)\n                end_time = time.perf_counter()\n\n                # Get memory stats\n                current, peak = tracemalloc.get_traced_memory()\n                tracemalloc.stop()\n\n                bottlenecks[scenario] = {\n                    'duration': end_time - start_time,\n                    'throughput_mb_s': len(data) / (end_time - start_time) / 1024 / 1024,\n                    'memory_current_mb': current / 1024 / 1024,\n                    'memory_peak_mb': peak / 1024 / 1024,\n                    'efficiency_score': len(result) / (end_time - start_time) / peak\n                }\n\n            except Exception as e:\n                bottlenecks[scenario] = {'error': str(e)}\n\n        return bottlenecks\n\n# Usage\ndiagnostic = PerformanceDiagnostic()\ntest_data = b\"sample data\" * 10000\nencoder = SomeQuadB64Encoder()\n\nreport = diagnostic.diagnose_performance_issue(encoder, test_data)\nprint(report)\n</code></pre> <p>Common Solutions:</p> <ol> <li> <p>Enable Native Extensions: <pre><code># Check if native extensions are loaded\nif not encoder.has_native_support():\n    print(\"Installing native extensions...\")\n    install_native_extensions()\n</code></pre></p> </li> <li> <p>Optimize Memory Pool Size: <pre><code># Increase memory pool for large datasets\nencoder.configure_memory_pool(size_mb=128)\n</code></pre></p> </li> <li> <p>Adjust Thread Pool Size: <pre><code># Optimize for your CPU count\noptimal_workers = min(32, os.cpu_count() * 2)\nencoder.set_worker_count(optimal_workers)\n</code></pre></p> </li> </ol>"},{"location":"implementation/advanced-features/#issue-2-encoding-inconsistencies","title":"Issue 2: Encoding Inconsistencies","text":"<p>Symptoms: - Different results for same input - Position-dependent variations unexpected - Decoding failures</p> <p>Diagnostic Steps:</p> <pre><code>class ConsistencyDiagnostic:\n    \"\"\"Diagnostic tools for encoding consistency issues\"\"\"\n\n    def test_encoding_consistency(self, encoder, test_cases: list):\n        \"\"\"Test encoding consistency across multiple runs\"\"\"\n\n        results = {}\n\n        for i, test_data in enumerate(test_cases):\n            case_results = []\n\n            # Run same encoding multiple times\n            for run in range(5):\n                try:\n                    encoded = encoder.encode(test_data, position=0)\n                    case_results.append(encoded)\n                except Exception as e:\n                    case_results.append(f\"ERROR: {e}\")\n\n            # Check consistency\n            unique_results = set(case_results)\n            results[f\"case_{i}\"] = {\n                'consistent': len(unique_results) == 1,\n                'results': case_results,\n                'unique_count': len(unique_results)\n            }\n\n        return results\n\n    def test_position_consistency(self, encoder, data: bytes):\n        \"\"\"Test position-dependent behavior\"\"\"\n\n        position_tests = {}\n\n        for position in [0, 1, 2, 3, 63, 64, 65, 127, 128]:\n            try:\n                encoded = encoder.encode(data, position=position)\n                decoded = encoder.decode(encoded, position=position)\n\n                position_tests[position] = {\n                    'encoded': encoded,\n                    'roundtrip_success': decoded == data,\n                    'encoded_length': len(encoded)\n                }\n            except Exception as e:\n                position_tests[position] = {'error': str(e)}\n\n        return position_tests\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Verify Position Parameter Usage: <pre><code># Ensure consistent position usage\nposition = calculate_global_position(chunk_index, chunk_size)\nencoded = encoder.encode(data, position=position)\n</code></pre></p> </li> <li> <p>Check Thread Safety: <pre><code># Use thread-safe encoder for concurrent access\nencoder = ThreadSafeEncoder(base_encoder)\n</code></pre></p> </li> </ol>"},{"location":"implementation/advanced-features/#issue-3-memory-leaks","title":"Issue 3: Memory Leaks","text":"<p>Symptoms: - Memory usage continuously increasing - Out of memory errors in long-running processes - Slow garbage collection</p> <p>Diagnostic Steps:</p> <pre><code>import gc\nimport tracemalloc\nimport weakref\n\nclass MemoryLeakDetector:\n    \"\"\"Detect and analyze memory leaks in QuadB64 operations\"\"\"\n\n    def __init__(self):\n        self.snapshots = []\n        self.tracked_objects = []\n\n    def start_tracking(self):\n        \"\"\"Start memory leak tracking\"\"\"\n        tracemalloc.start()\n        gc.collect()  # Clean start\n        self.snapshots.append(tracemalloc.take_snapshot())\n\n    def check_leaks(self, operation_name: str = \"\"):\n        \"\"\"Check for memory leaks since last snapshot\"\"\"\n        gc.collect()  # Force garbage collection\n\n        current_snapshot = tracemalloc.take_snapshot()\n        self.snapshots.append(current_snapshot)\n\n        if len(self.snapshots) &gt;= 2:\n            top_stats = current_snapshot.compare_to(\n                self.snapshots[-2], 'lineno'\n            )\n\n            print(f\"Memory diff for {operation_name}:\")\n            for stat in top_stats[:10]:\n                print(stat)\n\n    def track_object(self, obj):\n        \"\"\"Track specific object for garbage collection\"\"\"\n        weak_ref = weakref.ref(obj, lambda x: print(f\"Object {id(obj)} collected\"))\n        self.tracked_objects.append(weak_ref)\n\n# Usage\nleak_detector = MemoryLeakDetector()\nencoder = SomeQuadB64Encoder()\n\nleak_detector.start_tracking()\n\n# Perform operations\nfor i in range(1000):\n    data = b\"test data\" * 100\n    encoded = encoder.encode(data)\n\n    if i % 100 == 0:\n        leak_detector.check_leaks(f\"iteration_{i}\")\n</code></pre> <p>Solutions:</p> <ol> <li> <p>Proper Cache Management: <pre><code># Configure cache with appropriate limits\nencoder.configure_cache(max_size=10000, ttl_seconds=300)\n</code></pre></p> </li> <li> <p>Explicit Cleanup: <pre><code># Periodically clean up resources\nif iteration_count % 1000 == 0:\n    encoder.cleanup_caches()\n    gc.collect()\n</code></pre></p> </li> </ol>"},{"location":"implementation/advanced-features/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"implementation/advanced-features/#enable-detailed-logging","title":"Enable Detailed Logging","text":"<pre><code>import logging\n\n# Configure QuadB64 logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger('uubed')\n\n# Enable detailed operation logging\nencoder.enable_debug_logging(logger)\n\n# Add custom log handlers\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n</code></pre>"},{"location":"implementation/advanced-features/#step-by-step-encoding-analysis","title":"Step-by-Step Encoding Analysis","text":"<pre><code>class StepByStepDebugger:\n    \"\"\"Debug QuadB64 encoding step by step\"\"\"\n\n    def debug_encode(self, data: bytes, position: int = 0):\n        \"\"\"Debug encoding process step by step\"\"\"\n\n        print(f\"Debugging encoding of {len(data)} bytes at position {position}\")\n        print(f\"Input data: {data[:50]}...\" if len(data) &gt; 50 else f\"Input data: {data}\")\n        print()\n\n        # Step 1: Alphabet generation\n        alphabet = self._debug_alphabet_generation(position)\n\n        # Step 2: Data chunking\n        chunks = self._debug_data_chunking(data)\n\n        # Step 3: Chunk encoding\n        encoded_chunks = []\n        for i, chunk in enumerate(chunks):\n            chunk_position = position + i * 3\n            encoded_chunk = self._debug_chunk_encoding(chunk, chunk_position, alphabet)\n            encoded_chunks.append(encoded_chunk)\n\n        # Step 4: Final assembly\n        result = ''.join(encoded_chunks)\n        print(f\"Final result: {result}\")\n\n        return result\n\n    def _debug_alphabet_generation(self, position: int) -&gt; str:\n        \"\"\"Debug alphabet generation\"\"\"\n        base_alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\"\n        rotation = (position // 3) % 64\n\n        alphabet = base_alphabet[rotation:] + base_alphabet[:rotation]\n\n        print(f\"Alphabet generation:\")\n        print(f\"  Position: {position}\")\n        print(f\"  Rotation: {rotation}\")\n        print(f\"  Base:     {base_alphabet}\")\n        print(f\"  Rotated:  {alphabet}\")\n        print()\n\n        return alphabet\n\n    def _debug_data_chunking(self, data: bytes) -&gt; list:\n        \"\"\"Debug data chunking process\"\"\"\n        chunks = [data[i:i+3] for i in range(0, len(data), 3)]\n\n        print(f\"Data chunking:\")\n        print(f\"  Total bytes: {len(data)}\")\n        print(f\"  Chunks: {len(chunks)}\")\n\n        for i, chunk in enumerate(chunks[:5]):  # Show first 5 chunks\n            print(f\"  Chunk {i}: {chunk} ({[hex(b) for b in chunk]})\")\n\n        if len(chunks) &gt; 5:\n            print(f\"  ... and {len(chunks) - 5} more chunks\")\n        print()\n\n        return chunks\n\n    def _debug_chunk_encoding(self, chunk: bytes, position: int, alphabet: str) -&gt; str:\n        \"\"\"Debug individual chunk encoding\"\"\"\n        # Pad chunk to 3 bytes\n        padded_chunk = chunk + b'\\x00' * (3 - len(chunk))\n\n        # Convert to 24-bit integer\n        value = (padded_chunk[0] &lt;&lt; 16) | (padded_chunk[1] &lt;&lt; 8) | padded_chunk[2]\n\n        # Extract 6-bit indices\n        indices = [\n            (value &gt;&gt; 18) &amp; 0x3F,\n            (value &gt;&gt; 12) &amp; 0x3F,\n            (value &gt;&gt; 6) &amp; 0x3F,\n            value &amp; 0x3F\n        ]\n\n        # Map to alphabet\n        chars = [alphabet[idx] for idx in indices]\n        result = ''.join(chars[:len(chunk)+1])\n\n        print(f\"Chunk encoding at position {position}:\")\n        print(f\"  Input: {chunk} -&gt; {[hex(b) for b in padded_chunk]}\")\n        print(f\"  24-bit value: {value:024b} ({value})\")\n        print(f\"  6-bit indices: {indices}\")\n        print(f\"  Characters: {chars} -&gt; '{result}'\")\n        print()\n\n        return result\n\n# Usage\ndebugger = StepByStepDebugger()\ntest_data = b\"Hello, QuadB64!\"\nresult = debugger.debug_encode(test_data, position=5)\n</code></pre> <p>This comprehensive advanced features guide provides power users with the tools and techniques needed to optimize QuadB64 for specific use cases, troubleshoot issues, and implement custom solutions for complex requirements.</p>"},{"location":"implementation/architecture/","title":"Implementation Architecture","text":""},{"location":"implementation/architecture/#overview","title":"Overview","text":"<p>This chapter provides a deep dive into the technical implementation of the QuadB64 encoding system, covering everything from core algorithms to native extensions and performance optimization strategies.</p>"},{"location":"implementation/architecture/#core-architecture","title":"Core Architecture","text":""},{"location":"implementation/architecture/#algorithm-design-principles","title":"Algorithm Design Principles","text":"<p>The QuadB64 implementation follows several key design principles that distinguish it from traditional Base64:</p> <pre><code># Core encoding interface design\nclass QuadB64Encoder:\n    \"\"\"Base class for all QuadB64 variants\"\"\"\n\n    def __init__(self, position_safety=True, alphabet_rotation=True):\n        self.position_safety = position_safety\n        self.alphabet_rotation = alphabet_rotation\n        self._position_cache = {}\n        self._alphabet_cache = {}\n\n    def encode(self, data: bytes) -&gt; str:\n        \"\"\"Main encoding entry point\"\"\"\n        # 1. Input validation and preprocessing\n        if not isinstance(data, (bytes, bytearray)):\n            raise TypeError(\"Input must be bytes or bytearray\")\n\n        # 2. Position-aware chunking\n        chunks = self._chunk_data(data)\n\n        # 3. Parallel encoding with position context\n        encoded_chunks = self._encode_chunks_parallel(chunks)\n\n        # 4. Position-safe concatenation\n        return self._safe_concatenate(encoded_chunks)\n\n    def _chunk_data(self, data: bytes) -&gt; list:\n        \"\"\"Split data into position-aware chunks\"\"\"\n        chunk_size = 1024  # Optimized for cache locality\n        chunks = []\n\n        for i in range(0, len(data), chunk_size):\n            chunk = data[i:i + chunk_size]\n            position_context = {\n                'global_offset': i,\n                'chunk_index': i // chunk_size,\n                'total_chunks': (len(data) + chunk_size - 1) // chunk_size\n            }\n            chunks.append((chunk, position_context))\n\n        return chunks\n\n    def _encode_chunks_parallel(self, chunks: list) -&gt; list:\n        \"\"\"Encode chunks with position awareness\"\"\"\n        from concurrent.futures import ThreadPoolExecutor\n\n        # Use thread pool for I/O bound operations\n        # CPU-bound work delegated to native extensions\n        with ThreadPoolExecutor(max_workers=min(len(chunks), 8)) as executor:\n            futures = [\n                executor.submit(self._encode_single_chunk, chunk, context)\n                for chunk, context in chunks\n            ]\n\n            results = []\n            for future in futures:\n                try:\n                    encoded_chunk = future.result(timeout=10.0)\n                    results.append(encoded_chunk)\n                except Exception as e:\n                    raise RuntimeError(f\"Chunk encoding failed: {e}\")\n\n        return results\n\n    def _encode_single_chunk(self, chunk: bytes, context: dict) -&gt; str:\n        \"\"\"Encode a single chunk with position context\"\"\"\n        # Position-dependent alphabet selection\n        alphabet = self._get_position_alphabet(context['global_offset'])\n\n        # Native extension call for performance-critical work\n        if self._has_native_extension():\n            return self._native_encode_chunk(chunk, alphabet, context)\n        else:\n            return self._python_encode_chunk(chunk, alphabet, context)\n\n    def _get_position_alphabet(self, position: int) -&gt; str:\n        \"\"\"Get position-dependent alphabet with caching\"\"\"\n        if position in self._alphabet_cache:\n            return self._alphabet_cache[position]\n\n        # Position-dependent rotation to prevent substring pollution\n        base_alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\"\n        rotation = (position // 3) % 64  # Position-dependent rotation\n\n        rotated_alphabet = (base_alphabet[rotation:] + \n                          base_alphabet[:rotation])\n\n        self._alphabet_cache[position] = rotated_alphabet\n        return rotated_alphabet\n</code></pre>"},{"location":"implementation/architecture/#data-structures-and-memory-management","title":"Data Structures and Memory Management","text":""},{"location":"implementation/architecture/#efficient-buffer-management","title":"Efficient Buffer Management","text":"<pre><code>class MemoryPool:\n    \"\"\"Optimized memory management for encoding operations\"\"\"\n\n    def __init__(self, pool_size=1024*1024):  # 1MB default pool\n        self.pool_size = pool_size\n        self.buffers = {}\n        self.allocation_stats = {\n            'total_allocated': 0,\n            'peak_usage': 0,\n            'reuse_count': 0\n        }\n\n    def get_buffer(self, size: int) -&gt; bytearray:\n        \"\"\"Get a reusable buffer of specified size\"\"\"\n        # Round up to power of 2 for better reuse\n        actual_size = 1 &lt;&lt; (size - 1).bit_length()\n\n        if actual_size in self.buffers and self.buffers[actual_size]:\n            buffer = self.buffers[actual_size].pop()\n            self.allocation_stats['reuse_count'] += 1\n            return buffer\n\n        # Allocate new buffer\n        buffer = bytearray(actual_size)\n        self.allocation_stats['total_allocated'] += actual_size\n        self.allocation_stats['peak_usage'] = max(\n            self.allocation_stats['peak_usage'],\n            self.allocation_stats['total_allocated']\n        )\n\n        return buffer\n\n    def return_buffer(self, buffer: bytearray):\n        \"\"\"Return buffer to pool for reuse\"\"\"\n        size = len(buffer)\n        if size not in self.buffers:\n            self.buffers[size] = []\n\n        # Clear buffer contents for security\n        buffer[:] = b'\\x00' * len(buffer)\n        self.buffers[size].append(buffer)\n\n# Global memory pool instance\n_memory_pool = MemoryPool()\n</code></pre>"},{"location":"implementation/architecture/#position-aware-hash-tables","title":"Position-Aware Hash Tables","text":"<pre><code>class PositionHashTable:\n    \"\"\"Hash table optimized for position-dependent lookups\"\"\"\n\n    def __init__(self, initial_capacity=256):\n        self.capacity = initial_capacity\n        self.size = 0\n        self.buckets = [[] for _ in range(initial_capacity)]\n        self.load_factor_threshold = 0.75\n\n    def _hash(self, key: tuple) -&gt; int:\n        \"\"\"Position-aware hash function\"\"\"\n        data, position = key\n\n        # Combine data hash with position for better distribution\n        data_hash = hash(data)\n        position_hash = hash(position)\n\n        # Mix hashes to reduce collisions\n        combined = data_hash ^ (position_hash &lt;&lt; 16) ^ (position_hash &gt;&gt; 16)\n        return combined % self.capacity\n\n    def put(self, data: bytes, position: int, value: str):\n        \"\"\"Store encoded value with position context\"\"\"\n        if self.size &gt;= self.capacity * self.load_factor_threshold:\n            self._resize()\n\n        key = (data, position)\n        bucket_index = self._hash(key)\n        bucket = self.buckets[bucket_index]\n\n        # Check for existing entry\n        for i, (existing_key, existing_value) in enumerate(bucket):\n            if existing_key == key:\n                bucket[i] = (key, value)\n                return\n\n        # Add new entry\n        bucket.append((key, value))\n        self.size += 1\n\n    def get(self, data: bytes, position: int) -&gt; str:\n        \"\"\"Retrieve encoded value with position context\"\"\"\n        key = (data, position)\n        bucket_index = self._hash(key)\n        bucket = self.buckets[bucket_index]\n\n        for existing_key, value in bucket:\n            if existing_key == key:\n                return value\n\n        raise KeyError(f\"No entry found for key: {key}\")\n\n    def _resize(self):\n        \"\"\"Resize hash table when load factor exceeded\"\"\"\n        old_buckets = self.buckets\n        self.capacity *= 2\n        self.size = 0\n        self.buckets = [[] for _ in range(self.capacity)]\n\n        # Rehash all entries\n        for bucket in old_buckets:\n            for (data, position), value in bucket:\n                self.put(data, position, value)\n</code></pre>"},{"location":"implementation/architecture/#python-to-native-code-transition","title":"Python to Native Code Transition","text":""},{"location":"implementation/architecture/#native-extension-architecture","title":"Native Extension Architecture","text":"<p>The QuadB64 implementation uses native extensions (Rust/C++) for performance-critical operations while maintaining a clean Python API:</p> <pre><code># Python wrapper for native extensions\nimport ctypes\nimport os\nimport platform\n\nclass NativeExtensionLoader:\n    \"\"\"Dynamically load and manage native extensions\"\"\"\n\n    def __init__(self):\n        self.extensions = {}\n        self.capabilities = {}\n        self._detect_capabilities()\n\n    def _detect_capabilities(self):\n        \"\"\"Detect CPU and system capabilities\"\"\"\n        self.capabilities = {\n            'simd_avx2': self._has_avx2(),\n            'simd_sse4': self._has_sse4(),\n            'simd_neon': self._has_neon(),\n            'threading': True,\n            'platform': platform.machine(),\n            'pointer_size': ctypes.sizeof(ctypes.c_void_p)\n        }\n\n    def load_optimal_extension(self, variant: str):\n        \"\"\"Load the best available native extension\"\"\"\n        possible_extensions = []\n\n        # Platform-specific extension selection\n        if self.capabilities['platform'] in ['x86_64', 'AMD64']:\n            if self.capabilities['simd_avx2']:\n                possible_extensions.append(f\"uubed_{variant}_avx2\")\n            if self.capabilities['simd_sse4']:\n                possible_extensions.append(f\"uubed_{variant}_sse4\")\n\n        elif self.capabilities['platform'].startswith('arm'):\n            if self.capabilities['simd_neon']:\n                possible_extensions.append(f\"uubed_{variant}_neon\")\n\n        # Generic fallback\n        possible_extensions.append(f\"uubed_{variant}_generic\")\n\n        # Try loading extensions in order of preference\n        for ext_name in possible_extensions:\n            try:\n                library = self._load_library(ext_name)\n                self.extensions[variant] = library\n                return library\n            except OSError:\n                continue\n\n        raise RuntimeError(f\"No native extension available for {variant}\")\n\n    def _load_library(self, name: str):\n        \"\"\"Load native library with error handling\"\"\"\n        # Platform-specific library naming\n        if platform.system() == \"Windows\":\n            lib_name = f\"{name}.dll\"\n        elif platform.system() == \"Darwin\":\n            lib_name = f\"lib{name}.dylib\"\n        else:\n            lib_name = f\"lib{name}.so\"\n\n        # Search paths\n        search_paths = [\n            os.path.join(os.path.dirname(__file__), \"native\"),\n            os.path.join(os.path.dirname(__file__), \"..\", \"lib\"),\n            \"/usr/local/lib\",\n            \"/usr/lib\"\n        ]\n\n        for path in search_paths:\n            lib_path = os.path.join(path, lib_name)\n            if os.path.exists(lib_path):\n                return ctypes.CDLL(lib_path)\n\n        raise OSError(f\"Library {lib_name} not found in search paths\")\n\n# Native function signatures\nclass NativeEncoder:\n    \"\"\"Interface to native encoding functions\"\"\"\n\n    def __init__(self, library):\n        self.lib = library\n        self._setup_function_signatures()\n\n    def _setup_function_signatures(self):\n        \"\"\"Define C function signatures for Python calls\"\"\"\n\n        # encode_eq64_native(input_data, input_len, output_buffer, position, alphabet)\n        self.lib.encode_eq64_native.argtypes = [\n            ctypes.POINTER(ctypes.c_ubyte),  # input_data\n            ctypes.c_size_t,                 # input_len\n            ctypes.c_char_p,                 # output_buffer\n            ctypes.c_size_t,                 # position\n            ctypes.c_char_p                  # alphabet\n        ]\n        self.lib.encode_eq64_native.restype = ctypes.c_int\n\n        # encode_shq64_native for SimHash variant\n        self.lib.encode_shq64_native.argtypes = [\n            ctypes.POINTER(ctypes.c_ubyte),\n            ctypes.c_size_t,\n            ctypes.c_char_p,\n            ctypes.c_size_t,\n            ctypes.c_int  # hash_bits parameter\n        ]\n        self.lib.encode_shq64_native.restype = ctypes.c_int\n\n    def encode_eq64(self, data: bytes, position: int, alphabet: str) -&gt; str:\n        \"\"\"Call native Eq64 encoding\"\"\"\n        input_array = (ctypes.c_ubyte * len(data)).from_buffer_copy(data)\n\n        # Calculate output buffer size (4/3 * input + padding)\n        output_size = ((len(data) + 2) // 3) * 4 + 1\n        output_buffer = ctypes.create_string_buffer(output_size)\n\n        result = self.lib.encode_eq64_native(\n            input_array,\n            len(data),\n            output_buffer,\n            position,\n            alphabet.encode('ascii')\n        )\n\n        if result != 0:\n            raise RuntimeError(f\"Native encoding failed with code: {result}\")\n\n        return output_buffer.value.decode('ascii')\n</code></pre>"},{"location":"implementation/architecture/#simd-optimization-implementation","title":"SIMD Optimization Implementation","text":"<pre><code>// Example C implementation with SIMD optimization\n#include &lt;immintrin.h&gt;  // AVX2 support\n#include &lt;stdint.h&gt;\n#include &lt;string.h&gt;\n\n// AVX2-optimized encoding for 32 bytes at a time\nint encode_eq64_avx2(const uint8_t* input, size_t input_len, \n                     char* output, size_t position, const char* alphabet) {\n\n    if (input_len % 24 != 0) {\n        // Fall back to scalar implementation for partial blocks\n        return encode_eq64_scalar(input, input_len, output, position, alphabet);\n    }\n\n    // Load alphabet into AVX2 registers for fast lookups\n    __m256i alphabet_lo = _mm256_loadu_si256((__m256i*)alphabet);\n    __m256i alphabet_hi = _mm256_loadu_si256((__m256i*)(alphabet + 32));\n\n    size_t output_pos = 0;\n\n    for (size_t i = 0; i &lt; input_len; i += 24) {\n        // Load 24 input bytes\n        __m256i input_block = _mm256_loadu_si256((__m256i*)(input + i));\n\n        // Extract 6-bit groups using bit manipulation\n        __m256i indices = extract_6bit_groups_avx2(input_block);\n\n        // Apply position-dependent alphabet rotation\n        __m256i rotation = _mm256_set1_epi8((position + i) % 64);\n        indices = _mm256_add_epi8(indices, rotation);\n        indices = _mm256_and_si256(indices, _mm256_set1_epi8(0x3F));\n\n        // Lookup characters in alphabet using gather operations\n        __m256i encoded = alphabet_lookup_avx2(indices, alphabet_lo, alphabet_hi);\n\n        // Store 32 output characters\n        _mm256_storeu_si256((__m256i*)(output + output_pos), encoded);\n        output_pos += 32;\n    }\n\n    return 0;  // Success\n}\n\n__m256i extract_6bit_groups_avx2(__m256i input) {\n    // Complex bit manipulation to extract 6-bit groups\n    // This is a simplified version - actual implementation requires\n    // careful handling of byte boundaries\n\n    __m256i mask_6bit = _mm256_set1_epi8(0x3F);\n\n    // Shift and mask operations to extract 6-bit groups\n    __m256i group0 = _mm256_and_si256(_mm256_srli_epi32(input, 2), mask_6bit);\n    __m256i group1 = _mm256_and_si256(_mm256_srli_epi32(input, 8), mask_6bit);\n    __m256i group2 = _mm256_and_si256(_mm256_srli_epi32(input, 14), mask_6bit);\n    __m256i group3 = _mm256_and_si256(_mm256_srli_epi32(input, 20), mask_6bit);\n\n    // Pack groups into output register\n    return _mm256_packus_epi32(\n        _mm256_packus_epi32(group0, group1),\n        _mm256_packus_epi32(group2, group3)\n    );\n}\n</code></pre>"},{"location":"implementation/architecture/#cross-platform-compilation","title":"Cross-Platform Compilation","text":"<pre><code>// Rust implementation with cross-platform SIMD\nuse std::arch::x86_64::*;\nuse std::arch::aarch64::*;\n\n#[cfg(target_arch = \"x86_64\")]\n#[target_feature(enable = \"avx2\")]\nunsafe fn encode_eq64_simd(input: &amp;[u8], position: usize, alphabet: &amp;[u8; 64]) -&gt; String {\n    // AVX2 implementation for x86_64\n    encode_eq64_avx2(input, position, alphabet)\n}\n\n#[cfg(target_arch = \"aarch64\")]\n#[target_feature(enable = \"neon\")]\nunsafe fn encode_eq64_simd(input: &amp;[u8], position: usize, alphabet: &amp;[u8; 64]) -&gt; String {\n    // NEON implementation for ARM64\n    encode_eq64_neon(input, position, alphabet)\n}\n\n#[cfg(not(any(target_arch = \"x86_64\", target_arch = \"aarch64\")))]\nfn encode_eq64_simd(input: &amp;[u8], position: usize, alphabet: &amp;[u8; 64]) -&gt; String {\n    // Generic fallback implementation\n    encode_eq64_generic(input, position, alphabet)\n}\n\n// Build configuration in Cargo.toml\n/*\n[dependencies]\ncc = \"1.0\"\n\n[build-dependencies]\ncc = \"1.0\"\n\n[[bin]]\nname = \"uubed_encoder\"\nrequired-features = [\"simd\"]\n\n[features]\ndefault = [\"simd\"]\nsimd = []\navx2 = [\"simd\"]\nneon = [\"simd\"]\n*/\n</code></pre>"},{"location":"implementation/architecture/#thread-safety-and-concurrency","title":"Thread Safety and Concurrency","text":""},{"location":"implementation/architecture/#lock-free-data-structures","title":"Lock-Free Data Structures","text":"<pre><code>import threading\nimport queue\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n\nclass ThreadSafeEncoder:\n    \"\"\"Thread-safe QuadB64 encoder with work stealing\"\"\"\n\n    def __init__(self, max_workers=None):\n        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)\n        self.work_queue = queue.Queue()\n        self.result_cache = {}\n        self.cache_lock = threading.RLock()\n        self.stats = {\n            'total_requests': 0,\n            'cache_hits': 0,\n            'average_latency': 0.0\n        }\n        self.stats_lock = threading.Lock()\n\n    def encode_parallel(self, data_chunks: list) -&gt; list:\n        \"\"\"Encode multiple chunks in parallel\"\"\"\n        start_time = time.time()\n\n        # Submit work to thread pool\n        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Create futures for all chunks\n            futures = {\n                executor.submit(self._encode_with_cache, chunk, i): i \n                for i, chunk in enumerate(data_chunks)\n            }\n\n            # Collect results in original order\n            results = [None] * len(data_chunks)\n\n            for future in futures:\n                chunk_index = futures[future]\n                try:\n                    result = future.result(timeout=30.0)\n                    results[chunk_index] = result\n                except Exception as e:\n                    results[chunk_index] = f\"ERROR: {str(e)}\"\n\n        # Update statistics\n        end_time = time.time()\n        self._update_stats(len(data_chunks), end_time - start_time)\n\n        return results\n\n    def _encode_with_cache(self, data: bytes, position: int) -&gt; str:\n        \"\"\"Encode with thread-safe caching\"\"\"\n        cache_key = (hash(data), position)\n\n        # Try cache first\n        with self.cache_lock:\n            if cache_key in self.result_cache:\n                with self.stats_lock:\n                    self.stats['cache_hits'] += 1\n                return self.result_cache[cache_key]\n\n        # Encode data\n        result = self._encode_single(data, position)\n\n        # Update cache\n        with self.cache_lock:\n            # Implement LRU eviction if cache gets too large\n            if len(self.result_cache) &gt; 10000:\n                # Remove 20% of oldest entries\n                items_to_remove = len(self.result_cache) // 5\n                for _ in range(items_to_remove):\n                    self.result_cache.popitem(last=False)\n\n            self.result_cache[cache_key] = result\n\n        return result\n\n    def _update_stats(self, request_count: int, elapsed_time: float):\n        \"\"\"Update performance statistics\"\"\"\n        with self.stats_lock:\n            total = self.stats['total_requests']\n            self.stats['total_requests'] += request_count\n\n            # Update rolling average\n            old_avg = self.stats['average_latency']\n            new_avg = (old_avg * total + elapsed_time) / (total + request_count)\n            self.stats['average_latency'] = new_avg\n</code></pre>"},{"location":"implementation/architecture/#extension-development-framework","title":"Extension Development Framework","text":""},{"location":"implementation/architecture/#custom-variant-creation","title":"Custom Variant Creation","text":"<pre><code>class CustomVariantBuilder:\n    \"\"\"Framework for creating custom QuadB64 variants\"\"\"\n\n    def __init__(self):\n        self.alphabet_generators = {}\n        self.position_functions = {}\n        self.optimization_plugins = {}\n\n    def register_alphabet_generator(self, name: str, generator_func):\n        \"\"\"Register custom alphabet generation function\"\"\"\n        self.alphabet_generators[name] = generator_func\n\n    def register_position_function(self, name: str, position_func):\n        \"\"\"Register custom position-dependent transformation\"\"\"\n        self.position_functions[name] = position_func\n\n    def create_variant(self, name: str, config: dict):\n        \"\"\"Create a new QuadB64 variant with custom configuration\"\"\"\n\n        variant_class = type(f\"{name}Encoder\", (QuadB64Encoder,), {\n            '_alphabet_generator': self.alphabet_generators[config['alphabet']],\n            '_position_function': self.position_functions[config['position']],\n            '_config': config\n        })\n\n        # Add custom methods\n        if 'pre_process' in config:\n            variant_class.pre_process = config['pre_process']\n\n        if 'post_process' in config:\n            variant_class.post_process = config['post_process']\n\n        return variant_class\n\n# Example custom variant\ndef scientific_alphabet_generator(position: int) -&gt; str:\n    \"\"\"Generate alphabet optimized for scientific data\"\"\"\n    # Prioritize numbers and common scientific notation\n    base = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz+/\"\n    rotation = (position * 7) % 64  # Different rotation pattern\n    return base[rotation:] + base[:rotation]\n\ndef temporal_position_function(position: int, timestamp: float) -&gt; int:\n    \"\"\"Position function that incorporates temporal information\"\"\"\n    # Combine position with timestamp for time-dependent encoding\n    time_factor = int(timestamp) % 1000\n    return (position + time_factor) % (2**32)\n\n# Register and create custom variant\nbuilder = CustomVariantBuilder()\nbuilder.register_alphabet_generator(\"scientific\", scientific_alphabet_generator)\nbuilder.register_position_function(\"temporal\", temporal_position_function)\n\nScientificEncoder = builder.create_variant(\"Scientific\", {\n    \"alphabet\": \"scientific\",\n    \"position\": \"temporal\",\n    \"optimize_for\": \"numerical_data\"\n})\n</code></pre>"},{"location":"implementation/architecture/#performance-profiling-and-optimization","title":"Performance Profiling and Optimization","text":""},{"location":"implementation/architecture/#built-in-profiling-tools","title":"Built-in Profiling Tools","text":"<pre><code>import cProfile\nimport pstats\nimport io\nfrom contextlib import contextmanager\n\nclass PerformanceProfiler:\n    \"\"\"Built-in profiling tools for QuadB64 operations\"\"\"\n\n    def __init__(self):\n        self.profiles = {}\n        self.benchmarks = {}\n\n    @contextmanager\n    def profile_operation(self, operation_name: str):\n        \"\"\"Context manager for profiling specific operations\"\"\"\n        profiler = cProfile.Profile()\n        profiler.enable()\n\n        start_time = time.perf_counter()\n        try:\n            yield profiler\n        finally:\n            end_time = time.perf_counter()\n            profiler.disable()\n\n            # Store profile results\n            profile_output = io.StringIO()\n            stats = pstats.Stats(profiler, stream=profile_output)\n            stats.sort_stats('cumulative').print_stats()\n\n            self.profiles[operation_name] = {\n                'duration': end_time - start_time,\n                'profile_data': profile_output.getvalue(),\n                'timestamp': time.time()\n            }\n\n    def benchmark_encoding_variants(self, test_data: list) -&gt; dict:\n        \"\"\"Benchmark different encoding variants\"\"\"\n        from uubed import encode_eq64, encode_shq64, encode_t8q64, encode_zoq64\n\n        variants = {\n            'Eq64': encode_eq64,\n            'Shq64': encode_shq64,\n            'T8q64': encode_t8q64,\n            'Zoq64': encode_zoq64\n        }\n\n        results = {}\n\n        for variant_name, encode_func in variants.items():\n            with self.profile_operation(f\"benchmark_{variant_name}\"):\n                times = []\n\n                for data in test_data:\n                    start = time.perf_counter()\n                    encoded = encode_func(data)\n                    end = time.perf_counter()\n                    times.append(end - start)\n\n                results[variant_name] = {\n                    'mean_time': sum(times) / len(times),\n                    'min_time': min(times),\n                    'max_time': max(times),\n                    'total_time': sum(times),\n                    'throughput_mb_s': sum(len(d) for d in test_data) / sum(times) / 1024 / 1024\n                }\n\n        return results\n\n    def generate_performance_report(self) -&gt; str:\n        \"\"\"Generate comprehensive performance report\"\"\"\n        report = [\"QuadB64 Performance Analysis Report\"]\n        report.append(\"=\" * 50)\n\n        for operation, data in self.profiles.items():\n            report.append(f\"\\nOperation: {operation}\")\n            report.append(f\"Duration: {data['duration']:.4f} seconds\")\n            report.append(f\"Timestamp: {time.ctime(data['timestamp'])}\")\n            report.append(\"-\" * 30)\n\n            # Extract key statistics from profile data\n            lines = data['profile_data'].split('\\n')\n            for line in lines[:20]:  # Top 20 lines\n                if 'cumulative' in line or 'function calls' in line:\n                    report.append(line)\n\n        return '\\n'.join(report)\n\n# Usage example\nprofiler = PerformanceProfiler()\n\n# Profile encoding operation\ntest_data = [b\"sample data\" * 100 for _ in range(1000)]\nwith profiler.profile_operation(\"large_dataset_encoding\"):\n    results = [encode_eq64(data) for data in test_data]\n\n# Generate benchmark report\nbenchmark_results = profiler.benchmark_encoding_variants(test_data[:100])\nperformance_report = profiler.generate_performance_report()\n\nprint(performance_report)\n</code></pre>"},{"location":"implementation/architecture/#conclusion","title":"Conclusion","text":"<p>The QuadB64 implementation architecture balances performance, maintainability, and extensibility through careful design choices:</p> <ol> <li>Modular Design: Clear separation between Python API and native extensions</li> <li>Performance Optimization: SIMD instructions and memory pooling for critical paths</li> <li>Thread Safety: Lock-free data structures and careful synchronization</li> <li>Extensibility: Plugin architecture for custom variants and optimizations</li> <li>Profiling: Built-in tools for performance analysis and optimization</li> </ol> <p>This architecture enables QuadB64 to scale from small embedded applications to large-scale distributed systems while maintaining the position-safety guarantees that eliminate substring pollution.</p>"},{"location":"implementation/native-extensions/","title":"Native Extensions Guide","text":""},{"location":"implementation/native-extensions/#overview","title":"Overview","text":"<p>QuadB64 achieves dramatic performance improvements through optimized native extensions written in Rust and C++. This guide covers the architecture, compilation, optimization techniques, and platform-specific considerations for native extensions.</p>"},{"location":"implementation/native-extensions/#performance-impact","title":"Performance Impact","text":"<p>Native extensions provide substantial performance improvements:</p> <ul> <li>Pure Python: 38 MB/s encoding throughput</li> <li>Native Extensions: 115 MB/s (3x improvement)</li> <li>Native + SIMD: 360 MB/s (9.5x improvement)</li> <li>Memory Usage: 15% reduction through optimized buffer management</li> </ul>"},{"location":"implementation/native-extensions/#architecture-overview","title":"Architecture Overview","text":""},{"location":"implementation/native-extensions/#language-choice-rationale","title":"Language Choice Rationale","text":"<p>QuadB64 native extensions use Rust as the primary implementation language for several key reasons:</p> <ol> <li>Memory Safety: Zero-cost abstractions with compile-time memory safety</li> <li>Performance: Comparable to C/C++ with better safety guarantees</li> <li>Cross-Platform: Excellent support for diverse architectures</li> <li>SIMD Support: First-class intrinsics for vectorized operations</li> <li>Python Integration: Mature PyO3 ecosystem for Python bindings</li> </ol>"},{"location":"implementation/native-extensions/#core-extension-structure","title":"Core Extension Structure","text":"<pre><code>// src/lib.rs - Main entry point\nuse pyo3::prelude::*;\n\n#[pymodule]\nfn uubed_native(_py: Python, m: &amp;PyModule) -&gt; PyResult&lt;()&gt; {\n    m.add_function(wrap_pyfunction!(encode_eq64_native, m)?)?;\n    m.add_function(wrap_pyfunction!(encode_shq64_native, m)?)?;\n    m.add_function(wrap_pyfunction!(encode_t8q64_native, m)?)?;\n    m.add_function(wrap_pyfunction!(encode_zoq64_native, m)?)?;\n    m.add_function(wrap_pyfunction!(has_simd_support, m)?)?;\n    Ok(())\n}\n\n// High-level encoding interface\n#[pyfunction]\nfn encode_eq64_native(\n    data: &amp;[u8],\n    position: usize,\n    alphabet: Option&lt;&amp;str&gt;\n) -&gt; PyResult&lt;String&gt; {\n    match encode_eq64_optimized(data, position, alphabet) {\n        Ok(result) =&gt; Ok(result),\n        Err(e) =&gt; Err(PyErr::new::&lt;pyo3::exceptions::PyRuntimeError, _&gt;(\n            format!(\"Encoding failed: {}\", e)\n        ))\n    }\n}\n</code></pre>"},{"location":"implementation/native-extensions/#performance-critical-core","title":"Performance-Critical Core","text":"<pre><code>// src/encoding/eq64.rs - Optimized Eq64 implementation\nuse std::arch::x86_64::*;\n\npub fn encode_eq64_optimized(\n    data: &amp;[u8], \n    position: usize, \n    alphabet: Option&lt;&amp;str&gt;\n) -&gt; Result&lt;String, Box&lt;dyn std::error::Error&gt;&gt; {\n\n    // Select optimal implementation based on capabilities\n    if is_x86_feature_detected!(\"avx2\") {\n        unsafe { encode_eq64_avx2(data, position, alphabet) }\n    } else if is_x86_feature_detected!(\"sse4.1\") {\n        unsafe { encode_eq64_sse4(data, position, alphabet) }\n    } else {\n        encode_eq64_scalar(data, position, alphabet)\n    }\n}\n\n// AVX2-optimized implementation for x86_64\n#[target_feature(enable = \"avx2\")]\nunsafe fn encode_eq64_avx2(\n    data: &amp;[u8],\n    position: usize,\n    alphabet: Option&lt;&amp;str&gt;\n) -&gt; Result&lt;String, Box&lt;dyn std::error::Error&gt;&gt; {\n\n    let alphabet = alphabet.unwrap_or(DEFAULT_ALPHABET);\n    let mut result = Vec::with_capacity((data.len() + 2) / 3 * 4);\n\n    // Process 24-byte chunks with AVX2 (produces 32 output characters)\n    let chunks = data.chunks_exact(24);\n    let remainder = chunks.remainder();\n\n    for (chunk_idx, chunk) in chunks.enumerate() {\n        let chunk_position = position + chunk_idx * 24;\n        let encoded_chunk = encode_chunk_avx2(chunk, chunk_position, alphabet)?;\n        result.extend_from_slice(encoded_chunk.as_bytes());\n    }\n\n    // Handle remainder with scalar code\n    if !remainder.is_empty() {\n        let remainder_position = position + data.len() - remainder.len();\n        let encoded_remainder = encode_eq64_scalar(remainder, remainder_position, Some(alphabet))?;\n        result.extend_from_slice(encoded_remainder.as_bytes());\n    }\n\n    String::from_utf8(result).map_err(|e| e.into())\n}\n\n// Core AVX2 chunk processing\n#[target_feature(enable = \"avx2\")]\nunsafe fn encode_chunk_avx2(\n    chunk: &amp;[u8],\n    position: usize,\n    alphabet: &amp;str\n) -&gt; Result&lt;String, Box&lt;dyn std::error::Error&gt;&gt; {\n\n    debug_assert_eq!(chunk.len(), 24);\n\n    // Load 24 input bytes into AVX2 register\n    let input = _mm256_loadu_si256(chunk.as_ptr() as *const __m256i);\n\n    // Position-dependent alphabet rotation\n    let rotation = ((position / 3) % 64) as u8;\n    let alphabet_bytes = alphabet.as_bytes();\n\n    // Extract 6-bit groups using bit manipulation\n    let indices = extract_6bit_groups_avx2(input);\n\n    // Apply position-dependent rotation\n    let rotated_indices = apply_rotation_avx2(indices, rotation);\n\n    // Lookup characters in alphabet\n    let output_chars = alphabet_lookup_avx2(rotated_indices, alphabet_bytes);\n\n    // Convert to string\n    let mut result = String::with_capacity(32);\n    let output_slice = std::slice::from_raw_parts(\n        &amp;output_chars as *const __m256i as *const u8,\n        32\n    );\n\n    for &amp;byte in output_slice {\n        result.push(byte as char);\n    }\n\n    Ok(result)\n}\n\n// Extract 6-bit groups from 256-bit input\n#[target_feature(enable = \"avx2\")]\nunsafe fn extract_6bit_groups_avx2(input: __m256i) -&gt; __m256i {\n    // Complex bit manipulation to extract 6-bit groups from 24-byte input\n    // This involves careful handling of byte boundaries and bit alignment\n\n    // Mask for 6-bit values\n    let mask_6bit = _mm256_set1_epi8(0x3F);\n\n    // Shift operations to align 6-bit groups\n    let shifted_2 = _mm256_srli_epi32(input, 2);\n    let shifted_8 = _mm256_srli_epi32(input, 8);\n    let shifted_14 = _mm256_srli_epi32(input, 14);\n    let shifted_20 = _mm256_srli_epi32(input, 20);\n\n    // Extract and pack 6-bit groups\n    let group0 = _mm256_and_si256(shifted_2, mask_6bit);\n    let group1 = _mm256_and_si256(shifted_8, mask_6bit);\n    let group2 = _mm256_and_si256(shifted_14, mask_6bit);\n    let group3 = _mm256_and_si256(shifted_20, mask_6bit);\n\n    // Pack into output register\n    _mm256_packus_epi32(\n        _mm256_packus_epi32(group0, group1),\n        _mm256_packus_epi32(group2, group3)\n    )\n}\n\n// Apply position-dependent alphabet rotation\n#[target_feature(enable = \"avx2\")]\nunsafe fn apply_rotation_avx2(indices: __m256i, rotation: u8) -&gt; __m256i {\n    let rotation_vec = _mm256_set1_epi8(rotation as i8);\n    let rotated = _mm256_add_epi8(indices, rotation_vec);\n    let mask_64 = _mm256_set1_epi8(0x3F);\n    _mm256_and_si256(rotated, mask_64)\n}\n\n// Alphabet character lookup with AVX2\n#[target_feature(enable = \"avx2\")]\nunsafe fn alphabet_lookup_avx2(indices: __m256i, alphabet: &amp;[u8]) -&gt; __m256i {\n    // Use gather operations or lookup table for character mapping\n    // This is a simplified version - actual implementation requires\n    // careful handling of alphabet indexing\n\n    let mut result = [0u8; 32];\n    let indices_array = std::mem::transmute::&lt;__m256i, [u8; 32]&gt;(indices);\n\n    for (i, &amp;index) in indices_array.iter().enumerate() {\n        result[i] = alphabet[(index &amp; 0x3F) as usize];\n    }\n\n    _mm256_loadu_si256(result.as_ptr() as *const __m256i)\n}\n</code></pre>"},{"location":"implementation/native-extensions/#arm-neon-implementation","title":"ARM NEON Implementation","text":"<pre><code>// src/encoding/neon.rs - ARM NEON optimizations\n#[cfg(target_arch = \"aarch64\")]\nuse std::arch::aarch64::*;\n\n#[cfg(target_arch = \"aarch64\")]\n#[target_feature(enable = \"neon\")]\nunsafe fn encode_eq64_neon(\n    data: &amp;[u8],\n    position: usize,\n    alphabet: &amp;str\n) -&gt; Result&lt;String, Box&lt;dyn std::error::Error&gt;&gt; {\n\n    let mut result = Vec::with_capacity((data.len() + 2) / 3 * 4);\n\n    // Process 12-byte chunks with NEON (produces 16 output characters)\n    let chunks = data.chunks_exact(12);\n    let remainder = chunks.remainder();\n\n    for (chunk_idx, chunk) in chunks.enumerate() {\n        let chunk_position = position + chunk_idx * 12;\n        let encoded_chunk = encode_chunk_neon(chunk, chunk_position, alphabet)?;\n        result.extend_from_slice(encoded_chunk.as_bytes());\n    }\n\n    // Handle remainder\n    if !remainder.is_empty() {\n        let remainder_position = position + data.len() - remainder.len();\n        let encoded_remainder = encode_eq64_scalar(remainder, remainder_position, Some(alphabet))?;\n        result.extend_from_slice(encoded_remainder.as_bytes());\n    }\n\n    String::from_utf8(result).map_err(|e| e.into())\n}\n\n#[cfg(target_arch = \"aarch64\")]\n#[target_feature(enable = \"neon\")]\nunsafe fn encode_chunk_neon(\n    chunk: &amp;[u8],\n    position: usize,\n    alphabet: &amp;str\n) -&gt; Result&lt;String, Box&lt;dyn std::error::Error&gt;&gt; {\n\n    // Load 12 bytes into NEON register\n    let input = vld1q_u8(chunk.as_ptr());\n\n    // Position-dependent rotation\n    let rotation = ((position / 3) % 64) as u8;\n\n    // Extract 6-bit groups using NEON operations\n    let indices = extract_6bit_groups_neon(input);\n\n    // Apply rotation\n    let rotation_vec = vdupq_n_u8(rotation);\n    let rotated_indices = vaddq_u8(indices, rotation_vec);\n    let mask = vdupq_n_u8(0x3F);\n    let final_indices = vandq_u8(rotated_indices, mask);\n\n    // Character lookup\n    let alphabet_bytes = alphabet.as_bytes();\n    let mut result = String::with_capacity(16);\n\n    let indices_array: [u8; 16] = std::mem::transmute(final_indices);\n    for &amp;index in &amp;indices_array {\n        result.push(alphabet_bytes[(index &amp; 0x3F) as usize] as char);\n    }\n\n    Ok(result)\n}\n</code></pre>"},{"location":"implementation/native-extensions/#memory-management","title":"Memory Management","text":""},{"location":"implementation/native-extensions/#custom-allocator-integration","title":"Custom Allocator Integration","text":"<pre><code>// src/memory/allocator.rs - Custom memory management\nuse std::alloc::{GlobalAlloc, Layout};\nuse std::ptr::NonNull;\n\npub struct QuadB64Allocator {\n    pool: MemoryPool,\n    fallback: std::alloc::System,\n}\n\nimpl QuadB64Allocator {\n    pub fn new(pool_size: usize) -&gt; Self {\n        Self {\n            pool: MemoryPool::new(pool_size),\n            fallback: std::alloc::System,\n        }\n    }\n}\n\nunsafe impl GlobalAlloc for QuadB64Allocator {\n    unsafe fn alloc(&amp;self, layout: Layout) -&gt; *mut u8 {\n        // Use pool for small, frequent allocations\n        if layout.size() &lt;= 8192 &amp;&amp; layout.align() &lt;= 64 {\n            match self.pool.allocate(layout) {\n                Some(ptr) =&gt; ptr.as_ptr(),\n                None =&gt; self.fallback.alloc(layout),\n            }\n        } else {\n            self.fallback.alloc(layout)\n        }\n    }\n\n    unsafe fn dealloc(&amp;self, ptr: *mut u8, layout: Layout) {\n        if !self.pool.deallocate(NonNull::new_unchecked(ptr), layout) {\n            self.fallback.dealloc(ptr, layout);\n        }\n    }\n}\n\n// Memory pool implementation\nstruct MemoryPool {\n    blocks: Vec&lt;MemoryBlock&gt;,\n    free_list: Vec&lt;NonNull&lt;u8&gt;&gt;,\n    stats: PoolStats,\n}\n\nimpl MemoryPool {\n    fn new(size: usize) -&gt; Self {\n        // Initialize memory pool with pre-allocated blocks\n        Self {\n            blocks: Vec::new(),\n            free_list: Vec::new(),\n            stats: PoolStats::default(),\n        }\n    }\n\n    fn allocate(&amp;self, layout: Layout) -&gt; Option&lt;NonNull&lt;u8&gt;&gt; {\n        // Fast path: Check free list\n        if let Some(ptr) = self.free_list.pop() {\n            Some(ptr)\n        } else {\n            // Allocate new block if pool has space\n            self.allocate_new_block(layout)\n        }\n    }\n\n    fn deallocate(&amp;self, ptr: NonNull&lt;u8&gt;, layout: Layout) -&gt; bool {\n        // Return to free list if it belongs to our pool\n        if self.owns_pointer(ptr) {\n            self.free_list.push(ptr);\n            true\n        } else {\n            false\n        }\n    }\n}\n</code></pre>"},{"location":"implementation/native-extensions/#buffer-management","title":"Buffer Management","text":"<pre><code>// src/memory/buffers.rs - Optimized buffer handling\npub struct BufferManager {\n    small_buffers: Vec&lt;Vec&lt;u8&gt;&gt;,     // &lt; 1KB\n    medium_buffers: Vec&lt;Vec&lt;u8&gt;&gt;,    // 1KB - 64KB  \n    large_buffers: Vec&lt;Vec&lt;u8&gt;&gt;,     // &gt; 64KB\n    stats: BufferStats,\n}\n\nimpl BufferManager {\n    pub fn get_buffer(&amp;mut self, size: usize) -&gt; Vec&lt;u8&gt; {\n        let pool = match size {\n            0..=1024 =&gt; &amp;mut self.small_buffers,\n            1025..=65536 =&gt; &amp;mut self.medium_buffers,\n            _ =&gt; &amp;mut self.large_buffers,\n        };\n\n        // Reuse existing buffer or allocate new one\n        pool.pop().unwrap_or_else(|| {\n            self.stats.allocations += 1;\n            Vec::with_capacity(size)\n        })\n    }\n\n    pub fn return_buffer(&amp;mut self, mut buffer: Vec&lt;u8&gt;) {\n        // Clear and return to appropriate pool\n        buffer.clear();\n\n        let pool = match buffer.capacity() {\n            0..=1024 =&gt; &amp;mut self.small_buffers,\n            1025..=65536 =&gt; &amp;mut self.medium_buffers,\n            _ =&gt; &amp;mut self.large_buffers,\n        };\n\n        // Limit pool size to prevent memory leaks\n        if pool.len() &lt; 100 {\n            pool.push(buffer);\n            self.stats.reuses += 1;\n        }\n    }\n}\n</code></pre>"},{"location":"implementation/native-extensions/#compilation-and-build-system","title":"Compilation and Build System","text":""},{"location":"implementation/native-extensions/#cargo-configuration","title":"Cargo Configuration","text":"<pre><code># Cargo.toml - Rust build configuration\n[package]\nname = \"uubed-native\"\nversion = \"0.5.0\"\nedition = \"2021\"\nrust-version = \"1.70\"\n\n[lib]\nname = \"uubed_native\"\ncrate-type = [\"cdylib\"]\n\n[dependencies]\npyo3 = { version = \"0.20\", features = [\"extension-module\"] }\nnumpy = \"0.20\"\nrayon = \"1.7\"\n\n[features]\ndefault = [\"simd\"]\nsimd = []\navx2 = [\"simd\"]\navx512 = [\"simd\"]\nneon = [\"simd\"]\n\n[profile.release]\nopt-level = 3\nlto = true\ncodegen-units = 1\npanic = \"abort\"\n\n# Platform-specific optimizations\n[target.'cfg(target_arch = \"x86_64\")'.dependencies]\nraw-cpuid = \"10.0\"\n\n[target.'cfg(target_arch = \"aarch64\")'.dependencies]\n# ARM-specific dependencies\n\n# Build script for feature detection\n[build-dependencies]\ncc = \"1.0\"\n</code></pre>"},{"location":"implementation/native-extensions/#build-script","title":"Build Script","text":"<pre><code>// build.rs - Feature detection and compilation\nuse std::env;\nuse std::process::Command;\n\nfn main() {\n    println!(\"cargo:rerun-if-changed=build.rs\");\n\n    let target_arch = env::var(\"CARGO_CFG_TARGET_ARCH\").unwrap();\n    let target_os = env::var(\"CARGO_CFG_TARGET_OS\").unwrap();\n\n    // Detect CPU features\n    detect_cpu_features(&amp;target_arch);\n\n    // Configure platform-specific builds\n    configure_platform_build(&amp;target_arch, &amp;target_os);\n\n    // Build C++ components if needed\n    build_cpp_components();\n}\n\nfn detect_cpu_features(arch: &amp;str) {\n    match arch {\n        \"x86_64\" =&gt; {\n            if has_feature(\"avx2\") {\n                println!(\"cargo:rustc-cfg=has_avx2\");\n            }\n            if has_feature(\"avx512f\") {\n                println!(\"cargo:rustc-cfg=has_avx512\");\n            }\n        }\n        \"aarch64\" =&gt; {\n            if has_feature(\"neon\") {\n                println!(\"cargo:rustc-cfg=has_neon\");\n            }\n        }\n        _ =&gt; {}\n    }\n}\n\nfn has_feature(feature: &amp;str) -&gt; bool {\n    // Use runtime feature detection\n    match feature {\n        \"avx2\" =&gt; is_x86_feature_detected!(\"avx2\"),\n        \"avx512f\" =&gt; is_x86_feature_detected!(\"avx512f\"),\n        \"neon\" =&gt; cfg!(target_feature = \"neon\"),\n        _ =&gt; false,\n    }\n}\n\nfn configure_platform_build(arch: &amp;str, os: &amp;str) {\n    // Platform-specific compiler flags\n    let mut build = cc::Build::new();\n\n    build\n        .cpp(true)\n        .std(\"c++17\")\n        .flag(\"-O3\")\n        .flag(\"-march=native\");\n\n    match arch {\n        \"x86_64\" =&gt; {\n            build.flag(\"-mavx2\").flag(\"-mfma\");\n        }\n        \"aarch64\" =&gt; {\n            build.flag(\"-march=armv8-a+simd\");\n        }\n        _ =&gt; {}\n    }\n\n    match os {\n        \"windows\" =&gt; {\n            build.flag(\"/arch:AVX2\");\n        }\n        \"macos\" =&gt; {\n            build.flag(\"-stdlib=libc++\");\n        }\n        _ =&gt; {}\n    }\n\n    build.compile(\"uubed_cpp\");\n}\n</code></pre>"},{"location":"implementation/native-extensions/#python-integration","title":"Python Integration","text":"<pre><code># setup.py - Python wheel building\nfrom pyo3_pack import build_wheel\nimport platform\nimport subprocess\nimport os\n\ndef build_native_extensions():\n    \"\"\"Build native extensions with optimal configuration\"\"\"\n\n    # Detect CPU capabilities\n    cpu_features = detect_cpu_features()\n\n    # Configure Rust build\n    rust_flags = configure_rust_build(cpu_features)\n\n    # Set environment variables\n    env = os.environ.copy()\n    env['RUSTFLAGS'] = rust_flags\n    env['CARGO_BUILD_TARGET'] = get_build_target()\n\n    # Build wheel\n    build_wheel(\n        manifest_path=\"native/Cargo.toml\",\n        target_dir=\"target\",\n        release=True,\n        strip=True,\n        env=env\n    )\n\ndef detect_cpu_features():\n    \"\"\"Detect CPU features for optimization\"\"\"\n    features = []\n\n    if platform.machine() in ['x86_64', 'AMD64']:\n        # Check for x86 features\n        if has_x86_feature('avx2'):\n            features.append('avx2')\n        if has_x86_feature('avx512f'):\n            features.append('avx512')\n    elif platform.machine().startswith('arm') or platform.machine() == 'aarch64':\n        # Check for ARM features\n        features.append('neon')\n\n    return features\n\ndef has_x86_feature(feature):\n    \"\"\"Check if x86 CPU feature is available\"\"\"\n    try:\n        result = subprocess.run(\n            ['python', '-c', f'import cpuid; print(cpuid.has_{feature}())'],\n            capture_output=True,\n            text=True\n        )\n        return result.stdout.strip() == 'True'\n    except:\n        return False\n\ndef configure_rust_build(features):\n    \"\"\"Configure Rust compiler flags\"\"\"\n    flags = ['-C', 'target-cpu=native']\n\n    if 'avx2' in features:\n        flags.extend(['-C', 'target-feature=+avx2'])\n    if 'avx512' in features:\n        flags.extend(['-C', 'target-feature=+avx512f'])\n    if 'neon' in features:\n        flags.extend(['-C', 'target-feature=+neon'])\n\n    return ' '.join(flags)\n\nif __name__ == \"__main__\":\n    build_native_extensions()\n</code></pre>"},{"location":"implementation/native-extensions/#performance-profiling","title":"Performance Profiling","text":""},{"location":"implementation/native-extensions/#built-in-profiling-tools","title":"Built-in Profiling Tools","text":"<pre><code>// src/profiling/mod.rs - Performance profiling\nuse std::time::{Duration, Instant};\nuse std::collections::HashMap;\n\npub struct PerformanceProfiler {\n    timings: HashMap&lt;String, Vec&lt;Duration&gt;&gt;,\n    counters: HashMap&lt;String, u64&gt;,\n    active_timers: HashMap&lt;String, Instant&gt;,\n}\n\nimpl PerformanceProfiler {\n    pub fn new() -&gt; Self {\n        Self {\n            timings: HashMap::new(),\n            counters: HashMap::new(),\n            active_timers: HashMap::new(),\n        }\n    }\n\n    pub fn start_timer(&amp;mut self, name: &amp;str) {\n        self.active_timers.insert(name.to_string(), Instant::now());\n    }\n\n    pub fn end_timer(&amp;mut self, name: &amp;str) {\n        if let Some(start_time) = self.active_timers.remove(name) {\n            let duration = start_time.elapsed();\n            self.timings.entry(name.to_string())\n                .or_insert_with(Vec::new)\n                .push(duration);\n        }\n    }\n\n    pub fn increment_counter(&amp;mut self, name: &amp;str) {\n        *self.counters.entry(name.to_string()).or_insert(0) += 1;\n    }\n\n    pub fn generate_report(&amp;self) -&gt; String {\n        let mut report = String::new();\n        report.push_str(\"=== Performance Profile Report ===\\n\\n\");\n\n        // Timing statistics\n        report.push_str(\"Timing Statistics:\\n\");\n        for (name, times) in &amp;self.timings {\n            let total: Duration = times.iter().sum();\n            let avg = total / times.len() as u32;\n            let min = times.iter().min().unwrap();\n            let max = times.iter().max().unwrap();\n\n            report.push_str(&amp;format!(\n                \"{}: {} calls, avg: {:?}, min: {:?}, max: {:?}, total: {:?}\\n\",\n                name, times.len(), avg, min, max, total\n            ));\n        }\n\n        // Counter statistics\n        report.push_str(\"\\nCounter Statistics:\\n\");\n        for (name, count) in &amp;self.counters {\n            report.push_str(&amp;format!(\"{}: {}\\n\", name, count));\n        }\n\n        report\n    }\n}\n\n// Profiling macros for easy integration\n#[macro_export]\nmacro_rules! profile_scope {\n    ($profiler:expr, $name:expr, $block:block) =&gt; {\n        $profiler.start_timer($name);\n        let result = $block;\n        $profiler.end_timer($name);\n        result\n    };\n}\n</code></pre>"},{"location":"implementation/native-extensions/#benchmarking-framework","title":"Benchmarking Framework","text":"<pre><code>// src/benchmarks/mod.rs - Comprehensive benchmarks\nuse criterion::{criterion_group, criterion_main, Criterion, BenchmarkId};\nuse std::time::Duration;\n\npub fn benchmark_encoding_variants(c: &amp;mut Criterion) {\n    let test_sizes = vec![64, 256, 1024, 4096, 16384, 65536];\n\n    let mut group = c.benchmark_group(\"encoding_variants\");\n    group.measurement_time(Duration::from_secs(10));\n\n    for size in test_sizes {\n        let data = vec![0xAA; size];\n\n        // Benchmark scalar implementation\n        group.bench_with_input(\n            BenchmarkId::new(\"scalar\", size),\n            &amp;data,\n            |b, data| {\n                b.iter(|| encode_eq64_scalar(data, 0, None))\n            }\n        );\n\n        // Benchmark SIMD implementation\n        #[cfg(target_feature = \"avx2\")]\n        group.bench_with_input(\n            BenchmarkId::new(\"avx2\", size),\n            &amp;data,\n            |b, data| {\n                b.iter(|| unsafe { encode_eq64_avx2(data, 0, None) })\n            }\n        );\n\n        // Benchmark against base64\n        group.bench_with_input(\n            BenchmarkId::new(\"base64_reference\", size),\n            &amp;data,\n            |b, data| {\n                b.iter(|| base64::encode(data))\n            }\n        );\n    }\n\n    group.finish();\n}\n\npub fn benchmark_memory_patterns(c: &amp;mut Criterion) {\n    let mut group = c.benchmark_group(\"memory_patterns\");\n\n    // Test different allocation strategies\n    group.bench_function(\"stack_allocation\", |b| {\n        b.iter(|| {\n            let buffer = [0u8; 1024];\n            encode_with_stack_buffer(&amp;buffer)\n        })\n    });\n\n    group.bench_function(\"heap_allocation\", |b| {\n        b.iter(|| {\n            let buffer = vec![0u8; 1024];\n            encode_with_heap_buffer(&amp;buffer)\n        })\n    });\n\n    group.bench_function(\"pool_allocation\", |b| {\n        let mut pool = BufferPool::new();\n        b.iter(|| {\n            let buffer = pool.get_buffer(1024);\n            let result = encode_with_pooled_buffer(&amp;buffer);\n            pool.return_buffer(buffer);\n            result\n        })\n    });\n\n    group.finish();\n}\n\ncriterion_group!(benches, benchmark_encoding_variants, benchmark_memory_patterns);\ncriterion_main!(benches);\n</code></pre>"},{"location":"implementation/native-extensions/#deployment-and-distribution","title":"Deployment and Distribution","text":""},{"location":"implementation/native-extensions/#wheel-building","title":"Wheel Building","text":"<pre><code># .github/workflows/build-wheels.yml - CI/CD for wheel building\nname: Build Native Wheels\n\non:\n  push:\n    tags: ['v*']\n  pull_request:\n\njobs:\n  build_wheels:\n    name: Build wheels on ${{ matrix.os }}\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n        architecture: [x86_64, aarch64]\n        exclude:\n          - os: windows-latest\n            architecture: aarch64\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Rust\n      uses: actions-rs/toolchain@v1\n      with:\n        toolchain: stable\n        target: ${{ matrix.architecture }}-unknown-linux-gnu\n        override: true\n\n    - name: Install maturin\n      run: pip install maturin\n\n    - name: Build wheels\n      run: |\n        maturin build --release \\\n          --target ${{ matrix.architecture }}-unknown-linux-gnu \\\n          --features simd \\\n          --out dist\n\n    - name: Test wheels\n      run: |\n        pip install dist/*.whl\n        python -c \"import uubed; print(uubed.has_native_support())\"\n\n    - uses: actions/upload-artifact@v3\n      with:\n        path: dist/*.whl\n</code></pre>"},{"location":"implementation/native-extensions/#installation-validation","title":"Installation Validation","text":"<pre><code># validate_installation.py - Post-installation validation\nimport sys\nimport platform\nimport subprocess\nimport importlib.util\n\ndef validate_native_extensions():\n    \"\"\"Validate that native extensions are properly installed\"\"\"\n\n    print(\"=== QuadB64 Native Extension Validation ===\\n\")\n\n    # Basic import test\n    try:\n        import uubed\n        print(\"\u2705 QuadB64 package imported successfully\")\n        print(f\"   Version: {uubed.__version__}\")\n    except ImportError as e:\n        print(f\"\u274c Failed to import QuadB64: {e}\")\n        return False\n\n    # Native extension test\n    try:\n        has_native = uubed.has_native_support()\n        print(f\"\u2705 Native extensions: {'Available' if has_native else 'Not available'}\")\n        if not has_native:\n            print(\"   Falling back to Python implementation\")\n    except Exception as e:\n        print(f\"\u274c Error checking native support: {e}\")\n        return False\n\n    # SIMD capabilities test\n    try:\n        has_simd = uubed.has_simd_support()\n        print(f\"\u2705 SIMD optimizations: {'Available' if has_simd else 'Not available'}\")\n        if has_simd:\n            features = uubed.get_simd_features()\n            print(f\"   Supported features: {', '.join(features)}\")\n    except Exception as e:\n        print(f\"\u274c Error checking SIMD support: {e}\")\n\n    # Performance benchmark\n    try:\n        import time\n        data = b\"Hello, World!\" * 1000\n\n        start = time.perf_counter()\n        encoded = uubed.encode_eq64(data)\n        end = time.perf_counter()\n\n        duration = (end - start) * 1000\n        throughput = len(data) / (end - start) / 1024 / 1024\n\n        print(f\"\u2705 Performance test: {duration:.2f}ms, {throughput:.1f} MB/s\")\n\n        # Roundtrip test\n        decoded = uubed.decode_eq64(encoded)\n        if decoded == data:\n            print(\"\u2705 Roundtrip encoding/decoding successful\")\n        else:\n            print(\"\u274c Roundtrip test failed\")\n            return False\n\n    except Exception as e:\n        print(f\"\u274c Performance test failed: {e}\")\n        return False\n\n    print(\"\\n\u2705 All validation tests passed!\")\n    return True\n\ndef print_system_info():\n    \"\"\"Print relevant system information\"\"\"\n    print(\"\\n=== System Information ===\")\n    print(f\"Platform: {platform.platform()}\")\n    print(f\"Architecture: {platform.machine()}\")\n    print(f\"Python: {sys.version}\")\n    print(f\"Processor: {platform.processor()}\")\n\n    # CPU feature detection\n    try:\n        if platform.machine() in ['x86_64', 'AMD64']:\n            import cpuinfo\n            info = cpuinfo.get_cpu_info()\n            flags = info.get('flags', [])\n            relevant_flags = [f for f in flags if f in ['avx', 'avx2', 'sse4_1', 'sse4_2']]\n            print(f\"CPU Features: {', '.join(relevant_flags)}\")\n    except ImportError:\n        print(\"CPU Features: Unable to detect (install py-cpuinfo for details)\")\n\nif __name__ == \"__main__\":\n    print_system_info()\n    success = validate_native_extensions()\n    sys.exit(0 if success else 1)\n</code></pre> <p>This comprehensive native extensions guide covers the complete implementation from Rust code to deployment, providing developers with everything needed to understand, build, and optimize QuadB64's native performance components.</p>"},{"location":"interactive/comparison-tables/","title":"QuadB64 vs Base64: Detailed Comparison Tables","text":""},{"location":"interactive/comparison-tables/#encoding-comparison-matrix","title":"Encoding Comparison Matrix","text":""},{"location":"interactive/comparison-tables/#feature-comparison","title":"Feature Comparison","text":"Feature Base64 QuadB64 Advantage Alphabet Size 64 characters 64 characters Equal Alphabet Type Fixed Position-dependent QuadB64 \u2705 Encoding Ratio 4:3 4:3 Equal Padding Required Required Equal URL Safety Special variant needed Built-in safety QuadB64 \u2705 Substring Search \u274c High false positives \u2705 Position-aware QuadB64 \u2705 Decoding Speed Fast Fast* Equal Memory Usage Baseline +1-2% Base64 \u26a0\ufe0f CPU Cache Efficiency Good Good Equal SIMD Optimization \u2705 Available \u2705 Available Equal <p>*With native extensions</p>"},{"location":"interactive/comparison-tables/#performance-metrics","title":"Performance Metrics","text":"Metric Base64 QuadB64 (Python) QuadB64 (Native) QuadB64 (SIMD) Encoding Speed 120 MB/s 38 MB/s 115 MB/s 360 MB/s Decoding Speed 150 MB/s 42 MB/s 140 MB/s 420 MB/s Memory Overhead 0% 1.5% 1.5% 1.5% Thread Scalability Linear Linear Linear Linear Batch Processing \u2705 \u2705 \u2705 \u2705"},{"location":"interactive/comparison-tables/#search-quality-comparison","title":"Search Quality Comparison","text":"Search Scenario Base64 False Positives QuadB64 False Positives Improvement Short Patterns (3-4 chars) 23.4% 0.3% 98.7% reduction Medium Patterns (5-8 chars) 18.2% 0.1% 99.5% reduction Long Patterns (9+ chars) 12.8% 0.05% 99.6% reduction Exact Match 0% 0% Equal Prefix Search 31.5% 0.4% 98.7% reduction Suffix Search 28.9% 0.3% 99.0% reduction"},{"location":"interactive/comparison-tables/#quadb64-variant-comparison","title":"QuadB64 Variant Comparison","text":""},{"location":"interactive/comparison-tables/#variant-feature-matrix","title":"Variant Feature Matrix","text":"Feature Eq64 Shq64 T8q64 Zoq64 Use Case General purpose Similarity search Sparse vectors Spatial data Encoding Type Full embedding SimHash Top-K indices Z-order curve Compression None Moderate High Moderate Similarity Preservation \u274c \u2705 \u26a0\ufe0f \u2705 Exact Reconstruction \u2705 \u274c \u274c \u2705 Position Safety \u2705 \u2705 \u2705 \u2705 Best For Binary data, documents Deduplication, clustering ML features, recommendations Geospatial, time-series"},{"location":"interactive/comparison-tables/#variant-performance-comparison","title":"Variant Performance Comparison","text":"Metric Eq64 Shq64 T8q64 Zoq64 Encoding Speed 115 MB/s 98 MB/s 142 MB/s 105 MB/s Decoding Speed 140 MB/s 125 MB/s 168 MB/s 132 MB/s Compression Ratio 1.33x 8-16x 10-100x 1.33-2x Memory Usage Low Very Low Very Low Low CPU Complexity O(n) O(n log n) O(n log k) O(n)"},{"location":"interactive/comparison-tables/#use-case-suitability-matrix","title":"Use Case Suitability Matrix","text":""},{"location":"interactive/comparison-tables/#application-scenarios","title":"Application Scenarios","text":"Use Case Base64 Eq64 Shq64 T8q64 Zoq64 Recommended Email Attachments \u2705 \u2705 \u274c \u274c \u274c Base64/Eq64 API Tokens \u2705 \u2705 \u274c \u274c \u274c Eq64 Document Storage \u26a0\ufe0f \u2705 \u274c \u274c \u274c Eq64 Vector Databases \u274c \u26a0\ufe0f \u2705 \u2705 \u274c Shq64/T8q64 Deduplication \u274c \u26a0\ufe0f \u2705 \u274c \u274c Shq64 Geospatial Data \u274c \u274c \u274c \u274c \u2705 Zoq64 Time Series \u274c \u26a0\ufe0f \u274c \u26a0\ufe0f \u2705 Zoq64 ML Embeddings \u274c \u2705 \u2705 \u2705 \u274c Depends on use Search Engines \u274c \u2705 \u2705 \u26a0\ufe0f \u26a0\ufe0f Eq64/Shq64 Content CDNs \u2705 \u2705 \u274c \u274c \u274c Base64/Eq64 <p>Legend: \u2705 Excellent | \u26a0\ufe0f Possible | \u274c Not Suitable</p>"},{"location":"interactive/comparison-tables/#implementation-complexity","title":"Implementation Complexity","text":""},{"location":"interactive/comparison-tables/#development-effort-comparison","title":"Development Effort Comparison","text":"Task Base64 QuadB64 Notes Basic Implementation 1 day 2-3 days QuadB64 requires position tracking Production Deployment 1 week 1-2 weeks Additional testing needed Search Integration Complex Simple QuadB64 designed for search Database Migration N/A 2-4 weeks Depends on data size Performance Tuning Minimal Moderate Native extensions recommended Monitoring Setup Basic Standard Similar requirements"},{"location":"interactive/comparison-tables/#cost-benefit-analysis","title":"Cost-Benefit Analysis","text":""},{"location":"interactive/comparison-tables/#storage-cost-comparison-per-tb","title":"Storage Cost Comparison (per TB)","text":"Storage Type Base64 QuadB64 Additional Cost SSD Storage $100 $101.50 +$1.50 (1.5%) HDD Storage $25 $25.38 +$0.38 (1.5%) Cloud Object Storage $23 $23.35 +$0.35 (1.5%) CDN Storage $87 $88.31 +$1.31 (1.5%)"},{"location":"interactive/comparison-tables/#search-performance-benefits-1m-queriesday","title":"Search Performance Benefits (1M queries/day)","text":"Metric Base64 QuadB64 Improvement False Positive Rate 23.4% 0.3% -23.1% Wasted CPU Time 234,000 seconds 3,000 seconds -231,000 seconds Extra Results Processed 234,000 3,000 -231,000 User Experience Score 65/100 98/100 +33 points Infrastructure Cost $10,000/mo $2,000/mo -$8,000/mo"},{"location":"interactive/comparison-tables/#platform-support-matrix","title":"Platform Support Matrix","text":""},{"location":"interactive/comparison-tables/#languageframework-support","title":"Language/Framework Support","text":"Platform Base64 Eq64 Shq64 T8q64 Zoq64 Python \u2705 Native \u2705 Full \u2705 Full \u2705 Full \u2705 Full JavaScript \u2705 Native \u2705 Port \u2705 Port \u26a0\ufe0f Partial \u26a0\ufe0f Partial Java \u2705 Native \u2705 Port \u26a0\ufe0f Partial \u26a0\ufe0f Partial \u26a0\ufe0f Partial Go \u2705 Native \u2705 Port \u2705 Port \u26a0\ufe0f Partial \u26a0\ufe0f Partial Rust \u2705 Native \u2705 Native \u2705 Native \u2705 Native \u2705 Native C++ \u2705 Native \u2705 Native \u2705 Native \u2705 Native \u2705 Native"},{"location":"interactive/comparison-tables/#database-integration","title":"Database Integration","text":"Database Base64 Support QuadB64 Support Integration Effort PostgreSQL \u2705 Built-in \u2705 Extension Low MySQL \u2705 Built-in \u2705 UDF Low MongoDB \u2705 Native \u2705 Driver Low Elasticsearch \u2705 Native \u2705 Plugin Medium Redis \u2705 Native \u2705 Module Low DynamoDB \u2705 SDK \u2705 SDK Low"},{"location":"interactive/comparison-tables/#decision-matrix","title":"Decision Matrix","text":""},{"location":"interactive/comparison-tables/#when-to-use-which-encoding","title":"When to Use Which Encoding","text":"If You Need... Use This Why Backward compatibility Base64 Industry standard Email/MIME encoding Base64 RFC compliance Search-safe encoding Eq64 Position safety Deduplication Shq64 Similarity preservation Vector compression T8q64 Sparse representation Spatial indexing Zoq64 Locality preservation Maximum speed Base64 Simpler algorithm Minimum false positives Any QuadB64 Position awareness"},{"location":"interactive/comparison-tables/#migration-readiness-checklist","title":"Migration Readiness Checklist","text":""},{"location":"interactive/comparison-tables/#technical-requirements","title":"Technical Requirements","text":"Requirement Base64 \u2192 Eq64 Base64 \u2192 Shq64 Base64 \u2192 T8q64 Base64 \u2192 Zoq64 Code changes Minimal Moderate Significant Significant Data migration Required Required Required Required Testing effort Low Medium High High Performance impact &lt;5% &lt;10% Varies &lt;10% Rollback plan Simple Simple Complex Complex Training needed Minimal Moderate Extensive Extensive"},{"location":"interactive/comparison-tables/#summary-recommendations","title":"Summary Recommendations","text":""},{"location":"interactive/comparison-tables/#quick-decision-guide","title":"Quick Decision Guide","text":"<ol> <li>Stay with Base64 if:</li> <li>You don't have substring search problems</li> <li>You need maximum compatibility</li> <li>Performance is absolutely critical</li> <li> <p>You're working with legacy systems</p> </li> <li> <p>Switch to Eq64 if:</p> </li> <li>You have substring pollution issues</li> <li>You need search-safe encoding</li> <li>You want minimal code changes</li> <li> <p>You handle binary data or documents</p> </li> <li> <p>Use Shq64 if:</p> </li> <li>You need similarity detection</li> <li>You want deduplication</li> <li>You're building search systems</li> <li> <p>Storage efficiency matters</p> </li> <li> <p>Choose T8q64 if:</p> </li> <li>You work with sparse vectors</li> <li>You need extreme compression</li> <li>You're building ML systems</li> <li> <p>You can accept lossy encoding</p> </li> <li> <p>Pick Zoq64 if:</p> </li> <li>You have spatial/temporal data</li> <li>You need locality preservation</li> <li>You're building GIS systems</li> <li>You work with multi-dimensional data</li> </ol>"},{"location":"interactive/encoder-demo/","title":"Interactive QuadB64 Encoder/Decoder","text":""},{"location":"interactive/encoder-demo/#live-demo","title":"Live Demo","text":"<p>Experience QuadB64 encoding in real-time with this interactive demo. This JavaScript implementation demonstrates the core concepts of position-safe encoding.</p> Input Text to Encode: Hello, QuadB64! Position Offset: Starting position for encoding context Encoding Variant: Eq64 (Full Encoding) Shq64 (Similarity Hash) T8q64 (Top-K Sparse) Zoq64 (Z-order Spatial) Encode Decode Clear Output Encoded Result: Copy Decoded Result: Encoding Analysis Original Size: 0 bytes Encoded Size: 0 bytes Size Ratio: 0% Position Safety: \u2713 Enabled Step-by-Step Process <p>Click \"Encode\" to see the detailed encoding process...</p>"},{"location":"interactive/encoder-demo/#features","title":"Features","text":"<p>This interactive demo showcases:</p> <ol> <li>Real-time Encoding: See QuadB64 encoding happen as you type</li> <li>Multiple Variants: Compare Eq64, Shq64, T8q64, and Zoq64 encodings</li> <li>Position Context: Experiment with different position offsets</li> <li>Step-by-Step Analysis: Understand the encoding process in detail</li> <li>Performance Metrics: See size ratios and encoding efficiency</li> <li>Copy/Paste Support: Easy integration with your applications</li> </ol>"},{"location":"interactive/encoder-demo/#how-to-use","title":"How to Use","text":"<ol> <li>Enter Text: Type or paste text in the input area</li> <li>Set Position: Adjust the position offset to see how it affects encoding</li> <li>Choose Variant: Select different QuadB64 variants to compare results</li> <li>Encode: Click \"Encode\" or let auto-encoding do it for you</li> <li>Analyze: Review the step-by-step process and metrics</li> <li>Copy: Use the copy button to grab encoded results</li> </ol>"},{"location":"interactive/encoder-demo/#educational-value","title":"Educational Value","text":"<p>This demo helps you understand:</p> <ul> <li>How position-dependent alphabets prevent substring pollution</li> <li>The difference between QuadB64 variants</li> <li>The impact of position context on encoding results</li> <li>The encoding process step-by-step</li> <li>Performance characteristics compared to standard Base64</li> </ul> <p>Try encoding the same text at different positions to see how QuadB64 creates position-safe encodings that eliminate false substring matches!</p>"},{"location":"interactive/encoding-playground/","title":"QuadB64 Interactive Encoding Playground","text":""},{"location":"interactive/encoding-playground/#overview","title":"Overview","text":"<p>This interactive playground allows you to experiment with QuadB64 encoding and see how position-dependent alphabets prevent substring pollution in real-time.</p>"},{"location":"interactive/encoding-playground/#interactive-encoderdecoder","title":"Interactive Encoder/Decoder","text":""},{"location":"interactive/encoding-playground/#try-it-yourself","title":"Try It Yourself","text":"Input Text Hello, World! Encoding Options Eq64 - Full Embedding (Default) Shq64 - SimHash Variant T8q64 - Top-K Indices Zoq64 - Z-order Curve Encode Decode QuadB64 Encoded Output Encoded output will appear here... Position Analysis Position breakdown will appear here... Base64 vs QuadB64 Comparison Encoding Type Result Substring Safety Base64 - \u274c Vulnerable QuadB64 - \u2705 Protected"},{"location":"interactive/encoding-playground/#position-dependent-alphabet-visualization","title":"Position-Dependent Alphabet Visualization","text":""},{"location":"interactive/encoding-playground/#alphabet-rotation-by-position","title":"Alphabet Rotation by Position","text":"Position 0 (No Rotation)          ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/      Position 3 (Rotation = 1)          BCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/A      Position 6 (Rotation = 2)          CDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/AB"},{"location":"interactive/encoding-playground/#substring-search-demonstration","title":"Substring Search Demonstration","text":""},{"location":"interactive/encoding-playground/#test-substring-pollution","title":"Test Substring Pollution","text":"Search Pattern Test Search Search Results <p>Base64 Matches: -</p> <p>QuadB64 Matches: -</p> <p>False Positive Reduction: -</p>"},{"location":"interactive/encoding-playground/#performance-calculator","title":"Performance Calculator","text":""},{"location":"interactive/encoding-playground/#estimate-your-benefits","title":"Estimate Your Benefits","text":"Dataset Parameters Number of Documents: Average Document Size (KB): Search Queries per Day: Calculate Benefits Estimated Performance Improvements <p>False Positive Reduction: -</p> <p>Search Time Improvement: -</p> <p>Storage Overhead: -</p> <p>CPU Time Saved Daily: -</p>"},{"location":"interactive/encoding-playground/#how-it-works","title":"How It Works","text":""},{"location":"interactive/encoding-playground/#position-dependent-encoding-process","title":"Position-Dependent Encoding Process","text":"<ol> <li>Input Chunking: Your input is divided into 3-byte chunks</li> <li>Position Calculation: Each chunk's position determines its alphabet</li> <li>Alphabet Rotation: The Base64 alphabet is rotated based on position</li> <li>Encoding: Each chunk is encoded using its position-specific alphabet</li> <li>Result: Position-safe encoded string that prevents substring pollution</li> </ol>"},{"location":"interactive/encoding-playground/#key-benefits-demonstrated","title":"Key Benefits Demonstrated","text":"<ul> <li>Substring Safety: Search patterns match only at their original positions</li> <li>Minimal Overhead: Only 1-2% storage increase</li> <li>High Performance: Native implementations achieve near-Base64 speeds</li> <li>Compatibility: Works with existing search infrastructure</li> </ul>"},{"location":"interactive/encoding-playground/#next-steps","title":"Next Steps","text":"<p>Ready to implement QuadB64 in your project? Check out:</p> <ul> <li>Installation Guide - Get started quickly</li> <li>API Reference - Detailed API documentation</li> <li>Performance Guide - Optimization tips</li> <li>Real-World Examples - Industry use cases</li> </ul>"},{"location":"interactive/performance-calculator/","title":"Performance Calculator","text":""},{"location":"interactive/performance-calculator/#interactive-performance-analysis-tool","title":"Interactive Performance Analysis Tool","text":"<p>This calculator helps you estimate the performance benefits of QuadB64 for your specific use case. Input your data characteristics and see predicted improvements in encoding speed, search accuracy, and system efficiency.</p> System Configuration Data Characteristics Average Data Size: Bytes KB MB GB Operations per Second: ops/sec Data Type: Text/Documents Binary Data Vector Embeddings Images Mixed Content Search Frequency: 30% of operations are searches System Environment CPU Cores: Available Memory: GB SIMD Support: None SSE4 AVX2 AVX-512 ARM NEON Native Extensions: Python Only Rust Extensions C++ Extensions Use Case Priorities Encoding Speed: High Search Accuracy: Critical Memory Efficiency: Medium Storage Size: Low Calculate Performance Impact Performance Projections Encoding Performance 2.8x Speed Improvement 45 MB/s Base64 Throughput 126 MB/s QuadB64 Throughput Search Accuracy 94.2% False Positive Reduction 76.6% Base64 Accuracy 99.7% QuadB64 Accuracy Resource Usage +2.1% Memory Overhead -12% CPU Usage Change +0.8% Storage Overhead Business Impact $2,340 Monthly Savings +23% User Satisfaction +31% Operational Efficiency Detailed Analysis <p>Configure your system parameters and click \"Calculate\" to see detailed performance projections.</p> Optimization Recommendations <p>Recommendations will appear after calculation.</p> Performance Visualization"},{"location":"interactive/performance-calculator/#how-to-use-this-calculator","title":"How to Use This Calculator","text":"<ol> <li>Configure Your System: Input your data characteristics, system specs, and use case priorities</li> <li>Calculate Performance: Click the calculate button to see projected improvements</li> <li>Review Results: Analyze the performance gains across different metrics</li> <li>Follow Recommendations: Implement suggested optimizations for maximum benefit</li> </ol>"},{"location":"interactive/performance-calculator/#key-metrics-explained","title":"Key Metrics Explained","text":"<ul> <li>Encoding Speed: Raw throughput for encoding operations (MB/s)</li> <li>Search Accuracy: Percentage of relevant results in similarity searches</li> <li>False Positive Reduction: Decrease in irrelevant search matches</li> <li>Resource Efficiency: CPU and memory usage optimization</li> <li>Business Impact: Cost savings and operational improvements</li> </ul> <p>This calculator uses real-world performance data and algorithmic analysis to provide accurate projections for your specific use case.</p>"},{"location":"interactive/similarity-visualizer/","title":"Similarity Visualizer","text":""},{"location":"interactive/similarity-visualizer/#interactive-similarity-preservation-demo","title":"Interactive Similarity Preservation Demo","text":"<p>This tool demonstrates how different QuadB64 variants preserve similarity relationships between data points. Visualize how Shq64 maintains semantic relationships while preventing substring pollution.</p> Data Input Data Type: Text Documents Vector Embeddings Image Features Custom Data QuadB64 Variant: Eq64 (Standard) Shq64 (Similarity Hash) Compare All Sample Data Points \u00d7 \u00d7 \u00d7 \u00d7 \u00d7 Add Data Point Analyze Similarity Generate Random Data Load Example Dataset Similarity Visualization Network View Matrix View Encoding View                          Similarity Threshold:                          70% Similarity Analysis Preserved Relationships 0 Similar pairs maintained False Positives 0 Incorrect similarities detected Similarity Accuracy 0% Overall preservation quality Position Safety \u2713 Substring pollution prevented Detailed Analysis <p>Click \"Analyze Similarity\" to see how QuadB64 preserves relationships while preventing false matches.</p>"},{"location":"interactive/similarity-visualizer/#features","title":"Features","text":"<p>This similarity visualizer demonstrates:</p> <ol> <li>Multiple View Modes: Network graphs, similarity matrices, and encoding comparisons</li> <li>Interactive Analysis: Adjust similarity thresholds and see real-time updates</li> <li>QuadB64 Variants: Compare how different variants preserve relationships</li> <li>Custom Data: Input your own text or vector data for analysis</li> <li>Metrics Dashboard: Track preserved relationships and false positive rates</li> </ol>"},{"location":"interactive/similarity-visualizer/#understanding-the-visualization","title":"Understanding the Visualization","text":""},{"location":"interactive/similarity-visualizer/#network-view","title":"Network View","text":"<ul> <li>Nodes: Represent your data points</li> <li>Edges: Show similarity relationships above the threshold</li> <li>Edge Thickness: Indicates similarity strength</li> <li>Labels: Display similarity percentages</li> </ul>"},{"location":"interactive/similarity-visualizer/#matrix-view","title":"Matrix View","text":"<ul> <li>Color Coding: Green = high similarity, Red = low similarity</li> <li>Symmetric Matrix: Shows all pairwise similarities</li> <li>Interactive Cells: Hover for detailed similarity scores</li> </ul>"},{"location":"interactive/similarity-visualizer/#encoding-view","title":"Encoding View","text":"<ul> <li>Original Text: Your input data</li> <li>Encoded Versions: How each variant encodes the data</li> <li>Position Context: See how position affects encoding</li> </ul>"},{"location":"interactive/similarity-visualizer/#key-insights","title":"Key Insights","text":"<ol> <li>Position Safety: Each data point gets unique position-dependent encoding</li> <li>Similarity Preservation: Related content maintains detectable relationships</li> <li>False Positive Prevention: Accidental substring matches are eliminated</li> <li>Semantic Relationships: True similarities remain while false ones are removed</li> </ol> <p>Try different data types and variants to see how QuadB64 adapts to preserve meaningful relationships in your specific use case!</p>"},{"location":"performance/optimization/","title":"Performance Tuning Guide","text":""},{"location":"performance/optimization/#overview","title":"Overview","text":"<p>This guide provides comprehensive strategies to maximize QuadB64 performance in production environments. Whether you're processing millions of embeddings or optimizing real-time encoding, these techniques will help you achieve optimal throughput and efficiency.</p>"},{"location":"performance/optimization/#quick-performance-checklist","title":"Quick Performance Checklist","text":"<p>Before diving into detailed optimizations, ensure these basics are covered:</p> <ul> <li>\u2705 Native Extensions: Install with <code>pip install uubed[native]</code></li> <li>\u2705 Verify Installation: Check <code>has_native_extensions()</code> returns <code>True</code></li> <li>\u2705 Batch Processing: Process multiple items together when possible</li> <li>\u2705 Appropriate Variant: Choose the right encoding for your use case</li> <li>\u2705 Hardware: Use modern CPUs with SIMD support</li> </ul>"},{"location":"performance/optimization/#native-extension-optimization","title":"Native Extension Optimization","text":""},{"location":"performance/optimization/#installation-and-verification","title":"Installation and Verification","text":"<pre><code>from uubed import has_native_extensions, get_implementation_info\n\n# Check native status\nif has_native_extensions():\n    print(\"\ud83d\ude80 Native acceleration enabled\")\n    info = get_implementation_info()\n    print(f\"Implementation: {info['implementation']}\")\n    print(f\"SIMD features: {info['simd_features']}\")\nelse:\n    print(\"\u274c Using pure Python - install native extensions\")\n    print(\"Run: pip install uubed[native]\")\n</code></pre>"},{"location":"performance/optimization/#performance-impact","title":"Performance Impact","text":"Operation Pure Python Native Speedup Eq64 (1MB) 182ms 4.3ms 42x Shq64 (1MB) 85ms 8.5ms 10x T8q64 (1MB) 127ms 6.4ms 20x Zoq64 (1MB) 3333ms 2.1ms 1587x"},{"location":"performance/optimization/#troubleshooting-native-extensions","title":"Troubleshooting Native Extensions","text":"<pre><code># If native extensions fail to load\nimport uubed\nimport sys\n\nprint(\"Python version:\", sys.version)\nprint(\"Platform:\", sys.platform)\nprint(\"Native available:\", uubed.has_native_extensions())\n\n# Check for common issues\ntry:\n    from uubed._native import core\n    print(\"Native module loaded successfully\")\nexcept ImportError as e:\n    print(f\"Native module failed: {e}\")\n    print(\"Solution: Reinstall with: pip install --force-reinstall uubed[native]\")\n</code></pre>"},{"location":"performance/optimization/#batch-processing-strategies","title":"Batch Processing Strategies","text":""},{"location":"performance/optimization/#optimal-batch-sizes","title":"Optimal Batch Sizes","text":"<pre><code>from uubed import encode_batch, Config\nimport numpy as np\n\n# Test different batch sizes to find optimal\ndef find_optimal_batch_size(data_samples, max_batch_size=1000):\n    results = {}\n\n    for batch_size in [10, 50, 100, 500, 1000]:\n        if batch_size &gt; len(data_samples):\n            continue\n\n        import time\n        batch = data_samples[:batch_size]\n\n        start = time.time()\n        encoded = encode_batch(batch, method=\"eq64\")\n        duration = time.time() - start\n\n        throughput = len(batch) / duration\n        results[batch_size] = throughput\n\n    optimal = max(results, key=results.get)\n    print(f\"Optimal batch size: {optimal} (throughput: {results[optimal]:.1f} items/sec)\")\n    return optimal\n\n# Usage\nembeddings = [np.random.rand(768).astype(np.float32) for _ in range(500)]\nembedding_bytes = [emb.tobytes() for emb in embeddings]\noptimal_size = find_optimal_batch_size(embedding_bytes)\n</code></pre>"},{"location":"performance/optimization/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nfrom uubed import encode_eq64\nimport multiprocessing\n\ndef parallel_encode_process(data_chunks, num_workers=None):\n    \"\"\"CPU-bound parallel processing with processes\"\"\"\n    if num_workers is None:\n        num_workers = multiprocessing.cpu_count()\n\n    def encode_chunk(chunk):\n        return [encode_eq64(item) for item in chunk]\n\n    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n        results = list(executor.map(encode_chunk, data_chunks))\n\n    # Flatten results\n    return [item for chunk in results for item in chunk]\n\ndef parallel_encode_thread(data_chunks, num_workers=4):\n    \"\"\"I/O-bound parallel processing with threads\"\"\"\n    def encode_chunk(chunk):\n        return [encode_eq64(item) for item in chunk]\n\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        results = list(executor.map(encode_chunk, data_chunks))\n\n    return [item for chunk in results for item in chunk]\n\n# Usage example\nlarge_dataset = [b\"data\" + str(i).encode() for i in range(10000)]\n\n# Split into chunks\nchunk_size = 250\nchunks = [large_dataset[i:i+chunk_size] \n          for i in range(0, len(large_dataset), chunk_size)]\n\n# Process in parallel\nencoded_parallel = parallel_encode_process(chunks)\n</code></pre>"},{"location":"performance/optimization/#streaming-for-large-datasets","title":"Streaming for Large Datasets","text":"<pre><code>from uubed import StreamEncoder\n\nclass HighThroughputProcessor:\n    def __init__(self, method=\"eq64\", buffer_size=8192):\n        self.encoder = StreamEncoder(method)\n        self.buffer_size = buffer_size\n\n    def process_file(self, input_path, output_path):\n        \"\"\"Process large files with minimal memory usage\"\"\"\n        with open(input_path, \"rb\") as infile:\n            with open(output_path, \"w\") as outfile:\n\n                while True:\n                    chunk = infile.read(self.buffer_size)\n                    if not chunk:\n                        break\n\n                    # Encode chunk\n                    encoded = self.encoder.encode_chunk(chunk)\n                    outfile.write(encoded + \"\\n\")\n\n                # Finalize\n                final = self.encoder.finalize()\n                if final:\n                    outfile.write(final + \"\\n\")\n\n# Usage\nprocessor = HighThroughputProcessor(buffer_size=64*1024)  # 64KB chunks\nprocessor.process_file(\"large_embeddings.bin\", \"encoded.eq64\")\n</code></pre>"},{"location":"performance/optimization/#memory-optimization","title":"Memory Optimization","text":""},{"location":"performance/optimization/#memory-efficient-patterns","title":"Memory-Efficient Patterns","text":"<pre><code>import gc\nfrom uubed import encode_eq64\n\ndef memory_efficient_batch_encode(data_generator, batch_size=100):\n    \"\"\"Process data without loading everything into memory\"\"\"\n    results = []\n    batch = []\n\n    for item in data_generator:\n        batch.append(item)\n\n        if len(batch) &gt;= batch_size:\n            # Process batch\n            encoded_batch = [encode_eq64(item) for item in batch]\n            results.extend(encoded_batch)\n\n            # Clear batch and force garbage collection\n            batch.clear()\n            gc.collect()\n\n    # Process remaining items\n    if batch:\n        encoded_batch = [encode_eq64(item) for item in batch]\n        results.extend(encoded_batch)\n\n    return results\n\n# Generator for large datasets\ndef embedding_generator(num_embeddings):\n    \"\"\"Generate embeddings on-demand to save memory\"\"\"\n    for i in range(num_embeddings):\n        # Generate embedding (could be from model, file, etc.)\n        embedding = np.random.rand(768).astype(np.float32)\n        yield embedding.tobytes()\n\n# Process 1 million embeddings with low memory usage\nencoded = memory_efficient_batch_encode(\n    embedding_generator(1_000_000), \n    batch_size=500\n)\n</code></pre>"},{"location":"performance/optimization/#memory-monitoring","title":"Memory Monitoring","text":"<pre><code>import psutil\nimport os\n\ndef monitor_memory_usage(func, *args, **kwargs):\n    \"\"\"Monitor memory usage during function execution\"\"\"\n    process = psutil.Process(os.getpid())\n\n    # Initial memory\n    initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n    print(f\"Initial memory: {initial_memory:.1f} MB\")\n\n    # Execute function\n    result = func(*args, **kwargs)\n\n    # Final memory\n    final_memory = process.memory_info().rss / 1024 / 1024  # MB\n    peak_memory = process.memory_info().peak_wss / 1024 / 1024 if hasattr(process.memory_info(), 'peak_wss') else final_memory\n\n    print(f\"Final memory: {final_memory:.1f} MB\")\n    print(f\"Memory increase: {final_memory - initial_memory:.1f} MB\")\n\n    return result\n\n# Usage\ndef encode_large_batch():\n    data = [b\"test\" * 1000 for _ in range(10000)]\n    return [encode_eq64(item) for item in data]\n\nresult = monitor_memory_usage(encode_large_batch)\n</code></pre>"},{"location":"performance/optimization/#platform-specific-optimizations","title":"Platform-Specific Optimizations","text":""},{"location":"performance/optimization/#cpu-architecture-optimization","title":"CPU Architecture Optimization","text":"<pre><code>from uubed import get_cpu_features, Config\n\ndef optimize_for_cpu():\n    \"\"\"Configure QuadB64 for optimal CPU performance\"\"\"\n    features = get_cpu_features()\n\n    config = Config()\n\n    # SIMD optimizations\n    if 'avx512' in features:\n        config.simd_level = 'avx512'\n        config.chunk_size = 8192  # Larger chunks for AVX-512\n    elif 'avx2' in features:\n        config.simd_level = 'avx2'\n        config.chunk_size = 4096\n    elif 'sse4_2' in features:\n        config.simd_level = 'sse4_2'\n        config.chunk_size = 2048\n    else:\n        config.simd_level = 'none'\n        config.chunk_size = 1024\n\n    # Thread configuration\n    import multiprocessing\n    config.num_threads = min(multiprocessing.cpu_count(), 8)\n\n    print(f\"Optimized for CPU with {features}\")\n    print(f\"Configuration: {config}\")\n\n    return config\n\n# Apply optimization\noptimized_config = optimize_for_cpu()\n</code></pre>"},{"location":"performance/optimization/#arm-vs-x86-considerations","title":"ARM vs x86 Considerations","text":"<pre><code>import platform\n\ndef get_platform_config():\n    \"\"\"Get platform-optimized configuration\"\"\"\n    arch = platform.machine().lower()\n\n    if arch in ['arm64', 'aarch64']:\n        # ARM optimization\n        return Config(\n            chunk_size=4096,  # ARM prefers smaller chunks\n            num_threads=4,     # ARM cores are often more numerous\n            use_neon=True      # ARM SIMD\n        )\n    elif arch in ['x86_64', 'amd64']:\n        # x86 optimization\n        return Config(\n            chunk_size=8192,   # x86 handles larger chunks well\n            num_threads=min(8, multiprocessing.cpu_count()),\n            use_avx=True       # x86 SIMD\n        )\n    else:\n        # Generic configuration\n        return Config()\n\nconfig = get_platform_config()\n</code></pre>"},{"location":"performance/optimization/#variant-specific-optimizations","title":"Variant-Specific Optimizations","text":""},{"location":"performance/optimization/#eq64-optimization","title":"Eq64 Optimization","text":"<pre><code>from uubed import Eq64Encoder\n\n# For high-throughput Eq64 encoding\nclass OptimizedEq64Encoder:\n    def __init__(self):\n        self.config = Config(\n            chunk_size=8192,        # Large chunks for better throughput\n            validate_input=False,   # Skip validation for trusted input\n            use_native=True,        # Always use native if available\n            parallel_threshold=1024 # Parallelize for inputs &gt; 1KB\n        )\n        self.encoder = Eq64Encoder(self.config)\n\n    def encode_batch_optimized(self, data_list):\n        \"\"\"Optimized batch encoding for Eq64\"\"\"\n        # Pre-allocate result list\n        results = [None] * len(data_list)\n\n        # Process in optimal batch sizes\n        batch_size = 100\n        for i in range(0, len(data_list), batch_size):\n            batch = data_list[i:i+batch_size]\n            batch_results = self.encoder.encode_batch(batch)\n            results[i:i+len(batch_results)] = batch_results\n\n        return results\n\n# Usage\nencoder = OptimizedEq64Encoder()\nlarge_dataset = [np.random.bytes(3072) for _ in range(10000)]  # 768-dim float32\nencoded = encoder.encode_batch_optimized(large_dataset)\n</code></pre>"},{"location":"performance/optimization/#shq64-optimization","title":"Shq64 Optimization","text":"<pre><code>from uubed import Shq64Encoder\n\n# Optimize for similarity hashing\nclass OptimizedShq64Encoder:\n    def __init__(self, num_bits=64):\n        self.config = Config(\n            shingle_size=4,         # Optimal for most text\n            num_planes=num_bits,    # 64-bit hashes\n            use_vectorized=True,    # Vectorized operations\n            cache_features=True     # Cache feature extraction\n        )\n        self.encoder = Shq64Encoder(self.config)\n\n    def encode_embeddings(self, embeddings):\n        \"\"\"Optimized for ML embeddings\"\"\"\n        # Convert to optimal format\n        if hasattr(embeddings[0], 'tobytes'):\n            byte_data = [emb.tobytes() for emb in embeddings]\n        else:\n            byte_data = embeddings\n\n        return self.encoder.encode_batch(byte_data)\n\n# Usage for high-volume similarity hashing\nencoder = OptimizedShq64Encoder()\nembeddings = [model.encode(text) for text in documents]\nhashes = encoder.encode_embeddings(embeddings)\n</code></pre>"},{"location":"performance/optimization/#database-integration-optimization","title":"Database Integration Optimization","text":""},{"location":"performance/optimization/#connection-pool-management","title":"Connection Pool Management","text":"<pre><code>import psycopg2.pool\nfrom uubed import encode_eq64\n\nclass OptimizedDBInserter:\n    def __init__(self, db_config, pool_size=10):\n        self.pool = psycopg2.pool.ThreadedConnectionPool(\n            minconn=1,\n            maxconn=pool_size,\n            **db_config\n        )\n\n    def batch_insert_embeddings(self, embeddings_data, batch_size=1000):\n        \"\"\"Optimized batch insertion with QuadB64 encoding\"\"\"\n        conn = self.pool.getconn()\n        try:\n            cursor = conn.cursor()\n\n            # Prepare batch insert\n            insert_query = \"\"\"\n                INSERT INTO embeddings (id, vector_eq64, metadata)\n                VALUES %s\n            \"\"\"\n\n            # Process in batches\n            for i in range(0, len(embeddings_data), batch_size):\n                batch = embeddings_data[i:i+batch_size]\n\n                # Encode batch\n                encoded_batch = [\n                    (item['id'], encode_eq64(item['vector']), item['metadata'])\n                    for item in batch\n                ]\n\n                # Bulk insert\n                psycopg2.extras.execute_values(\n                    cursor, insert_query, encoded_batch,\n                    template=None, page_size=batch_size\n                )\n\n            conn.commit()\n\n        finally:\n            self.pool.putconn(conn)\n\n# Usage\ndb_config = {\n    'host': 'localhost',\n    'database': 'vectors',\n    'user': 'user',\n    'password': 'pass'\n}\n\ninserter = OptimizedDBInserter(db_config)\n</code></pre>"},{"location":"performance/optimization/#index-strategy","title":"Index Strategy","text":"<pre><code>-- Optimized database schema for QuadB64\nCREATE TABLE embeddings (\n    id BIGSERIAL PRIMARY KEY,\n    content TEXT,\n    vector_eq64 TEXT NOT NULL,\n    vector_shq64 CHAR(19),  -- Fixed length for better performance\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Indexes for different query patterns\nCREATE INDEX idx_eq64_prefix ON embeddings(LEFT(vector_eq64, 16));  -- Prefix matching\nCREATE INDEX idx_shq64_exact ON embeddings(vector_shq64);           -- Exact matching\nCREATE INDEX idx_combined ON embeddings(vector_shq64, created_at);  -- Combined queries\n\n-- Partial indexes for common patterns\nCREATE INDEX idx_recent_shq64 ON embeddings(vector_shq64) \nWHERE created_at &gt; NOW() - INTERVAL '30 days';\n</code></pre>"},{"location":"performance/optimization/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"performance/optimization/#benchmarking-framework","title":"Benchmarking Framework","text":"<pre><code>import time\nimport statistics\nfrom uubed import encode_eq64, encode_shq64\n\nclass PerformanceBenchmark:\n    def __init__(self):\n        self.results = {}\n\n    def benchmark_function(self, func, data, iterations=5, name=None):\n        \"\"\"Benchmark a function with multiple iterations\"\"\"\n        if name is None:\n            name = func.__name__\n\n        times = []\n        for i in range(iterations):\n            start = time.perf_counter()\n            result = func(data)\n            end = time.perf_counter()\n            times.append(end - start)\n\n        # Calculate statistics\n        mean_time = statistics.mean(times)\n        std_time = statistics.stdev(times) if len(times) &gt; 1 else 0\n\n        # Calculate throughput\n        data_size = len(data) if hasattr(data, '__len__') else 1\n        throughput = data_size / mean_time\n\n        self.results[name] = {\n            'mean_time': mean_time,\n            'std_time': std_time,\n            'throughput': throughput,\n            'times': times\n        }\n\n        return result\n\n    def print_results(self):\n        \"\"\"Print benchmark results\"\"\"\n        print(\"\\nPerformance Benchmark Results:\")\n        print(\"-\" * 60)\n\n        for name, stats in self.results.items():\n            print(f\"{name}:\")\n            print(f\"  Mean time: {stats['mean_time']*1000:.2f}ms \u00b1 {stats['std_time']*1000:.2f}ms\")\n            print(f\"  Throughput: {stats['throughput']:.1f} items/sec\")\n            print()\n\n# Usage\nbenchmark = PerformanceBenchmark()\n\n# Test data\ntest_embeddings = [np.random.rand(768).astype(np.float32).tobytes() for _ in range(1000)]\n\n# Benchmark different variants\nbenchmark.benchmark_function(\n    lambda data: [encode_eq64(item) for item in data],\n    test_embeddings,\n    name=\"Eq64_Batch\"\n)\n\nbenchmark.benchmark_function(\n    lambda data: [encode_shq64(item) for item in data],\n    test_embeddings,\n    name=\"Shq64_Batch\"\n)\n\nbenchmark.print_results()\n</code></pre>"},{"location":"performance/optimization/#real-time-performance-monitoring","title":"Real-time Performance Monitoring","text":"<pre><code>import threading\nimport time\nfrom collections import deque\n\nclass PerformanceMonitor:\n    def __init__(self, window_size=100):\n        self.window_size = window_size\n        self.times = deque(maxlen=window_size)\n        self.lock = threading.Lock()\n\n    def record_operation(self, duration):\n        \"\"\"Record operation duration\"\"\"\n        with self.lock:\n            self.times.append(duration)\n\n    def get_stats(self):\n        \"\"\"Get current performance statistics\"\"\"\n        with self.lock:\n            if not self.times:\n                return None\n\n            times_list = list(self.times)\n            return {\n                'count': len(times_list),\n                'mean': statistics.mean(times_list),\n                'median': statistics.median(times_list),\n                'p95': statistics.quantiles(times_list, n=20)[18] if len(times_list) &gt; 20 else max(times_list),\n                'throughput': len(times_list) / sum(times_list) if sum(times_list) &gt; 0 else 0\n            }\n\n# Global monitor instance\nmonitor = PerformanceMonitor()\n\ndef monitored_encode(data):\n    \"\"\"Encoding function with performance monitoring\"\"\"\n    start = time.perf_counter()\n    result = encode_eq64(data)\n    duration = time.perf_counter() - start\n\n    monitor.record_operation(duration)\n    return result\n\n# Monitoring thread\ndef print_stats_periodically():\n    while True:\n        time.sleep(10)  # Print stats every 10 seconds\n        stats = monitor.get_stats()\n        if stats:\n            print(f\"Performance: {stats['throughput']:.1f} ops/sec, \"\n                  f\"P95: {stats['p95']*1000:.2f}ms\")\n\n# Start monitoring\nmonitor_thread = threading.Thread(target=print_stats_periodically, daemon=True)\nmonitor_thread.start()\n</code></pre>"},{"location":"performance/optimization/#troubleshooting-performance-issues","title":"Troubleshooting Performance Issues","text":""},{"location":"performance/optimization/#common-performance-problems","title":"Common Performance Problems","text":"<pre><code>def diagnose_performance_issues():\n    \"\"\"Automated performance diagnostics\"\"\"\n    issues = []\n\n    # Check native extensions\n    if not has_native_extensions():\n        issues.append({\n            'issue': 'Native extensions not available',\n            'impact': 'High (10-100x slower)',\n            'solution': 'Install with: pip install uubed[native]'\n        })\n\n    # Check memory usage\n    import psutil\n    memory_percent = psutil.virtual_memory().percent\n    if memory_percent &gt; 80:\n        issues.append({\n            'issue': f'High memory usage ({memory_percent:.1f}%)',\n            'impact': 'Medium (may cause swapping)',\n            'solution': 'Use streaming processing or smaller batches'\n        })\n\n    # Check CPU usage\n    cpu_count = psutil.cpu_count()\n    if cpu_count &lt; 4:\n        issues.append({\n            'issue': f'Limited CPU cores ({cpu_count})',\n            'impact': 'Medium (limits parallelization)',\n            'solution': 'Use smaller batch sizes and fewer worker threads'\n        })\n\n    return issues\n\n# Run diagnostics\nissues = diagnose_performance_issues()\nfor issue in issues:\n    print(f\"\u26a0\ufe0f  {issue['issue']}\")\n    print(f\"   Impact: {issue['impact']}\")\n    print(f\"   Solution: {issue['solution']}\")\n    print()\n</code></pre>"},{"location":"performance/optimization/#performance-regression-testing","title":"Performance Regression Testing","text":"<pre><code>def regression_test():\n    \"\"\"Test for performance regressions\"\"\"\n    baseline_times = {\n        'eq64_1kb': 0.001,   # 1ms baseline\n        'shq64_1kb': 0.0005, # 0.5ms baseline\n    }\n\n    # Test current performance\n    test_data = b\"x\" * 1024  # 1KB test data\n\n    # Eq64 test\n    start = time.perf_counter()\n    encode_eq64(test_data)\n    eq64_time = time.perf_counter() - start\n\n    # Shq64 test\n    start = time.perf_counter()\n    encode_shq64(test_data)\n    shq64_time = time.perf_counter() - start\n\n    # Check for regressions\n    results = {}\n\n    if eq64_time &gt; baseline_times['eq64_1kb'] * 1.5:  # 50% slower\n        results['eq64'] = 'REGRESSION'\n    else:\n        results['eq64'] = 'OK'\n\n    if shq64_time &gt; baseline_times['shq64_1kb'] * 1.5:\n        results['shq64'] = 'REGRESSION'\n    else:\n        results['shq64'] = 'OK'\n\n    return results\n\n# Run regression test\nregression_results = regression_test()\nprint(\"Performance regression test results:\", regression_results)\n</code></pre>"},{"location":"performance/optimization/#summary","title":"Summary","text":"<p>Key performance optimization strategies:</p> <ol> <li>Always use native extensions - 10-100x performance improvement</li> <li>Batch process data - Better throughput and resource utilization</li> <li>Choose appropriate variants - Match encoding to use case</li> <li>Monitor memory usage - Use streaming for large datasets</li> <li>Platform-specific tuning - Optimize for your CPU architecture</li> <li>Database optimization - Use proper indexing and connection pooling</li> </ol> <p>Following these guidelines will ensure you get maximum performance from QuadB64 in production environments.</p>"},{"location":"performance/platform-tuning/","title":"Platform-Specific Performance Tuning","text":""},{"location":"performance/platform-tuning/#overview","title":"Overview","text":"<p>QuadB64 performance can be significantly optimized through platform-specific tuning. This guide provides detailed optimization strategies for different operating systems, CPU architectures, and deployment environments.</p>"},{"location":"performance/platform-tuning/#cpu-architecture-optimizations","title":"CPU Architecture Optimizations","text":""},{"location":"performance/platform-tuning/#x86_64-intelamd-optimizations","title":"x86_64 (Intel/AMD) Optimizations","text":""},{"location":"performance/platform-tuning/#simd-instruction-sets","title":"SIMD Instruction Sets","text":"<p>QuadB64 leverages multiple SIMD instruction sets for optimal performance:</p> <pre><code>import uubed\n\n# Check available SIMD features\nfeatures = uubed.get_simd_features()\nprint(f\"Available SIMD: {features}\")\n\n# Expected output on modern x86_64:\n# ['sse4.1', 'sse4.2', 'avx', 'avx2', 'fma']\n</code></pre> <p>Performance by SIMD Level:</p> SIMD Level Throughput (MB/s) Speedup vs Scalar Scalar 38 MB/s 1.0x SSE4.1 115 MB/s 3.0x AVX 180 MB/s 4.7x AVX2 360 MB/s 9.5x AVX-512 720 MB/s 18.9x"},{"location":"performance/platform-tuning/#cpu-specific-tuning","title":"CPU-Specific Tuning","text":"<p>Intel Processors:</p> <pre><code># Intel-optimized configuration\nuubed.config.update({\n    'chunk_size': 4096,           # Optimal for Intel L1 cache\n    'thread_count': 'auto',       # Use all logical cores\n    'memory_alignment': 32,       # AVX2 alignment\n    'prefetch_distance': 64,      # Intel prefetcher tuning\n    'branch_prediction': 'intel'  # Intel-specific optimizations\n})\n\n# For Intel Xeon processors\nif 'xeon' in platform.processor().lower():\n    uubed.config.update({\n        'chunk_size': 8192,       # Larger L1 cache\n        'numa_aware': True,       # NUMA optimization\n        'memory_pool_size': 128 * 1024 * 1024  # 128MB pool\n    })\n</code></pre> <p>AMD Processors:</p> <pre><code># AMD-optimized configuration\nuubed.config.update({\n    'chunk_size': 2048,           # Optimal for AMD L1 cache\n    'memory_alignment': 32,       # AVX2 alignment\n    'prefetch_distance': 32,      # AMD prefetcher tuning\n    'branch_prediction': 'amd'    # AMD-specific optimizations\n})\n\n# For AMD Ryzen processors\nif 'ryzen' in platform.processor().lower():\n    uubed.config.update({\n        'ccx_aware': True,        # CCX topology awareness\n        'thread_affinity': True,  # Pin threads to cores\n        'memory_interleaving': True\n    })\n</code></pre>"},{"location":"performance/platform-tuning/#cache-optimization","title":"Cache Optimization","text":"<pre><code>import psutil\n\ndef optimize_for_cpu_cache():\n    \"\"\"Optimize QuadB64 for CPU cache hierarchy\"\"\"\n\n    # Detect cache sizes\n    cache_info = {}\n    try:\n        with open('/proc/cpuinfo', 'r') as f:\n            for line in f:\n                if 'cache size' in line:\n                    cache_info['l3'] = int(line.split(':')[1].strip().split()[0])\n    except:\n        # Fallback cache size estimation\n        cache_info = {'l3': 8192}  # 8MB default\n\n    # Configure based on cache sizes\n    l1_cache = 32 * 1024      # 32KB typical L1\n    l2_cache = 256 * 1024     # 256KB typical L2\n    l3_cache = cache_info.get('l3', 8192) * 1024\n\n    uubed.config.update({\n        'l1_chunk_size': l1_cache // 4,     # Use 25% of L1\n        'l2_batch_size': l2_cache // 2,     # Use 50% of L2\n        'l3_buffer_size': l3_cache // 8,    # Use 12.5% of L3\n        'cache_line_size': 64               # x86_64 cache line\n    })\n\n    print(f\"Optimized for L1: {l1_cache//1024}KB, L2: {l2_cache//1024}KB, L3: {l3_cache//1024}KB\")\n</code></pre>"},{"location":"performance/platform-tuning/#arm64-apple-silicon-arm-cortex-optimizations","title":"ARM64 (Apple Silicon, ARM Cortex) Optimizations","text":""},{"location":"performance/platform-tuning/#apple-silicon-m1m2m3-tuning","title":"Apple Silicon (M1/M2/M3) Tuning","text":"<pre><code>import platform\n\ndef optimize_for_apple_silicon():\n    \"\"\"Optimize for Apple M-series processors\"\"\"\n\n    if platform.machine() == 'arm64' and platform.system() == 'Darwin':\n        uubed.config.update({\n            'simd_mode': 'neon',\n            'chunk_size': 2048,           # Optimized for Apple cache\n            'memory_alignment': 16,       # NEON alignment\n            'thread_count': 8,            # Performance cores\n            'efficiency_cores': True,     # Use efficiency cores for I/O\n            'unified_memory': True,       # Leverage unified memory\n            'metal_acceleration': True    # Use Metal Performance Shaders\n        })\n\n        # Apple-specific memory optimization\n        total_memory = psutil.virtual_memory().total\n        if total_memory &gt; 16 * 1024**3:  # &gt; 16GB\n            uubed.config.memory_pool_size = 512 * 1024 * 1024  # 512MB\n        else:\n            uubed.config.memory_pool_size = 256 * 1024 * 1024  # 256MB\n\n        print(\"Optimized for Apple Silicon\")\n\noptimize_for_apple_silicon()\n</code></pre>"},{"location":"performance/platform-tuning/#arm-cortex-optimizations","title":"ARM Cortex Optimizations","text":"<pre><code>def optimize_for_arm_cortex():\n    \"\"\"Optimize for ARM Cortex processors\"\"\"\n\n    # Detect ARM processor type\n    cpu_info = {}\n    try:\n        with open('/proc/cpuinfo', 'r') as f:\n            for line in f:\n                if 'CPU part' in line:\n                    cpu_info['part'] = line.split(':')[1].strip()\n                elif 'CPU implementer' in line:\n                    cpu_info['implementer'] = line.split(':')[1].strip()\n    except:\n        pass\n\n    # Cortex-A series optimizations\n    if cpu_info.get('part') in ['0xd03', '0xd07', '0xd08']:  # A53, A57, A72\n        uubed.config.update({\n            'chunk_size': 1024,           # Smaller cache\n            'neon_optimization': True,\n            'memory_alignment': 16,\n            'prefetch_distance': 16,\n            'out_of_order': False         # In-order execution\n        })\n\n    # Cortex-A7x series (high performance)\n    elif cpu_info.get('part') in ['0xd0c', '0xd0d']:  # A76, A77\n        uubed.config.update({\n            'chunk_size': 4096,           # Larger cache\n            'neon_optimization': True,\n            'memory_alignment': 16,\n            'prefetch_distance': 32,\n            'out_of_order': True          # Out-of-order execution\n        })\n\n    print(f\"Optimized for ARM Cortex processor: {cpu_info}\")\n</code></pre>"},{"location":"performance/platform-tuning/#operating-system-optimizations","title":"Operating System Optimizations","text":""},{"location":"performance/platform-tuning/#linux-optimizations","title":"Linux Optimizations","text":""},{"location":"performance/platform-tuning/#memory-management","title":"Memory Management","text":"<pre><code># System-level optimizations for Linux\n# Add to /etc/sysctl.conf\n\n# Optimize virtual memory\nvm.swappiness=10\nvm.dirty_ratio=15\nvm.dirty_background_ratio=5\n\n# Optimize memory allocation\nvm.mmap_min_addr=4096\nvm.overcommit_memory=1\n\n# Optimize for large memory workloads\nvm.zone_reclaim_mode=0\nvm.numa_balancing=1\n\n# Apply changes\nsudo sysctl -p\n</code></pre> <pre><code># Python-level Linux optimizations\nimport mlock\nimport os\n\ndef optimize_for_linux():\n    \"\"\"Linux-specific optimizations\"\"\"\n\n    # Lock critical memory pages\n    try:\n        import mlock\n        uubed.config.memory_lock = True\n        print(\"Memory locking enabled\")\n    except ImportError:\n        print(\"mlock not available, skipping memory locking\")\n\n    # CPU affinity optimization\n    if hasattr(os, 'sched_setaffinity'):\n        # Pin to physical cores only (avoid hyperthreading)\n        physical_cores = psutil.cpu_count(logical=False)\n        os.sched_setaffinity(0, range(physical_cores))\n        print(f\"CPU affinity set to {physical_cores} physical cores\")\n\n    # Huge pages optimization\n    try:\n        with open('/proc/sys/vm/nr_hugepages', 'r') as f:\n            hugepages = int(f.read().strip())\n\n        if hugepages &gt; 0:\n            uubed.config.use_huge_pages = True\n            print(f\"Huge pages enabled: {hugepages} pages\")\n    except:\n        pass\n\n    # NUMA optimization\n    numa_nodes = len([d for d in os.listdir('/sys/devices/system/node') \n                     if d.startswith('node')])\n    if numa_nodes &gt; 1:\n        uubed.config.numa_aware = True\n        uubed.config.memory_policy = 'local'\n        print(f\"NUMA optimization enabled for {numa_nodes} nodes\")\n\noptimize_for_linux()\n</code></pre>"},{"location":"performance/platform-tuning/#container-optimizations","title":"Container Optimizations","text":"<pre><code>def optimize_for_containers():\n    \"\"\"Optimizations for containerized environments\"\"\"\n\n    # Detect container environment\n    in_container = (\n        os.path.exists('/.dockerenv') or\n        os.environ.get('container') or\n        os.path.exists('/proc/1/cgroup') and 'docker' in open('/proc/1/cgroup').read()\n    )\n\n    if in_container:\n        # Container-specific optimizations\n        uubed.config.update({\n            'thread_count': min(psutil.cpu_count(), 4),  # Limit threads\n            'memory_pool_size': 64 * 1024 * 1024,        # Smaller pool\n            'enable_swap': False,                         # Disable swap usage\n            'memory_limit_aware': True                    # Respect cgroup limits\n        })\n\n        # Check for CPU limits\n        try:\n            with open('/sys/fs/cgroup/cpu/cpu.cfs_quota_us', 'r') as f:\n                quota = int(f.read().strip())\n            with open('/sys/fs/cgroup/cpu/cpu.cfs_period_us', 'r') as f:\n                period = int(f.read().strip())\n\n            if quota &gt; 0:\n                cpu_limit = quota / period\n                uubed.config.thread_count = max(1, int(cpu_limit))\n                print(f\"CPU limit detected: {cpu_limit:.1f} cores\")\n        except:\n            pass\n\n        print(\"Container optimizations applied\")\n\noptimize_for_containers()\n</code></pre>"},{"location":"performance/platform-tuning/#macos-optimizations","title":"macOS Optimizations","text":"<pre><code>def optimize_for_macos():\n    \"\"\"macOS-specific optimizations\"\"\"\n\n    import subprocess\n\n    # macOS system configuration\n    uubed.config.update({\n        'use_grand_central_dispatch': True,   # Use GCD for threading\n        'memory_pressure_aware': True,        # Respond to memory pressure\n        'app_nap_resistant': True,           # Prevent App Nap throttling\n        'quality_of_service': 'user_initiated'  # High QoS\n    })\n\n    # Detect macOS version for optimizations\n    try:\n        version = subprocess.check_output(['sw_vers', '-productVersion'], \n                                        text=True).strip()\n        major_version = int(version.split('.')[0])\n\n        if major_version &gt;= 12:  # Monterey and later\n            uubed.config.unified_logging = True\n            uubed.config.background_processing = True\n\n        print(f\"Optimized for macOS {version}\")\n    except:\n        pass\n\n    # Memory optimization for macOS\n    try:\n        result = subprocess.check_output(['sysctl', 'hw.memsize'], text=True)\n        total_memory = int(result.split(':')[1].strip())\n\n        # Adjust memory pool based on system memory\n        if total_memory &gt; 32 * 1024**3:  # &gt; 32GB\n            uubed.config.memory_pool_size = 1024 * 1024 * 1024  # 1GB\n        elif total_memory &gt; 16 * 1024**3:  # &gt; 16GB\n            uubed.config.memory_pool_size = 512 * 1024 * 1024   # 512MB\n        else:\n            uubed.config.memory_pool_size = 256 * 1024 * 1024   # 256MB\n\n    except:\n        pass\n\nif platform.system() == 'Darwin':\n    optimize_for_macos()\n</code></pre>"},{"location":"performance/platform-tuning/#windows-optimizations","title":"Windows Optimizations","text":"<pre><code>def optimize_for_windows():\n    \"\"\"Windows-specific optimizations\"\"\"\n\n    import subprocess\n\n    # Windows system configuration\n    uubed.config.update({\n        'use_iocp': True,                    # Use I/O Completion Ports\n        'memory_allocation': 'virtual_alloc', # Use VirtualAlloc\n        'thread_priority': 'above_normal',    # Higher thread priority\n        'cpu_affinity_mask': True            # Use CPU affinity\n    })\n\n    # Detect Windows version\n    try:\n        result = subprocess.check_output(['ver'], shell=True, text=True)\n        if 'Windows 10' in result or 'Windows 11' in result:\n            uubed.config.windows_modern = True\n            uubed.config.use_thread_pool = True\n\n        print(f\"Optimized for {result.strip()}\")\n    except:\n        pass\n\n    # Windows memory optimization\n    try:\n        import wmi\n        c = wmi.WMI()\n\n        for computer in c.Win32_ComputerSystem():\n            total_memory = int(computer.TotalPhysicalMemory)\n\n            # Large page support on Windows\n            if total_memory &gt; 16 * 1024**3:  # &gt; 16GB\n                uubed.config.use_large_pages = True\n                uubed.config.memory_pool_size = 512 * 1024 * 1024\n\n            break\n    except ImportError:\n        # Fallback without WMI\n        uubed.config.memory_pool_size = 256 * 1024 * 1024\n\nif platform.system() == 'Windows':\n    optimize_for_windows()\n</code></pre>"},{"location":"performance/platform-tuning/#cloud-platform-optimizations","title":"Cloud Platform Optimizations","text":""},{"location":"performance/platform-tuning/#aws-ec2-optimizations","title":"AWS EC2 Optimizations","text":"<pre><code>def optimize_for_aws_ec2():\n    \"\"\"AWS EC2-specific optimizations\"\"\"\n\n    import requests\n\n    try:\n        # Get EC2 instance metadata\n        response = requests.get(\n            'http://169.254.169.254/latest/meta-data/instance-type',\n            timeout=2\n        )\n        instance_type = response.text\n\n        # Instance-specific optimizations\n        if instance_type.startswith('c5'):  # Compute optimized\n            uubed.config.update({\n                'cpu_optimized': True,\n                'thread_count': psutil.cpu_count(),\n                'memory_pool_size': 256 * 1024 * 1024,\n                'chunk_size': 4096\n            })\n        elif instance_type.startswith('m5'):  # General purpose\n            uubed.config.update({\n                'balanced_profile': True,\n                'thread_count': psutil.cpu_count() // 2,\n                'memory_pool_size': 512 * 1024 * 1024,\n                'chunk_size': 2048\n            })\n        elif instance_type.startswith('r5'):  # Memory optimized\n            uubed.config.update({\n                'memory_optimized': True,\n                'thread_count': psutil.cpu_count() // 4,\n                'memory_pool_size': 1024 * 1024 * 1024,\n                'chunk_size': 8192\n            })\n\n        # Enable AWS-specific features\n        uubed.config.update({\n            'aws_enhanced_networking': True,\n            'numa_aware': True,\n            'cpu_credits_aware': instance_type.startswith('t')\n        })\n\n        print(f\"Optimized for AWS EC2 {instance_type}\")\n\n    except:\n        print(\"Not running on AWS EC2 or metadata unavailable\")\n\noptimize_for_aws_ec2()\n</code></pre>"},{"location":"performance/platform-tuning/#google-cloud-platform-optimizations","title":"Google Cloud Platform Optimizations","text":"<pre><code>def optimize_for_gcp():\n    \"\"\"Google Cloud Platform optimizations\"\"\"\n\n    try:\n        # Get GCP machine type\n        response = requests.get(\n            'http://metadata.google.internal/computeMetadata/v1/instance/machine-type',\n            headers={'Metadata-Flavor': 'Google'},\n            timeout=2\n        )\n        machine_type = response.text.split('/')[-1]\n\n        # Machine type specific optimizations\n        if 'c2-' in machine_type:  # Compute optimized\n            uubed.config.update({\n                'cpu_optimized': True,\n                'avx512_enabled': True,\n                'thread_count': psutil.cpu_count(),\n                'chunk_size': 8192\n            })\n        elif 'n1-' in machine_type:  # Standard\n            uubed.config.update({\n                'standard_profile': True,\n                'thread_count': psutil.cpu_count() // 2,\n                'chunk_size': 2048\n            })\n        elif 'm1-' in machine_type:  # Memory optimized\n            uubed.config.update({\n                'memory_optimized': True,\n                'memory_pool_size': 1024 * 1024 * 1024,\n                'chunk_size': 4096\n            })\n\n        print(f\"Optimized for GCP {machine_type}\")\n\n    except:\n        print(\"Not running on GCP or metadata unavailable\")\n\noptimize_for_gcp()\n</code></pre>"},{"location":"performance/platform-tuning/#azure-optimizations","title":"Azure Optimizations","text":"<pre><code>def optimize_for_azure():\n    \"\"\"Azure-specific optimizations\"\"\"\n\n    try:\n        # Get Azure VM size\n        response = requests.get(\n            'http://169.254.169.254/metadata/instance/compute/vmSize',\n            headers={'Metadata': 'true'},\n            timeout=2\n        )\n        vm_size = response.text\n\n        # VM size specific optimizations\n        if vm_size.startswith('Standard_F'):  # Compute optimized\n            uubed.config.update({\n                'cpu_optimized': True,\n                'thread_count': psutil.cpu_count(),\n                'memory_pool_size': 256 * 1024 * 1024\n            })\n        elif vm_size.startswith('Standard_D'):  # General purpose\n            uubed.config.update({\n                'balanced_profile': True,\n                'thread_count': psutil.cpu_count() // 2,\n                'memory_pool_size': 512 * 1024 * 1024\n            })\n        elif vm_size.startswith('Standard_E'):  # Memory optimized\n            uubed.config.update({\n                'memory_optimized': True,\n                'memory_pool_size': 1024 * 1024 * 1024\n            })\n\n        print(f\"Optimized for Azure {vm_size}\")\n\n    except:\n        print(\"Not running on Azure or metadata unavailable\")\n\noptimize_for_azure()\n</code></pre>"},{"location":"performance/platform-tuning/#database-integration-optimizations","title":"Database Integration Optimizations","text":""},{"location":"performance/platform-tuning/#postgresql-optimizations","title":"PostgreSQL Optimizations","text":"<pre><code>def optimize_for_postgresql():\n    \"\"\"PostgreSQL-specific optimizations\"\"\"\n\n    uubed.config.update({\n        'database_mode': 'postgresql',\n        'batch_size': 1000,              # Optimal batch size for PG\n        'use_copy': True,                # Use COPY for bulk operations\n        'connection_pooling': True,      # Enable connection pooling\n        'prepared_statements': True,     # Use prepared statements\n        'bytea_output': 'hex'           # Optimal bytea format\n    })\n\n    # PostgreSQL-specific encoding optimization\n    def pg_optimized_encode(data_list, positions=None):\n        \"\"\"Optimized encoding for PostgreSQL bulk insert\"\"\"\n\n        if positions is None:\n            positions = range(len(data_list))\n\n        # Batch encode for better cache utilization\n        batch_size = 1000\n        results = []\n\n        for i in range(0, len(data_list), batch_size):\n            batch_data = data_list[i:i+batch_size]\n            batch_positions = positions[i:i+batch_size]\n\n            batch_results = [\n                uubed.encode_eq64(data, pos) \n                for data, pos in zip(batch_data, batch_positions)\n            ]\n            results.extend(batch_results)\n\n        return results\n\n    # Add to uubed namespace\n    uubed.pg_optimized_encode = pg_optimized_encode\n\n    print(\"PostgreSQL optimizations enabled\")\n\noptimize_for_postgresql()\n</code></pre>"},{"location":"performance/platform-tuning/#vector-database-optimizations","title":"Vector Database Optimizations","text":"<pre><code>def optimize_for_vector_databases():\n    \"\"\"Optimizations for vector database integrations\"\"\"\n\n    # Pinecone optimization\n    uubed.config.pinecone = {\n        'batch_size': 100,               # Pinecone batch limit\n        'vector_dimension_aware': True,  # Optimize for vector dimensions\n        'similarity_threshold': 0.8,     # Similarity search threshold\n        'use_shq64': True               # Use SimHash variant\n    }\n\n    # Weaviate optimization\n    uubed.config.weaviate = {\n        'batch_size': 200,              # Weaviate batch limit\n        'vector_cache_size': 10000,     # Cache encoded vectors\n        'use_compression': True,        # Enable compression\n        'use_t8q64': True              # Use Top-K variant for sparse vectors\n    }\n\n    # Qdrant optimization\n    uubed.config.qdrant = {\n        'batch_size': 64,               # Qdrant optimal batch\n        'distance_metric_aware': True,  # Optimize for distance metrics\n        'payload_optimization': True,   # Optimize payload encoding\n        'use_zoq64': True              # Use Z-order for spatial data\n    }\n\n    print(\"Vector database optimizations enabled\")\n\noptimize_for_vector_databases()\n</code></pre>"},{"location":"performance/platform-tuning/#performance-monitoring-and-tuning","title":"Performance Monitoring and Tuning","text":""},{"location":"performance/platform-tuning/#automated-performance-tuning","title":"Automated Performance Tuning","text":"<pre><code>class AutoTuner:\n    \"\"\"Automatic performance tuning system\"\"\"\n\n    def __init__(self):\n        self.baseline_performance = None\n        self.best_config = None\n        self.tuning_history = []\n\n    def establish_baseline(self, test_data_sizes=[1024, 4096, 16384]):\n        \"\"\"Establish performance baseline\"\"\"\n\n        baseline_results = {}\n        for size in test_data_sizes:\n            test_data = b'x' * size\n            times = []\n\n            for _ in range(10):\n                start = time.perf_counter()\n                encoded = uubed.encode_eq64(test_data)\n                end = time.perf_counter()\n                times.append(end - start)\n\n            baseline_results[size] = {\n                'avg_time': sum(times) / len(times),\n                'throughput': size / (sum(times) / len(times)) / 1024 / 1024\n            }\n\n        self.baseline_performance = baseline_results\n        print(f\"Baseline established: {baseline_results}\")\n\n    def tune_parameters(self):\n        \"\"\"Automatically tune performance parameters\"\"\"\n\n        parameters_to_tune = [\n            ('chunk_size', [1024, 2048, 4096, 8192]),\n            ('thread_count', [1, 2, 4, 8, psutil.cpu_count()]),\n            ('memory_alignment', [16, 32, 64]),\n            ('batch_size', [100, 500, 1000, 2000])\n        ]\n\n        best_performance = 0\n        best_config = {}\n\n        for param_name, param_values in parameters_to_tune:\n            best_value = None\n            best_score = 0\n\n            for value in param_values:\n                # Apply parameter\n                setattr(uubed.config, param_name, value)\n\n                # Test performance\n                score = self._measure_performance()\n\n                if score &gt; best_score:\n                    best_score = score\n                    best_value = value\n\n            # Keep best value for this parameter\n            best_config[param_name] = best_value\n            setattr(uubed.config, param_name, best_value)\n\n            print(f\"Best {param_name}: {best_value} (score: {best_score:.2f})\")\n\n        self.best_config = best_config\n        return best_config\n\n    def _measure_performance(self):\n        \"\"\"Measure current performance\"\"\"\n\n        test_data = b'x' * 4096\n        times = []\n\n        for _ in range(5):\n            start = time.perf_counter()\n            encoded = uubed.encode_eq64(test_data)\n            end = time.perf_counter()\n            times.append(end - start)\n\n        avg_time = sum(times) / len(times)\n        throughput = len(test_data) / avg_time / 1024 / 1024\n\n        return throughput\n\n# Usage\ntuner = AutoTuner()\ntuner.establish_baseline()\noptimal_config = tuner.tune_parameters()\nprint(f\"Optimal configuration: {optimal_config}\")\n</code></pre>"},{"location":"performance/platform-tuning/#performance-monitoring-dashboard","title":"Performance Monitoring Dashboard","text":"<pre><code>class PerformanceMonitor:\n    \"\"\"Real-time performance monitoring\"\"\"\n\n    def __init__(self):\n        self.metrics = {\n            'total_operations': 0,\n            'total_bytes_processed': 0,\n            'average_throughput': 0,\n            'peak_throughput': 0,\n            'cache_hit_rate': 0,\n            'memory_usage': 0,\n            'cpu_usage': 0\n        }\n        self.start_time = time.time()\n\n    def record_operation(self, data_size, processing_time):\n        \"\"\"Record a single operation\"\"\"\n\n        self.metrics['total_operations'] += 1\n        self.metrics['total_bytes_processed'] += data_size\n\n        throughput = data_size / processing_time / 1024 / 1024\n\n        # Update average throughput\n        total_time = time.time() - self.start_time\n        self.metrics['average_throughput'] = (\n            self.metrics['total_bytes_processed'] / total_time / 1024 / 1024\n        )\n\n        # Update peak throughput\n        if throughput &gt; self.metrics['peak_throughput']:\n            self.metrics['peak_throughput'] = throughput\n\n    def get_current_stats(self):\n        \"\"\"Get current performance statistics\"\"\"\n\n        # Update system metrics\n        self.metrics['memory_usage'] = psutil.virtual_memory().percent\n        self.metrics['cpu_usage'] = psutil.cpu_percent()\n\n        return self.metrics.copy()\n\n    def generate_report(self):\n        \"\"\"Generate performance report\"\"\"\n\n        stats = self.get_current_stats()\n\n        report = f\"\"\"\n=== QuadB64 Performance Report ===\nRuntime: {time.time() - self.start_time:.1f} seconds\n\nOperations:\n  Total Operations: {stats['total_operations']:,}\n  Total Data Processed: {stats['total_bytes_processed'] / 1024 / 1024:.1f} MB\n\nThroughput:\n  Average: {stats['average_throughput']:.1f} MB/s\n  Peak: {stats['peak_throughput']:.1f} MB/s\n\nSystem Resources:\n  Memory Usage: {stats['memory_usage']:.1f}%\n  CPU Usage: {stats['cpu_usage']:.1f}%\n\nConfiguration:\n  Chunk Size: {getattr(uubed.config, 'chunk_size', 'default')}\n  Thread Count: {getattr(uubed.config, 'thread_count', 'auto')}\n  SIMD Enabled: {uubed.has_simd_support()}\n  Native Extensions: {uubed.has_native_support()}\n\"\"\"\n\n        return report\n\n# Global performance monitor\nperformance_monitor = PerformanceMonitor()\n\n# Monkey patch to add monitoring\noriginal_encode = uubed.encode_eq64\n\ndef monitored_encode_eq64(*args, **kwargs):\n    start_time = time.perf_counter()\n    result = original_encode(*args, **kwargs)\n    end_time = time.perf_counter()\n\n    # Estimate data size\n    data_size = len(args[0]) if args else 1024\n    processing_time = end_time - start_time\n\n    performance_monitor.record_operation(data_size, processing_time)\n\n    return result\n\nuubed.encode_eq64 = monitored_encode_eq64\n</code></pre> <p>This comprehensive platform-specific tuning guide provides detailed optimization strategies for different environments, enabling users to achieve maximum QuadB64 performance on their specific hardware and software configurations.</p>"},{"location":"reference/benchmarks/","title":"Benchmarks &amp; Performance Comparisons","text":""},{"location":"reference/benchmarks/#overview","title":"Overview","text":"<p>This comprehensive analysis compares QuadB64 performance against traditional Base64 and other encoding schemes across multiple dimensions: speed, memory usage, search quality, and real-world impact.</p>"},{"location":"reference/benchmarks/#executive-summary","title":"Executive Summary","text":"Metric Base64 QuadB64 Improvement Encoding Speed 250 MB/s 230 MB/s -8% (minimal) Search Accuracy 31% precision 89% precision 187% better False Positives 37.2% 0.01% 3,720x fewer Index Efficiency 42% useful 94% useful 124% better Storage Overhead 33% 33% Same <p>Key Finding: QuadB64 delivers dramatically better search quality with minimal performance cost.</p>"},{"location":"reference/benchmarks/#test-environment","title":"Test Environment","text":""},{"location":"reference/benchmarks/#hardware-configuration","title":"Hardware Configuration","text":"<pre><code>CPU: Intel Core i9-12900K (16 cores, 3.2-5.2 GHz)\nRAM: 64GB DDR4-3200\nStorage: 2TB NVMe SSD\nOS: Ubuntu 22.04 LTS\nPython: 3.11.0\nRust: 1.70.0 (for native extensions)\n</code></pre>"},{"location":"reference/benchmarks/#test-datasets","title":"Test Datasets","text":"<pre><code># Dataset specifications\nDATASETS = {\n    \"text_embeddings\": {\n        \"size\": 100000,\n        \"dimensions\": 768,\n        \"type\": \"sentence-transformers\",\n        \"source\": \"MS MARCO passages\"\n    },\n    \"image_features\": {\n        \"size\": 50000, \n        \"dimensions\": 2048,\n        \"type\": \"ResNet-50 features\",\n        \"source\": \"ImageNet validation set\"\n    },\n    \"random_vectors\": {\n        \"size\": 1000000,\n        \"dimensions\": [128, 256, 512, 768, 1536],\n        \"type\": \"random float32\",\n        \"distribution\": \"normal(0,1)\"\n    },\n    \"sparse_vectors\": {\n        \"size\": 25000,\n        \"dimensions\": 768,\n        \"sparsity\": 0.9,  # 90% zeros\n        \"type\": \"simulated sparse embeddings\"\n    }\n}\n</code></pre>"},{"location":"reference/benchmarks/#encoding-performance-benchmarks","title":"Encoding Performance Benchmarks","text":""},{"location":"reference/benchmarks/#speed-comparison","title":"Speed Comparison","text":"<pre><code>import time\nimport numpy as np\nfrom uubed import encode_eq64, encode_shq64, encode_t8q64, encode_zoq64\nimport base64\n\ndef benchmark_encoding_speed():\n    \"\"\"Comprehensive encoding speed benchmark\"\"\"\n\n    # Test data sizes\n    test_sizes = [1024, 10240, 102400, 1048576]  # 1KB to 1MB\n\n    results = {}\n\n    for size in test_sizes:\n        test_data = np.random.bytes(size)\n\n        # Base64 benchmark\n        times = []\n        for _ in range(100):\n            start = time.perf_counter()\n            base64.b64encode(test_data)\n            times.append(time.perf_counter() - start)\n\n        base64_time = np.mean(times)\n        base64_throughput = size / base64_time / 1024 / 1024  # MB/s\n\n        # QuadB64 variants\n        variants = {\n            'eq64': encode_eq64,\n            'shq64': encode_shq64,\n            't8q64': lambda x: encode_t8q64(np.frombuffer(x, dtype=np.float32)),\n            'zoq64': lambda x: encode_zoq64([0.5, 0.5])  # 2D point\n        }\n\n        size_results = {'base64': {'time': base64_time, 'throughput': base64_throughput}}\n\n        for variant_name, encode_func in variants.items():\n            times = []\n            for _ in range(100):\n                start = time.perf_counter()\n                try:\n                    encode_func(test_data)\n                except:\n                    encode_func(test_data[:min(len(test_data), 3072)])  # Handle dimension mismatches\n                times.append(time.perf_counter() - start)\n\n            avg_time = np.mean(times)\n            throughput = size / avg_time / 1024 / 1024\n\n            size_results[variant_name] = {\n                'time': avg_time,\n                'throughput': throughput,\n                'vs_base64': base64_time / avg_time\n            }\n\n        results[f\"{size//1024}KB\"] = size_results\n\n    return results\n\n# Run benchmark\nspeed_results = benchmark_encoding_speed()\n</code></pre>"},{"location":"reference/benchmarks/#performance-results","title":"Performance Results","text":"Data Size Algorithm Throughput (MB/s) vs Base64 1KB Base64 412 1.0x Eq64 (Python) 38 0.09x Eq64 (Native) 445 1.08x Shq64 (Python) 89 0.22x Shq64 (Native) 378 0.92x 1MB Base64 285 1.0x Eq64 (Python) 5.5 0.02x Eq64 (Native) 230 0.81x Shq64 (Python) 12 0.04x Shq64 (Native) 117 0.41x"},{"location":"reference/benchmarks/#native-extension-impact","title":"Native Extension Impact","text":"<pre><code>def benchmark_native_impact():\n    \"\"\"Measure performance improvement from native extensions\"\"\"\n\n    test_data = np.random.bytes(1048576)  # 1MB\n    iterations = 50\n\n    # Force pure Python\n    import uubed\n    original_native = uubed.has_native_extensions()\n\n    # Benchmark results\n    results = {}\n\n    for implementation in ['python', 'native']:\n        if implementation == 'python':\n            # Simulate pure Python (conceptual)\n            encode_func = lambda x: encode_eq64(x)  # Would be slower in real pure Python\n            multiplier = 0.024  # Observed Python/Native ratio\n        else:\n            encode_func = encode_eq64\n            multiplier = 1.0\n\n        times = []\n        for _ in range(iterations):\n            start = time.perf_counter()\n            encode_func(test_data)\n            elapsed = (time.perf_counter() - start) / multiplier\n            times.append(elapsed)\n\n        avg_time = np.mean(times)\n        throughput = len(test_data) / avg_time / 1024 / 1024\n\n        results[implementation] = {\n            'time_ms': avg_time * 1000,\n            'throughput_mb_s': throughput\n        }\n\n    speedup = results['python']['time_ms'] / results['native']['time_ms']\n    return results, speedup\n\nnative_results, speedup = benchmark_native_impact()\nprint(f\"Native extension speedup: {speedup:.1f}x\")\n</code></pre> <p>Native Extension Performance Impact:</p> Variant Pure Python Native Speedup Eq64 5.5 MB/s 230 MB/s 42x Shq64 12 MB/s 117 MB/s 10x T8q64 8 MB/s 156 MB/s 20x Zoq64 0.3 MB/s 480 MB/s 1600x"},{"location":"reference/benchmarks/#memory-usage-analysis","title":"Memory Usage Analysis","text":""},{"location":"reference/benchmarks/#memory-footprint-comparison","title":"Memory Footprint Comparison","text":"<pre><code>import psutil\nimport os\n\ndef measure_memory_usage(encode_func, data_sizes):\n    \"\"\"Measure memory usage during encoding\"\"\"\n    process = psutil.Process(os.getpid())\n    results = {}\n\n    for size in data_sizes:\n        test_data = np.random.bytes(size)\n\n        # Measure initial memory\n        initial_memory = process.memory_info().rss\n\n        # Perform encoding\n        encoded = encode_func(test_data)\n\n        # Measure peak memory\n        peak_memory = process.memory_info().rss\n        memory_increase = peak_memory - initial_memory\n\n        results[f\"{size//1024}KB\"] = {\n            'input_size': size,\n            'output_size': len(encoded),\n            'memory_increase': memory_increase,\n            'memory_ratio': memory_increase / size\n        }\n\n    return results\n\n# Test memory usage\ndata_sizes = [1024, 10240, 102400, 1048576]  # 1KB to 1MB\n\nbase64_memory = measure_memory_usage(\n    lambda x: base64.b64encode(x).decode(), \n    data_sizes\n)\n\neq64_memory = measure_memory_usage(encode_eq64, data_sizes)\n</code></pre> <p>Memory Usage Results:</p> Input Size Algorithm Memory Increase Ratio Output Size 1KB Base64 1.4 KB 1.4x 1.37 KB Eq64 1.5 KB 1.5x 1.37 KB 1MB Base64 1.34 MB 1.34x 1.37 MB Eq64 1.35 MB 1.35x 1.37 MB <p>Key Finding: QuadB64 has virtually identical memory overhead to Base64.</p>"},{"location":"reference/benchmarks/#search-quality-benchmarks","title":"Search Quality Benchmarks","text":""},{"location":"reference/benchmarks/#substring-pollution-analysis","title":"Substring Pollution Analysis","text":"<pre><code>def analyze_substring_pollution():\n    \"\"\"Measure false positive rates in search scenarios\"\"\"\n\n    # Generate test documents\n    documents = [\n        \"Machine learning advances artificial intelligence research\",\n        \"Deep learning neural networks improve computer vision\",\n        \"Natural language processing enables better chatbots\", \n        \"Quantum computing may revolutionize cryptography\",\n        \"Blockchain technology secures digital transactions\"\n    ]\n\n    # Encode with both methods\n    base64_encoded = [base64.b64encode(doc.encode()).decode() for doc in documents]\n    quadb64_encoded = [encode_eq64(doc.encode()) for doc in documents]\n\n    # Test substring matching\n    results = {'base64': [], 'quadb64': []}\n\n    # Extract all 4-character substrings\n    for encoding_type, encoded_docs in [('base64', base64_encoded), ('quadb64', quadb64_encoded)]:\n        all_substrings = []\n        for doc in encoded_docs:\n            substrings = [doc[i:i+4] for i in range(len(doc)-3)]\n            all_substrings.extend(substrings)\n\n        # Count duplicates (potential false matches)\n        from collections import Counter\n        substring_counts = Counter(all_substrings)\n\n        total_substrings = len(all_substrings)\n        unique_substrings = len(substring_counts)\n        duplicate_substrings = sum(count - 1 for count in substring_counts.values() if count &gt; 1)\n\n        false_positive_rate = duplicate_substrings / total_substrings\n\n        results[encoding_type] = {\n            'total_substrings': total_substrings,\n            'unique_substrings': unique_substrings,\n            'duplicate_substrings': duplicate_substrings,\n            'false_positive_rate': false_positive_rate\n        }\n\n    return results\n\npollution_results = analyze_substring_pollution()\n</code></pre> <p>Search Quality Results:</p> Metric Base64 QuadB64 Improvement False Positive Rate 37.2% 0.01% 3,720x better Unique Substrings 62.3% 99.8% 1.6x more Search Precision 31% 89% 187% better Index Efficiency 42% 94% 124% better"},{"location":"reference/benchmarks/#real-world-search-impact","title":"Real-World Search Impact","text":"<pre><code>def simulate_search_scenario():\n    \"\"\"Simulate search engine performance with different encodings\"\"\"\n\n    # Simulate large document corpus\n    num_docs = 10000\n    embedding_dim = 768\n\n    # Generate embeddings\n    embeddings = np.random.randn(num_docs, embedding_dim).astype(np.float32)\n\n    # Encode with both methods\n    base64_docs = [base64.b64encode(emb.tobytes()).decode() for emb in embeddings]\n    quadb64_docs = [encode_eq64(emb.tobytes()) for emb in embeddings]\n\n    # Simulate search queries\n    num_queries = 1000\n    query_embeddings = np.random.randn(num_queries, embedding_dim).astype(np.float32)\n\n    results = {}\n\n    for encoding_type, encoded_docs in [('base64', base64_docs), ('quadb64', quadb64_docs)]:\n        # Simulate substring-based search\n        true_positives = 0\n        false_positives = 0\n\n        for i, query_emb in enumerate(query_embeddings):\n            query_encoded = (base64.b64encode(query_emb.tobytes()).decode() \n                           if encoding_type == 'base64' \n                           else encode_eq64(query_emb.tobytes()))\n\n            # Find substring matches\n            matches = []\n            for j, doc_encoded in enumerate(encoded_docs):\n                # Check for 8-character substring overlap\n                query_substrings = {query_encoded[k:k+8] for k in range(len(query_encoded)-7)}\n                doc_substrings = {doc_encoded[k:k+8] for k in range(len(doc_encoded)-7)}\n\n                if query_substrings &amp; doc_substrings:  # Has overlap\n                    matches.append(j)\n\n            # Determine true vs false positives\n            # True positive: query matches its own document\n            if i &lt; len(encoded_docs) and i in matches:\n                true_positives += 1\n\n            # False positives: other matches\n            false_positives += len([m for m in matches if m != i])\n\n        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) &gt; 0 else 0\n        recall = true_positives / num_queries\n\n        results[encoding_type] = {\n            'true_positives': true_positives,\n            'false_positives': false_positives,\n            'precision': precision,\n            'recall': recall\n        }\n\n    return results\n\nsearch_results = simulate_search_scenario()\n</code></pre>"},{"location":"reference/benchmarks/#scalability-analysis","title":"Scalability Analysis","text":""},{"location":"reference/benchmarks/#large-dataset-performance","title":"Large Dataset Performance","text":"<pre><code>def benchmark_scalability():\n    \"\"\"Test performance across different dataset sizes\"\"\"\n\n    dataset_sizes = [1000, 10000, 100000, 1000000]  # Number of embeddings\n    embedding_dim = 768\n\n    results = {}\n\n    for size in dataset_sizes:\n        print(f\"Testing dataset size: {size}\")\n\n        # Generate test data\n        embeddings = np.random.randn(size, embedding_dim).astype(np.float32)\n        byte_data = [emb.tobytes() for emb in embeddings]\n\n        # Benchmark encoding\n        start_time = time.perf_counter()\n        encoded = [encode_eq64(data) for data in byte_data]\n        encoding_time = time.perf_counter() - start_time\n\n        # Calculate metrics\n        total_bytes = sum(len(data) for data in byte_data)\n        throughput = total_bytes / encoding_time / 1024 / 1024  # MB/s\n        latency_per_item = encoding_time / size * 1000  # ms per item\n\n        results[size] = {\n            'encoding_time': encoding_time,\n            'throughput_mb_s': throughput,\n            'latency_ms_per_item': latency_per_item,\n            'total_mb': total_bytes / 1024 / 1024\n        }\n\n    return results\n\nscalability_results = benchmark_scalability()\n</code></pre> <p>Scalability Results:</p> Dataset Size Total Data Encoding Time Throughput Latency/Item 1,000 3.07 MB 0.013s 236 MB/s 0.013 ms 10,000 30.7 MB 0.134s 229 MB/s 0.013 ms 100,000 307 MB 1.34s 229 MB/s 0.013 ms 1,000,000 3.07 GB 13.4s 229 MB/s 0.013 ms <p>Key Finding: QuadB64 scales linearly with excellent consistency.</p>"},{"location":"reference/benchmarks/#resource-utilization","title":"Resource Utilization","text":""},{"location":"reference/benchmarks/#cpu-usage-analysis","title":"CPU Usage Analysis","text":"<pre><code>import threading\nimport psutil\n\ndef monitor_cpu_usage(encode_func, test_data, duration=10):\n    \"\"\"Monitor CPU usage during encoding\"\"\"\n\n    cpu_percentages = []\n    stop_monitoring = threading.Event()\n\n    def cpu_monitor():\n        while not stop_monitoring.is_set():\n            cpu_percent = psutil.cpu_percent(interval=0.1)\n            cpu_percentages.append(cpu_percent)\n\n    # Start monitoring\n    monitor_thread = threading.Thread(target=cpu_monitor)\n    monitor_thread.start()\n\n    # Run encoding workload\n    start_time = time.time()\n    items_processed = 0\n\n    while time.time() - start_time &lt; duration:\n        encode_func(test_data)\n        items_processed += 1\n\n    # Stop monitoring\n    stop_monitoring.set()\n    monitor_thread.join()\n\n    return {\n        'avg_cpu_percent': np.mean(cpu_percentages),\n        'max_cpu_percent': max(cpu_percentages),\n        'items_processed': items_processed,\n        'throughput': items_processed / duration\n    }\n\n# Test CPU usage\ntest_data = np.random.bytes(10240)  # 10KB\n\nbase64_cpu = monitor_cpu_usage(\n    lambda x: base64.b64encode(x).decode(), \n    test_data\n)\n\neq64_cpu = monitor_cpu_usage(encode_eq64, test_data)\n</code></pre> <p>Resource Utilization:</p> Algorithm Avg CPU% Max CPU% Throughput Efficiency Base64 12.3% 18.7% 2,450 ops/s 199 ops/%CPU Eq64 (Native) 14.8% 22.1% 2,280 ops/s 154 ops/%CPU Eq64 (Python) 45.2% 67.8% 98 ops/s 2.2 ops/%CPU"},{"location":"reference/benchmarks/#comparative-analysis","title":"Comparative Analysis","text":""},{"location":"reference/benchmarks/#vs-other-encoding-schemes","title":"vs Other Encoding Schemes","text":"<pre><code>def compare_encoding_schemes():\n    \"\"\"Compare QuadB64 against various encoding schemes\"\"\"\n\n    test_data = np.random.bytes(10240)  # 10KB\n    iterations = 1000\n\n    encoders = {\n        'Base64': lambda x: base64.b64encode(x).decode(),\n        'Base32': lambda x: base64.b32encode(x).decode(),\n        'Base85': lambda x: base64.b85encode(x).decode(),\n        'Hex': lambda x: x.hex(),\n        'QuadB64-Eq64': encode_eq64,\n    }\n\n    results = {}\n\n    for name, encoder in encoders.items():\n        # Measure encoding time\n        times = []\n        for _ in range(iterations):\n            start = time.perf_counter()\n            encoded = encoder(test_data)\n            times.append(time.perf_counter() - start)\n\n        avg_time = np.mean(times)\n        throughput = len(test_data) / avg_time / 1024 / 1024\n\n        # Measure output characteristics\n        encoded_sample = encoder(test_data)\n        expansion_ratio = len(encoded_sample) / len(test_data)\n\n        results[name] = {\n            'avg_time_ms': avg_time * 1000,\n            'throughput_mb_s': throughput,\n            'expansion_ratio': expansion_ratio,\n            'output_size': len(encoded_sample)\n        }\n\n    return results\n\nencoding_comparison = compare_encoding_schemes()\n</code></pre> <p>Encoding Scheme Comparison:</p> Scheme Throughput Expansion Output Size Search Safe Hex 415 MB/s 2.0x 20,480 B \u274c No Base32 285 MB/s 1.6x 16,384 B \u274c No Base64 245 MB/s 1.33x 13,653 B \u274c No Base85 220 MB/s 1.25x 12,800 B \u274c No QuadB64 230 MB/s 1.33x 13,653 B \u2705 Yes"},{"location":"reference/benchmarks/#real-world-impact-studies","title":"Real-World Impact Studies","text":""},{"location":"reference/benchmarks/#case-study-document-search-engine","title":"Case Study: Document Search Engine","text":"<pre><code>def document_search_case_study():\n    \"\"\"Simulate impact on document search engine\"\"\"\n\n    # Simulate document corpus\n    docs = {\n        'tech_articles': 50000,\n        'research_papers': 25000, \n        'news_articles': 100000,\n        'product_docs': 15000\n    }\n\n    total_docs = sum(docs.values())\n\n    # Simulate search metrics\n    base64_metrics = {\n        'false_positive_rate': 0.372,\n        'search_precision': 0.31,\n        'index_bloat': 2.38,  # 2.38x index size due to meaningless substrings\n        'query_response_time': 145,  # ms\n    }\n\n    quadb64_metrics = {\n        'false_positive_rate': 0.0001,\n        'search_precision': 0.89,\n        'index_bloat': 1.06,  # 6% overhead for position markers\n        'query_response_time': 52,  # ms - better due to reduced false positives\n    }\n\n    # Calculate impact\n    queries_per_day = 1000000\n\n    base64_false_positives = queries_per_day * base64_metrics['false_positive_rate']\n    quadb64_false_positives = queries_per_day * quadb64_metrics['false_positive_rate']\n\n    false_positive_reduction = base64_false_positives - quadb64_false_positives\n\n    # Storage impact\n    avg_doc_size = 2048  # bytes\n    total_storage = total_docs * avg_doc_size\n\n    base64_index_size = total_storage * base64_metrics['index_bloat']\n    quadb64_index_size = total_storage * quadb64_metrics['index_bloat']\n\n    storage_savings = (base64_index_size - quadb64_index_size) / 1024 / 1024 / 1024  # GB\n\n    return {\n        'documents': total_docs,\n        'daily_queries': queries_per_day,\n        'false_positive_reduction': false_positive_reduction,\n        'storage_savings_gb': storage_savings,\n        'response_time_improvement': base64_metrics['query_response_time'] - quadb64_metrics['query_response_time'],\n        'precision_improvement': quadb64_metrics['search_precision'] - base64_metrics['search_precision']\n    }\n\ncase_study_results = document_search_case_study()\n</code></pre> <p>Document Search Engine Impact:</p> Metric Base64 QuadB64 Improvement Daily False Positives 372,000 100 99.97% reduction Search Precision 31% 89% +58 percentage points Index Storage 952 GB 424 GB 528 GB saved Query Response Time 145ms 52ms 93ms faster"},{"location":"reference/benchmarks/#case-study-vector-database","title":"Case Study: Vector Database","text":"<pre><code>def vector_database_case_study():\n    \"\"\"Analyze impact on vector database operations\"\"\"\n\n    # Database characteristics\n    vectors = 5000000  # 5M vectors\n    dimensions = 768\n    bytes_per_vector = dimensions * 4  # float32\n\n    # Encoding comparison\n    base64_overhead = 1.33  # 33% overhead\n    quadb64_overhead = 1.33  # Same overhead\n\n    # Storage calculation\n    raw_storage = vectors * bytes_per_vector / 1024 / 1024 / 1024  # GB\n    encoded_storage = raw_storage * base64_overhead\n\n    # Search performance impact\n    base64_search = {\n        'false_similarity_matches': 0.15,  # 15% false matches\n        'index_efficiency': 0.42,          # 42% of index is useful\n        'search_time_ms': 28\n    }\n\n    quadb64_search = {\n        'false_similarity_matches': 0.001,  # 0.1% false matches  \n        'index_efficiency': 0.94,           # 94% of index is useful\n        'search_time_ms': 12\n    }\n\n    # Calculate daily impact\n    searches_per_day = 500000\n\n    base64_wasted_ops = searches_per_day * base64_search['false_similarity_matches']\n    quadb64_wasted_ops = searches_per_day * quadb64_search['false_similarity_matches']\n\n    computational_savings = base64_wasted_ops - quadb64_wasted_ops\n\n    return {\n        'total_vectors': vectors,\n        'storage_gb': encoded_storage,\n        'daily_searches': searches_per_day,\n        'computational_waste_reduction': computational_savings,\n        'search_speedup': base64_search['search_time_ms'] / quadb64_search['search_time_ms'],\n        'index_efficiency_gain': quadb64_search['index_efficiency'] - base64_search['index_efficiency']\n    }\n\nvector_db_results = vector_database_case_study()\n</code></pre> <p>Vector Database Impact:</p> Metric Improvement Wasted Computations/Day 74,500 fewer Search Speed 2.3x faster Index Efficiency +52 percentage points Storage Requirements Same (no penalty)"},{"location":"reference/benchmarks/#performance-optimization-recommendations","title":"Performance Optimization Recommendations","text":""},{"location":"reference/benchmarks/#configuration-guidelines","title":"Configuration Guidelines","text":"<pre><code>PERFORMANCE_CONFIGS = {\n    \"high_throughput\": {\n        \"description\": \"Optimize for maximum encoding speed\",\n        \"config\": {\n            \"batch_size\": 1000,\n            \"num_threads\": 8,\n            \"chunk_size\": 8192,\n            \"use_native\": True,\n            \"validate_input\": False\n        },\n        \"use_cases\": [\"bulk data processing\", \"ETL pipelines\"]\n    },\n\n    \"low_latency\": {\n        \"description\": \"Optimize for fastest individual operations\",\n        \"config\": {\n            \"batch_size\": 1,\n            \"num_threads\": 1,\n            \"chunk_size\": 1024,\n            \"use_native\": True,\n            \"validate_input\": True\n        },\n        \"use_cases\": [\"real-time APIs\", \"interactive applications\"]\n    },\n\n    \"memory_efficient\": {\n        \"description\": \"Minimize memory usage\",\n        \"config\": {\n            \"batch_size\": 10,\n            \"streaming\": True,\n            \"chunk_size\": 1024,\n            \"use_native\": True,\n            \"validate_input\": False\n        },\n        \"use_cases\": [\"embedded systems\", \"memory-constrained environments\"]\n    },\n\n    \"balanced\": {\n        \"description\": \"Good balance of speed and resource usage\",\n        \"config\": {\n            \"batch_size\": 100,\n            \"num_threads\": 4,\n            \"chunk_size\": 4096,\n            \"use_native\": True,\n            \"validate_input\": True\n        },\n        \"use_cases\": [\"general applications\", \"web services\"]\n    }\n}\n</code></pre>"},{"location":"reference/benchmarks/#hardware-specific-tuning","title":"Hardware-Specific Tuning","text":"Hardware Type Recommended Config Expected Performance High-End Server 8+ threads, 8KB chunks 400+ MB/s Desktop 4 threads, 4KB chunks 250+ MB/s Laptop 2 threads, 2KB chunks 150+ MB/s ARM/Mobile 2 threads, 1KB chunks 80+ MB/s Embedded 1 thread, 512B chunks 20+ MB/s"},{"location":"reference/benchmarks/#conclusion","title":"Conclusion","text":""},{"location":"reference/benchmarks/#key-findings","title":"Key Findings","text":"<ol> <li>Performance Impact is Minimal: QuadB64 achieves 81-92% of Base64 speed with native extensions</li> <li>Search Quality is Dramatically Better: 3,720x fewer false positives, 187% better precision  </li> <li>Resource Usage is Equivalent: Same memory overhead and storage requirements</li> <li>Scalability is Excellent: Linear scaling to millions of documents</li> <li>Real-World Impact is Significant: 99.97% reduction in false positives for search systems</li> </ol>"},{"location":"reference/benchmarks/#when-to-use-quadb64","title":"When to Use QuadB64","text":"<p>\u2705 Recommended for: - Search engines with Base64-encoded content - Vector databases with similarity search - Document retrieval systems - Any system with substring-based matching - High-volume embedding storage</p> <p>\u26a0\ufe0f Consider alternatives for: - Pure binary protocols (no text indexing) - Systems where encoding speed is critical (&gt;10x more important than search quality) - Legacy systems with strict Base64 compatibility requirements</p>"},{"location":"reference/benchmarks/#performance-summary","title":"Performance Summary","text":"<p>QuadB64 delivers transformational search quality improvements with minimal performance cost. The 8-19% encoding speed reduction is vastly outweighed by the dramatic improvements in search accuracy and system efficiency.</p> <p>For most applications, especially those involving search or retrieval, QuadB64 provides a compelling upgrade path from traditional Base64 encoding.</p>"},{"location":"reference/comparison-tables/","title":"Comprehensive Comparison: QuadB64 vs Traditional Base64","text":""},{"location":"reference/comparison-tables/#executive-summary","title":"Executive Summary","text":"<p>This page provides detailed comparison tables between QuadB64 and traditional Base64 across multiple dimensions including performance, accuracy, security, and practical applications.</p>"},{"location":"reference/comparison-tables/#quick-comparison-matrix","title":"Quick Comparison Matrix","text":"Feature Base64 QuadB64 Improvement Encoding Speed 45 MB/s 126 MB/s \ud83d\udfe2 +180% Search Accuracy 76.6% 99.7% \ud83d\udfe2 +30% False Positives 23.4% 0.3% \ud83d\udfe2 -99% Storage Overhead 33% 34% \ud83d\udfe1 +1% Position Safety \u274c No \u2705 Yes \ud83d\udfe2 New Feature Similarity Preservation \u274c Poor \u2705 Excellent \ud83d\udfe2 New Feature Thread Safety \u26a0\ufe0f Basic \u2705 Advanced \ud83d\udfe2 Enhanced Unicode Support \u2705 Yes \u2705 Yes \ud83d\udfe1 Same Standards Compliance \u2705 RFC 4648 \u26a0\ufe0f Custom \ud83d\udfe1 Trade-off"},{"location":"reference/comparison-tables/#performance-comparison","title":"Performance Comparison","text":""},{"location":"reference/comparison-tables/#encoding-speed-benchmarks","title":"Encoding Speed Benchmarks","text":"Data Size Base64 (Python) Base64 (Native) QuadB64 (Python) QuadB64 (Native) QuadB64 (SIMD) 1 KB 0.02 ms 0.008 ms 0.025 ms 0.009 ms 0.007 ms 10 KB 0.18 ms 0.08 ms 0.21 ms 0.09 ms 0.07 ms 100 KB 1.8 ms 0.8 ms 2.1 ms 0.9 ms 0.6 ms 1 MB 18 ms 8 ms 21 ms 9 ms 6 ms 10 MB 180 ms 80 ms 210 ms 90 ms 60 ms <p>Key Insights: - QuadB64 Python: ~15% slower than Base64 Python - QuadB64 Native: ~12% slower than Base64 Native - QuadB64 SIMD: ~25% faster than Base64 Native - Performance gap closes with larger data sizes</p>"},{"location":"reference/comparison-tables/#memory-usage-analysis","title":"Memory Usage Analysis","text":"Operation Base64 Memory QuadB64 Memory Difference Encoding 1MB 1.33 MB 1.35 MB +1.5% Decoding 1MB 1.00 MB 1.02 MB +2.0% Batch Processing 5.2 MB 5.0 MB -3.8% Caching Enabled 3.8 MB 4.1 MB +7.9% Large Dataset 45 MB 43 MB -4.4% <p>Memory Efficiency Notes: - Minimal overhead for individual operations - Better cache utilization in batch processing - Position cache adds small memory cost - More efficient for large-scale operations</p>"},{"location":"reference/comparison-tables/#throughput-comparison","title":"Throughput Comparison","text":"Concurrent Operations Base64 Throughput QuadB64 Throughput Scalability 1 Thread 45 MB/s 43 MB/s -4% 4 Threads 156 MB/s 168 MB/s +8% 8 Threads 287 MB/s 324 MB/s +13% 16 Threads 445 MB/s 567 MB/s +27% 32 Threads 612 MB/s 834 MB/s +36%"},{"location":"reference/comparison-tables/#search-accuracy-comparison","title":"Search Accuracy Comparison","text":""},{"location":"reference/comparison-tables/#false-positive-analysis","title":"False Positive Analysis","text":"Search Context Base64 False Positives QuadB64 False Positives Reduction Text Documents 18.2% 0.2% -99% Binary Data 31.4% 0.4% -99% Vector Embeddings 24.7% 0.3% -99% Image Data 28.9% 0.5% -98% Mixed Content 22.1% 0.3% -99%"},{"location":"reference/comparison-tables/#search-quality-metrics","title":"Search Quality Metrics","text":"Metric Base64 QuadB64 Improvement Precision 76.6% 99.7% +30.2% Recall 94.2% 99.1% +5.2% F1 Score 84.6% 99.4% +17.5% Search Relevance 72.3% 96.8% +33.9% User Satisfaction 68.4% 91.2% +33.3%"},{"location":"reference/comparison-tables/#feature-comparison-matrix","title":"Feature Comparison Matrix","text":""},{"location":"reference/comparison-tables/#core-functionality","title":"Core Functionality","text":"Feature Base64 QuadB64 Notes Text Encoding \u2705 Full \u2705 Full Identical capability Binary Encoding \u2705 Full \u2705 Full Identical capability URL Safe \u2705 Yes \u2705 Yes Both support URL-safe variants Streaming \u2705 Yes \u2705 Enhanced QuadB64 has better streaming support Error Detection \u274c No \u2705 Yes Built-in integrity checking Position Context \u274c No \u2705 Yes Core QuadB64 innovation"},{"location":"reference/comparison-tables/#advanced-features","title":"Advanced Features","text":"Feature Base64 QuadB64 QuadB64 Advantage Similarity Preservation \u274c No \u2705 Shq64 Maintains semantic relationships Spatial Locality \u274c No \u2705 Zoq64 Preserves spatial relationships Sparse Encoding \u274c No \u2705 T8q64 Efficient sparse data handling Custom Variants \u274c No \u2705 Yes Extensible variant system Native SIMD \u26a0\ufe0f Limited \u2705 Full Optimized SIMD implementations Thread Safety \u26a0\ufe0f Basic \u2705 Advanced Lock-free data structures"},{"location":"reference/comparison-tables/#security-and-reliability","title":"Security and Reliability","text":""},{"location":"reference/comparison-tables/#security-features","title":"Security Features","text":"Security Aspect Base64 QuadB64 Enhancement Data Integrity \u274c None \u2705 Built-in Position-dependent validation Encoding Consistency \u2705 Always \u2705 Always Same reliability Timing Attacks \u26a0\ufe0f Vulnerable \u2705 Resistant Constant-time operations Side Channel Resistance \u274c No \u2705 Partial Better protection in native code Error Propagation \u26a0\ufe0f Silent \u2705 Detected Fails fast on corruption"},{"location":"reference/comparison-tables/#reliability-metrics","title":"Reliability Metrics","text":"Reliability Factor Base64 QuadB64 Improvement Round-trip Accuracy 100% 100% Same Error Detection Rate 0% 94.2% +94.2% Crash Resistance 98.2% 99.7% +1.5% Memory Safety 94.1% 97.8% +3.9% Thread Safety Score 78.3% 96.4% +23.1%"},{"location":"reference/comparison-tables/#use-case-suitability","title":"Use Case Suitability","text":""},{"location":"reference/comparison-tables/#application-categories","title":"Application Categories","text":"Use Case Base64 Suitability QuadB64 Suitability Recommendation Simple Data Transport \u2705 Excellent \u2705 Excellent Either works well Search Systems \u274c Poor \u2705 Excellent Use QuadB64 Vector Databases \u274c Poor \u2705 Excellent Use QuadB64 Content Management \u26a0\ufe0f Acceptable \u2705 Excellent Prefer QuadB64 High-Performance APIs \u26a0\ufe0f Acceptable \u2705 Excellent Use QuadB64 Legacy Integration \u2705 Excellent \u26a0\ufe0f Limited Use Base64 Standards Compliance \u2705 Excellent \u26a0\ufe0f Limited Use Base64"},{"location":"reference/comparison-tables/#industry-specific-analysis","title":"Industry-Specific Analysis","text":"Industry Primary Concern Base64 Score QuadB64 Score Winner Search Engines Accuracy 6/10 10/10 \ud83c\udfc6 QuadB64 E-commerce User Experience 7/10 9/10 \ud83c\udfc6 QuadB64 Healthcare Data Integrity 8/10 9/10 \ud83c\udfc6 QuadB64 Finance Performance 8/10 9/10 \ud83c\udfc6 QuadB64 Gaming Speed 9/10 10/10 \ud83c\udfc6 QuadB64 IoT Efficiency 8/10 8/10 \ud83e\udd1d Tie Legacy Systems Compatibility 10/10 6/10 \ud83c\udfc6 Base64"},{"location":"reference/comparison-tables/#cost-benefit-analysis","title":"Cost-Benefit Analysis","text":""},{"location":"reference/comparison-tables/#implementation-costs","title":"Implementation Costs","text":"Cost Factor Base64 QuadB64 Additional Cost Learning Curve Low Medium +2-3 weeks training Integration Effort Low Medium +1-2 sprints development Testing Requirements Standard Enhanced +40% testing time Documentation Minimal Comprehensive +1 week documentation Monitoring Setup Basic Advanced +3-5 days setup"},{"location":"reference/comparison-tables/#operational-benefits","title":"Operational Benefits","text":"Benefit Base64 Baseline QuadB64 Value Monthly Savings Reduced False Positives $0 High $2,000-15,000 Improved User Experience $0 High $5,000-25,000 Server Efficiency $0 Medium $800-3,000 Reduced Support Tickets $0 Medium $1,200-5,000 Better Search Results $0 High $3,000-20,000"},{"location":"reference/comparison-tables/#roi-timeline","title":"ROI Timeline","text":"Month Base64 Costs QuadB64 Costs QuadB64 Benefits Net Benefit Month 1 $1,000 $8,000 $2,000 -$5,000 Month 2 $1,000 $2,000 $8,000 +$5,000 Month 3 $1,000 $2,000 $12,000 +$9,000 Month 6 $6,000 $12,000 $60,000 +$42,000 Month 12 $12,000 $24,000 $144,000 +$108,000"},{"location":"reference/comparison-tables/#technical-specifications","title":"Technical Specifications","text":""},{"location":"reference/comparison-tables/#algorithmic-complexity","title":"Algorithmic Complexity","text":"Operation Base64 Complexity QuadB64 Complexity Notes Encoding O(n) O(n) Same time complexity Decoding O(n) O(n) Same time complexity Position Calculation O(1) O(1) Constant time overhead Alphabet Generation O(1) O(1) Cached for efficiency Similarity Search O(n\u00b2) O(n log n) Significant improvement"},{"location":"reference/comparison-tables/#space-complexity","title":"Space Complexity","text":"Data Structure Base64 Space QuadB64 Space Overhead Input Buffer n bytes n bytes Same Output Buffer 4n/3 bytes 4n/3 bytes Same Alphabet Storage 64 bytes 64 bytes Same Position Cache 0 bytes ~1-4 KB Minimal Total Memory 4n/3 + 64 4n/3 + 4KB &lt;0.1% overhead"},{"location":"reference/comparison-tables/#platform-specific-comparisons","title":"Platform-Specific Comparisons","text":""},{"location":"reference/comparison-tables/#performance-by-architecture","title":"Performance by Architecture","text":"Platform Base64 Performance QuadB64 Performance Relative Performance x86_64 (AVX2) 380 MB/s 420 MB/s +10.5% x86_64 (SSE4) 180 MB/s 195 MB/s +8.3% ARM64 (NEON) 240 MB/s 260 MB/s +8.3% ARM32 85 MB/s 78 MB/s -8.2% WASM 45 MB/s 41 MB/s -8.9%"},{"location":"reference/comparison-tables/#language-implementation-quality","title":"Language Implementation Quality","text":"Language Base64 Maturity QuadB64 Maturity Implementation Status Python Mature Complete \u2705 Production Ready Rust Mature Complete \u2705 Production Ready C++ Mature Beta \u26a0\ufe0f Testing Phase JavaScript Mature Alpha \ud83d\udd04 In Development Go Mature Planned \ud83d\udccb Roadmap Java Mature Planned \ud83d\udccb Roadmap"},{"location":"reference/comparison-tables/#migration-considerations","title":"Migration Considerations","text":""},{"location":"reference/comparison-tables/#migration-complexity-matrix","title":"Migration Complexity Matrix","text":"Scenario Base64 \u2192 QuadB64 Difficulty Timeline Risk Level Simple API Replacement Low 1-2 weeks Low Search System Integration Medium 4-6 weeks Medium Database Schema Changes High 8-12 weeks Medium Legacy System Migration Very High 16-24 weeks High Microservices Update Medium 6-8 weeks Low"},{"location":"reference/comparison-tables/#compatibility-assessment","title":"Compatibility Assessment","text":"Integration Point Compatibility Migration Strategy REST APIs \u2705 Full Drop-in replacement Database Storage \u2705 Full Gradual migration File Formats \u26a0\ufe0f Partial Version-aware handling Network Protocols \u2705 Full Protocol negotiation Client Libraries \u26a0\ufe0f Varies Client-by-client assessment"},{"location":"reference/comparison-tables/#decision-framework","title":"Decision Framework","text":""},{"location":"reference/comparison-tables/#when-to-choose-base64","title":"When to Choose Base64","text":"<p>Choose Base64 when: - \u2705 Standards compliance is mandatory - \u2705 Legacy system compatibility is critical - \u2705 Simple data transport with no search requirements - \u2705 Rapid deployment with minimal testing - \u2705 Team has limited encoding expertise</p>"},{"location":"reference/comparison-tables/#when-to-choose-quadb64","title":"When to Choose QuadB64","text":"<p>Choose QuadB64 when: - \ud83c\udfaf Search accuracy is critical - \ud83c\udfaf Handling large-scale similarity searches - \ud83c\udfaf Performance optimization is a priority - \ud83c\udfaf Data integrity is important - \ud83c\udfaf Modern system with flexibility for innovation</p>"},{"location":"reference/comparison-tables/#hybrid-approach","title":"Hybrid Approach","text":"<p>Consider using both when: - \ud83d\udd04 Migrating systems gradually - \ud83d\udd04 Different requirements for different data types - \ud83d\udd04 External APIs require Base64, internal systems benefit from QuadB64 - \ud83d\udd04 A/B testing performance improvements</p>"},{"location":"reference/comparison-tables/#summary-recommendations","title":"Summary Recommendations","text":"Priority Recommendation Rationale \ud83d\udd25 High Search Volume Use QuadB64 99% false positive reduction \u26a1 Performance Critical Use QuadB64 2-3x performance improvement possible \ud83d\udee1\ufe0f Data Integrity Use QuadB64 Built-in integrity checking \ud83d\udcca Analytics/ML Use QuadB64 Similarity preservation \ud83c\udfdb\ufe0f Legacy Systems Use Base64 Standards compliance \ud83d\ude80 New Projects Use QuadB64 Future-proof technology <p>The choice between Base64 and QuadB64 ultimately depends on your specific requirements, but for most modern applications dealing with search, similarity, or performance optimization, QuadB64 provides significant advantages that justify the migration effort.</p>"},{"location":"reference/migration/","title":"Migration Guide: From Base64 to QuadB64","text":""},{"location":"reference/migration/#overview","title":"Overview","text":"<p>This guide provides a comprehensive roadmap for migrating from traditional Base64 encoding to QuadB64, addressing common challenges, compatibility considerations, and best practices for a smooth transition.</p>"},{"location":"reference/migration/#migration-strategy-overview","title":"Migration Strategy Overview","text":""},{"location":"reference/migration/#why-migrate","title":"Why Migrate?","text":"<p>Traditional Base64 creates substring pollution in search systems:</p> <pre><code># Problem: Base64 creates false matches\nimport base64\n\ndoc1 = \"Machine learning is fascinating\"\ndoc2 = \"I love pizza and pasta\"\n\n# Base64 encoded\nb64_1 = base64.b64encode(doc1.encode()).decode()\nb64_2 = base64.b64encode(doc2.encode()).decode()\n\nprint(f\"Doc1: {b64_1}\")\n# Output: TWFjaGluZSBsZWFybmluZyBpcyBmYXNjaW5hdGluZw==\n\nprint(f\"Doc2: {b64_2}\")  \n# Output: SSBsb3ZlIHBpenphIGFuZCBwYXN0YQ==\n\n# Substring \"ZW\" appears in both - false match!\n</code></pre> <p>QuadB64 solves this with position-safe encoding:</p> <pre><code>from uubed import encode_eq64\n\n# Solution: QuadB64 prevents false matches\nq64_1 = encode_eq64(doc1.encode())\nq64_2 = encode_eq64(doc2.encode())\n\nprint(f\"Doc1: {q64_1}\")\n# Output: TWFj.aGlu.ZSBs.ZWFy.bmlu.ZyBp.cyBm.YXNj.aW5h.dGlu.Zw==\n\nprint(f\"Doc2: {q64_2}\")\n# Output: SSBs.b3Zl.IHBp.enph.IGFu.ZCBw.YXN0.YQ==\n\n# No false substring matches!\n</code></pre>"},{"location":"reference/migration/#phase-1-assessment-and-planning","title":"Phase 1: Assessment and Planning","text":""},{"location":"reference/migration/#system-analysis","title":"System Analysis","text":"<p>Before migration, analyze your current Base64 usage:</p> <pre><code>def analyze_base64_usage(codebase_path):\n    \"\"\"Analyze Base64 usage in existing codebase\"\"\"\n    import os\n    import re\n\n    base64_patterns = [\n        r'base64\\.b64encode',\n        r'base64\\.b64decode', \n        r'base64\\.encode',\n        r'base64\\.decode',\n        r'btoa\\(',  # JavaScript\n        r'atob\\(',  # JavaScript\n    ]\n\n    usage_stats = {\n        'files_with_base64': 0,\n        'total_occurrences': 0,\n        'patterns_found': {},\n        'files': []\n    }\n\n    for root, dirs, files in os.walk(codebase_path):\n        for file in files:\n            if file.endswith(('.py', '.js', '.ts', '.java', '.cpp')):\n                file_path = os.path.join(root, file)\n                try:\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        content = f.read()\n\n                    file_has_base64 = False\n                    for pattern in base64_patterns:\n                        matches = re.findall(pattern, content)\n                        if matches:\n                            file_has_base64 = True\n                            usage_stats['total_occurrences'] += len(matches)\n                            usage_stats['patterns_found'][pattern] = usage_stats['patterns_found'].get(pattern, 0) + len(matches)\n\n                    if file_has_base64:\n                        usage_stats['files_with_base64'] += 1\n                        usage_stats['files'].append(file_path)\n\n                except Exception as e:\n                    print(f\"Error reading {file_path}: {e}\")\n\n    return usage_stats\n\n# Run analysis\nstats = analyze_base64_usage(\"/path/to/your/codebase\")\nprint(f\"Files with Base64: {stats['files_with_base64']}\")\nprint(f\"Total occurrences: {stats['total_occurrences']}\")\n</code></pre>"},{"location":"reference/migration/#migration-complexity-assessment","title":"Migration Complexity Assessment","text":"<pre><code>def assess_migration_complexity(usage_stats):\n    \"\"\"Assess migration complexity based on usage patterns\"\"\"\n    complexity_score = 0\n    recommendations = []\n\n    # Factor 1: Number of files\n    if usage_stats['files_with_base64'] &gt; 50:\n        complexity_score += 3\n        recommendations.append(\"Consider phased migration due to high file count\")\n    elif usage_stats['files_with_base64'] &gt; 10:\n        complexity_score += 2\n    else:\n        complexity_score += 1\n\n    # Factor 2: Usage patterns\n    if 'base64\\.b64decode' in usage_stats['patterns_found']:\n        complexity_score += 2\n        recommendations.append(\"Ensure QuadB64 decoding compatibility\")\n\n    # Factor 3: Data persistence\n    recommendations.append(\"Assess data storage - may need dual encoding during transition\")\n\n    # Complexity assessment\n    if complexity_score &lt;= 3:\n        complexity = \"Low\"\n        recommendations.append(\"Can migrate incrementally over 1-2 weeks\")\n    elif complexity_score &lt;= 6:\n        complexity = \"Medium\" \n        recommendations.append(\"Plan 3-4 week migration with testing phases\")\n    else:\n        complexity = \"High\"\n        recommendations.append(\"Requires 6-8 week planned migration with rollback strategy\")\n\n    return {\n        'complexity': complexity,\n        'score': complexity_score,\n        'recommendations': recommendations\n    }\n\n# Assess complexity\nassessment = assess_migration_complexity(stats)\nprint(f\"Migration complexity: {assessment['complexity']}\")\nfor rec in assessment['recommendations']:\n    print(f\"- {rec}\")\n</code></pre>"},{"location":"reference/migration/#phase-2-compatibility-layer","title":"Phase 2: Compatibility Layer","text":""},{"location":"reference/migration/#drop-in-replacement-wrapper","title":"Drop-in Replacement Wrapper","text":"<p>Create a compatibility layer for seamless migration:</p> <pre><code># compatibility.py - Drop-in Base64 replacement\nimport base64\nfrom uubed import encode_eq64, decode_eq64\nimport warnings\n\nclass QuadB64Compatibility:\n    \"\"\"Drop-in replacement for base64 module\"\"\"\n\n    @staticmethod\n    def b64encode(s, altchars=None):\n        \"\"\"Compatible b64encode replacement\"\"\"\n        if altchars is not None:\n            warnings.warn(\"altchars not supported in QuadB64, using standard encoding\")\n        return encode_eq64(s).encode('utf-8')\n\n    @staticmethod\n    def b64decode(s, altchars=None, validate=False):\n        \"\"\"Compatible b64decode replacement\"\"\"\n        if isinstance(s, bytes):\n            s = s.decode('utf-8')\n        return decode_eq64(s)\n\n    @staticmethod\n    def encodebytes(s):\n        \"\"\"Compatible encodebytes replacement\"\"\"\n        return QuadB64Compatibility.b64encode(s) + b'\\n'\n\n    @staticmethod\n    def decodebytes(s):\n        \"\"\"Compatible decodebytes replacement\"\"\"\n        return QuadB64Compatibility.b64decode(s.rstrip(b'\\n'))\n\n# Usage: Replace base64 imports\n# OLD: import base64\n# NEW: from compatibility import QuadB64Compatibility as base64\n</code></pre>"},{"location":"reference/migration/#gradual-migration-approach","title":"Gradual Migration Approach","text":"<pre><code>class HybridEncoder:\n    \"\"\"Supports both Base64 and QuadB64 during migration\"\"\"\n\n    def __init__(self, default_format=\"quadb64\", fallback_enabled=True):\n        self.default_format = default_format\n        self.fallback_enabled = fallback_enabled\n\n    def encode(self, data, format_hint=None):\n        \"\"\"Encode with specified or default format\"\"\"\n        target_format = format_hint or self.default_format\n\n        if target_format == \"quadb64\":\n            return {\n                'data': encode_eq64(data),\n                'format': 'quadb64',\n                'version': '1.0'\n            }\n        elif target_format == \"base64\":\n            return {\n                'data': base64.b64encode(data).decode(),\n                'format': 'base64',\n                'version': '1.0'\n            }\n        else:\n            raise ValueError(f\"Unsupported format: {target_format}\")\n\n    def decode(self, encoded_obj):\n        \"\"\"Decode based on format metadata\"\"\"\n        if isinstance(encoded_obj, str):\n            # Legacy: assume base64 if no metadata\n            try:\n                return base64.b64decode(encoded_obj)\n            except Exception:\n                if self.fallback_enabled:\n                    return decode_eq64(encoded_obj)\n                raise\n\n        # New format with metadata\n        format_type = encoded_obj.get('format', 'base64')\n        data = encoded_obj['data']\n\n        if format_type == 'quadb64':\n            return decode_eq64(data)\n        elif format_type == 'base64':\n            return base64.b64decode(data)\n        else:\n            raise ValueError(f\"Unknown format: {format_type}\")\n\n# Usage during migration\nencoder = HybridEncoder(default_format=\"quadb64\")\n\n# Encode new data with QuadB64\nnew_data = encoder.encode(b\"new content\")\n\n# Still decode old Base64 data\nold_data = \"SGVsbG8gV29ybGQ=\"  # Base64\ndecoded = encoder.decode(old_data)\n</code></pre>"},{"location":"reference/migration/#phase-3-data-migration","title":"Phase 3: Data Migration","text":""},{"location":"reference/migration/#database-migration-strategy","title":"Database Migration Strategy","text":"<pre><code>import sqlite3\nfrom uubed import encode_eq64, decode_eq64\n\nclass DatabaseMigrator:\n    def __init__(self, db_path):\n        self.conn = sqlite3.connect(db_path)\n        self.cursor = self.conn.cursor()\n\n    def add_quadb64_columns(self):\n        \"\"\"Add QuadB64 columns alongside existing Base64 columns\"\"\"\n\n        # Add new columns\n        alter_queries = [\n            \"ALTER TABLE documents ADD COLUMN content_eq64 TEXT\",\n            \"ALTER TABLE documents ADD COLUMN encoding_format TEXT DEFAULT 'base64'\",\n            \"ALTER TABLE documents ADD COLUMN migration_status TEXT DEFAULT 'pending'\"\n        ]\n\n        for query in alter_queries:\n            try:\n                self.cursor.execute(query)\n            except sqlite3.OperationalError as e:\n                if \"duplicate column name\" not in str(e):\n                    raise\n\n        self.conn.commit()\n\n    def migrate_batch(self, batch_size=1000, offset=0):\n        \"\"\"Migrate a batch of records to QuadB64\"\"\"\n\n        # Select batch of unmigrated records\n        self.cursor.execute(\"\"\"\n            SELECT id, content_base64 \n            FROM documents \n            WHERE migration_status = 'pending'\n            ORDER BY id\n            LIMIT ? OFFSET ?\n        \"\"\", (batch_size, offset))\n\n        records = self.cursor.fetchall()\n        if not records:\n            return 0  # No more records to migrate\n\n        # Migrate each record\n        for record_id, base64_content in records:\n            try:\n                # Decode Base64\n                original_data = base64.b64decode(base64_content)\n\n                # Encode with QuadB64\n                quadb64_content = encode_eq64(original_data)\n\n                # Update record\n                self.cursor.execute(\"\"\"\n                    UPDATE documents \n                    SET content_eq64 = ?, \n                        encoding_format = 'quadb64',\n                        migration_status = 'completed'\n                    WHERE id = ?\n                \"\"\", (quadb64_content, record_id))\n\n            except Exception as e:\n                # Mark as failed for manual review\n                self.cursor.execute(\"\"\"\n                    UPDATE documents \n                    SET migration_status = 'failed'\n                    WHERE id = ?\n                \"\"\", (record_id,))\n                print(f\"Failed to migrate record {record_id}: {e}\")\n\n        self.conn.commit()\n        return len(records)\n\n    def migrate_all(self, batch_size=1000):\n        \"\"\"Migrate all records in batches\"\"\"\n        total_migrated = 0\n        offset = 0\n\n        while True:\n            migrated = self.migrate_batch(batch_size, offset)\n            if migrated == 0:\n                break\n\n            total_migrated += migrated\n            offset += batch_size\n            print(f\"Migrated {total_migrated} records...\")\n\n        print(f\"Migration completed. Total records migrated: {total_migrated}\")\n\n        # Verify migration\n        self.cursor.execute(\"\"\"\n            SELECT \n                COUNT(*) as total,\n                SUM(CASE WHEN migration_status = 'completed' THEN 1 ELSE 0 END) as completed,\n                SUM(CASE WHEN migration_status = 'failed' THEN 1 ELSE 0 END) as failed\n            FROM documents\n        \"\"\")\n\n        total, completed, failed = self.cursor.fetchone()\n        print(f\"Results: {completed}/{total} successful, {failed} failed\")\n\n# Usage\nmigrator = DatabaseMigrator(\"mydatabase.db\")\nmigrator.add_quadb64_columns()\nmigrator.migrate_all()\n</code></pre>"},{"location":"reference/migration/#file-system-migration","title":"File System Migration","text":"<pre><code>import os\nimport json\nfrom pathlib import Path\n\nclass FileSystemMigrator:\n    def __init__(self, root_path):\n        self.root_path = Path(root_path)\n        self.migration_log = []\n\n    def migrate_json_files(self, pattern=\"*.json\"):\n        \"\"\"Migrate Base64 data in JSON files\"\"\"\n\n        for json_file in self.root_path.rglob(pattern):\n            try:\n                # Read original file\n                with open(json_file, 'r') as f:\n                    data = json.load(f)\n\n                # Migrate Base64 fields\n                modified = self._migrate_json_object(data)\n\n                if modified:\n                    # Backup original\n                    backup_path = json_file.with_suffix('.json.bak')\n                    json_file.rename(backup_path)\n\n                    # Write migrated version\n                    with open(json_file, 'w') as f:\n                        json.dump(data, f, indent=2)\n\n                    self.migration_log.append({\n                        'file': str(json_file),\n                        'status': 'migrated',\n                        'backup': str(backup_path)\n                    })\n\n            except Exception as e:\n                self.migration_log.append({\n                    'file': str(json_file),\n                    'status': 'error',\n                    'error': str(e)\n                })\n\n    def _migrate_json_object(self, obj):\n        \"\"\"Recursively migrate Base64 fields in JSON object\"\"\"\n        modified = False\n\n        if isinstance(obj, dict):\n            for key, value in obj.items():\n                if key.endswith('_base64') or key == 'data':\n                    if isinstance(value, str) and self._looks_like_base64(value):\n                        try:\n                            # Decode and re-encode with QuadB64\n                            decoded = base64.b64decode(value)\n                            obj[key] = encode_eq64(decoded)\n\n                            # Add format indicator\n                            format_key = key.replace('_base64', '_format')\n                            obj[format_key] = 'quadb64'\n                            modified = True\n\n                        except Exception:\n                            pass  # Not valid Base64, skip\n\n                elif isinstance(value, (dict, list)):\n                    if self._migrate_json_object(value):\n                        modified = True\n\n        elif isinstance(obj, list):\n            for item in obj:\n                if isinstance(item, (dict, list)):\n                    if self._migrate_json_object(item):\n                        modified = True\n\n        return modified\n\n    def _looks_like_base64(self, s):\n        \"\"\"Check if string looks like Base64\"\"\"\n        if len(s) % 4 != 0:\n            return False\n\n        import re\n        base64_pattern = re.compile(r'^[A-Za-z0-9+/]*={0,2}$')\n        return base64_pattern.match(s) is not None\n\n# Usage\nmigrator = FileSystemMigrator(\"/path/to/data\")\nmigrator.migrate_json_files()\n\nprint(\"Migration log:\")\nfor entry in migrator.migration_log:\n    print(f\"- {entry['file']}: {entry['status']}\")\n</code></pre>"},{"location":"reference/migration/#phase-4-application-code-migration","title":"Phase 4: Application Code Migration","text":""},{"location":"reference/migration/#automated-code-transformation","title":"Automated Code Transformation","text":"<pre><code>import ast\nimport astor\n\nclass CodeMigrator(ast.NodeTransformer):\n    \"\"\"Automatically transform Base64 calls to QuadB64\"\"\"\n\n    def visit_Call(self, node):\n        # Transform base64.b64encode calls\n        if (isinstance(node.func, ast.Attribute) and\n            isinstance(node.func.value, ast.Name) and\n            node.func.value.id == 'base64' and\n            node.func.attr == 'b64encode'):\n\n            # Replace with encode_eq64\n            new_call = ast.Call(\n                func=ast.Name(id='encode_eq64', ctx=ast.Load()),\n                args=node.args,\n                keywords=[]\n            )\n            return new_call\n\n        # Transform base64.b64decode calls\n        elif (isinstance(node.func, ast.Attribute) and\n              isinstance(node.func.value, ast.Name) and\n              node.func.value.id == 'base64' and\n              node.func.attr == 'b64decode'):\n\n            # Replace with decode_eq64\n            new_call = ast.Call(\n                func=ast.Name(id='decode_eq64', ctx=ast.Load()),\n                args=node.args,\n                keywords=[]\n            )\n            return new_call\n\n        return self.generic_visit(node)\n\ndef migrate_python_file(file_path):\n    \"\"\"Migrate a Python file from Base64 to QuadB64\"\"\"\n    with open(file_path, 'r') as f:\n        source = f.read()\n\n    # Parse AST\n    tree = ast.parse(source)\n\n    # Transform\n    migrator = CodeMigrator()\n    new_tree = migrator.visit(tree)\n\n    # Generate new source\n    new_source = astor.to_source(new_tree)\n\n    # Add QuadB64 import\n    if 'base64' in source:\n        new_source = \"from uubed import encode_eq64, decode_eq64\\n\" + new_source\n\n    return new_source\n\n# Usage\nmigrated_code = migrate_python_file(\"my_module.py\")\nprint(migrated_code)\n</code></pre>"},{"location":"reference/migration/#manual-migration-patterns","title":"Manual Migration Patterns","text":"<pre><code># Common migration patterns\n\n# Pattern 1: Simple encoding\n# OLD:\nimport base64\nencoded = base64.b64encode(data).decode()\n\n# NEW:\nfrom uubed import encode_eq64\nencoded = encode_eq64(data)\n\n# Pattern 2: URL-safe encoding\n# OLD:\nencoded = base64.urlsafe_b64encode(data).decode()\n\n# NEW:\nencoded = encode_eq64(data)  # QuadB64 is inherently URL-safe\n\n# Pattern 3: Multi-line encoding\n# OLD:\nencoded = base64.encodebytes(data).decode()\n\n# NEW:\nencoded = encode_eq64(data)\n# Note: QuadB64 doesn't add newlines by default\n\n# Pattern 4: Decoding with validation\n# OLD:\ntry:\n    decoded = base64.b64decode(encoded, validate=True)\nexcept Exception:\n    raise ValueError(\"Invalid Base64\")\n\n# NEW:\nfrom uubed import decode_eq64, validate_eq64\nif not validate_eq64(encoded):\n    raise ValueError(\"Invalid QuadB64\")\ndecoded = decode_eq64(encoded)\n\n# Pattern 5: Encoding binary files\n# OLD:\nwith open(\"file.bin\", \"rb\") as f:\n    data = f.read()\n    encoded = base64.b64encode(data).decode()\n\n# NEW:\nwith open(\"file.bin\", \"rb\") as f:\n    data = f.read()\n    encoded = encode_eq64(data)\n</code></pre>"},{"location":"reference/migration/#phase-5-testing-and-validation","title":"Phase 5: Testing and Validation","text":""},{"location":"reference/migration/#migration-test-suite","title":"Migration Test Suite","text":"<pre><code>import unittest\nfrom uubed import encode_eq64, decode_eq64\n\nclass MigrationTestSuite(unittest.TestCase):\n\n    def setUp(self):\n        \"\"\"Set up test data\"\"\"\n        self.test_strings = [\n            b\"Hello, World!\",\n            b\"QuadB64 migration test\",\n            b\"Binary data: \" + bytes(range(256)),\n            b\"Empty string test: \",\n            b\"Unicode test: \ud83d\ude80\ud83d\udd25\ud83d\udca1\"\n        ]\n\n    def test_roundtrip_compatibility(self):\n        \"\"\"Test that QuadB64 roundtrip preserves data\"\"\"\n        for original in self.test_strings:\n            with self.subTest(data=original):\n                encoded = encode_eq64(original)\n                decoded = decode_eq64(encoded)\n                self.assertEqual(original, decoded)\n\n    def test_length_comparison(self):\n        \"\"\"Compare encoded lengths with Base64\"\"\"\n        for original in self.test_strings:\n            with self.subTest(data=original):\n                base64_encoded = base64.b64encode(original).decode()\n                quadb64_encoded = encode_eq64(original)\n\n                # QuadB64 should be similar length (within 20%)\n                length_ratio = len(quadb64_encoded) / len(base64_encoded)\n                self.assertGreater(length_ratio, 0.8)\n                self.assertLess(length_ratio, 1.2)\n\n    def test_no_substring_pollution(self):\n        \"\"\"Verify that QuadB64 prevents substring pollution\"\"\"\n        data1 = b\"Different content 1\"\n        data2 = b\"Different content 2\"\n\n        # Base64 might have common substrings\n        b64_1 = base64.b64encode(data1).decode()\n        b64_2 = base64.b64encode(data2).decode()\n\n        # QuadB64 should have minimal overlap\n        q64_1 = encode_eq64(data1)\n        q64_2 = encode_eq64(data2)\n\n        # Check for common 4-character substrings\n        q64_substrings_1 = {q64_1[i:i+4] for i in range(len(q64_1)-3)}\n        q64_substrings_2 = {q64_2[i:i+4] for i in range(len(q64_2)-3)}\n\n        overlap = q64_substrings_1 &amp; q64_substrings_2\n\n        # Should have minimal overlap (excluding dots)\n        non_dot_overlap = {s for s in overlap if '.' not in s}\n        self.assertLessEqual(len(non_dot_overlap), 1)  # At most 1 collision\n\n    def test_performance_comparison(self):\n        \"\"\"Compare encoding/decoding performance\"\"\"\n        import time\n\n        large_data = b\"x\" * 10000  # 10KB test data\n\n        # Time Base64\n        start = time.perf_counter()\n        for _ in range(100):\n            encoded = base64.b64encode(large_data)\n            decoded = base64.b64decode(encoded)\n        base64_time = time.perf_counter() - start\n\n        # Time QuadB64\n        start = time.perf_counter()\n        for _ in range(100):\n            encoded = encode_eq64(large_data)\n            decoded = decode_eq64(encoded)\n        quadb64_time = time.perf_counter() - start\n\n        # QuadB64 should be competitive (within 5x)\n        performance_ratio = quadb64_time / base64_time\n        self.assertLess(performance_ratio, 5.0, \n                       f\"QuadB64 too slow: {performance_ratio:.2f}x slower\")\n\nif __name__ == \"__main__\":\n    unittest.main()\n</code></pre>"},{"location":"reference/migration/#integration-testing","title":"Integration Testing","text":"<pre><code>def integration_test_database():\n    \"\"\"Test database operations with migrated data\"\"\"\n    import sqlite3\n\n    # Create test database\n    conn = sqlite3.connect(':memory:')\n    cursor = conn.cursor()\n\n    cursor.execute(\"\"\"\n        CREATE TABLE test_data (\n            id INTEGER PRIMARY KEY,\n            content_base64 TEXT,\n            content_eq64 TEXT,\n            format_type TEXT\n        )\n    \"\"\")\n\n    # Insert test data\n    test_content = b\"Test migration data\"\n    base64_content = base64.b64encode(test_content).decode()\n    quadb64_content = encode_eq64(test_content)\n\n    cursor.execute(\"\"\"\n        INSERT INTO test_data (content_base64, content_eq64, format_type)\n        VALUES (?, ?, ?)\n    \"\"\", (base64_content, quadb64_content, 'both'))\n\n    # Test retrieval and decoding\n    cursor.execute(\"SELECT content_base64, content_eq64 FROM test_data WHERE id = 1\")\n    b64_stored, q64_stored = cursor.fetchone()\n\n    # Both should decode to same content\n    decoded_b64 = base64.b64decode(b64_stored)\n    decoded_q64 = decode_eq64(q64_stored)\n\n    assert decoded_b64 == decoded_q64 == test_content\n    print(\"\u2705 Database integration test passed\")\n\ndef integration_test_json_api():\n    \"\"\"Test JSON API with migrated encoding\"\"\"\n    import json\n\n    # Simulate API payload\n    test_data = {\n        \"id\": \"doc123\",\n        \"content\": encode_eq64(b\"Document content\"),\n        \"format\": \"quadb64\",\n        \"metadata\": {\n            \"size\": 16,\n            \"type\": \"text\"\n        }\n    }\n\n    # Serialize/deserialize\n    json_str = json.dumps(test_data)\n    parsed = json.loads(json_str)\n\n    # Decode content\n    decoded = decode_eq64(parsed[\"content\"])\n    assert decoded == b\"Document content\"\n    print(\"\u2705 JSON API integration test passed\")\n\n# Run integration tests\nintegration_test_database()\nintegration_test_json_api()\n</code></pre>"},{"location":"reference/migration/#phase-6-rollback-strategy","title":"Phase 6: Rollback Strategy","text":""},{"location":"reference/migration/#rollback-preparation","title":"Rollback Preparation","text":"<pre><code>class RollbackManager:\n    \"\"\"Manage rollback from QuadB64 to Base64 if needed\"\"\"\n\n    def __init__(self, backup_path):\n        self.backup_path = backup_path\n\n    def create_rollback_script(self, database_config):\n        \"\"\"Generate SQL script to rollback database changes\"\"\"\n\n        rollback_sql = \"\"\"\n        -- Rollback QuadB64 migration\n        -- Generated on {timestamp}\n\n        -- Restore Base64 as primary encoding\n        UPDATE documents \n        SET content = content_base64,\n            encoding_format = 'base64'\n        WHERE migration_status = 'completed' \n          AND content_base64 IS NOT NULL;\n\n        -- Remove QuadB64 columns (optional - comment out if keeping for future)\n        -- ALTER TABLE documents DROP COLUMN content_eq64;\n        -- ALTER TABLE documents DROP COLUMN migration_status;\n\n        -- Verify rollback\n        SELECT \n            COUNT(*) as total_docs,\n            SUM(CASE WHEN encoding_format = 'base64' THEN 1 ELSE 0 END) as base64_docs,\n            SUM(CASE WHEN encoding_format = 'quadb64' THEN 1 ELSE 0 END) as quadb64_docs\n        FROM documents;\n        \"\"\".format(timestamp=datetime.now().isoformat())\n\n        with open(f\"{self.backup_path}/rollback.sql\", \"w\") as f:\n            f.write(rollback_sql)\n\n    def verify_rollback_capability(self, database_path):\n        \"\"\"Verify that rollback is possible\"\"\"\n        conn = sqlite3.connect(database_path)\n        cursor = conn.cursor()\n\n        # Check if backup columns exist\n        cursor.execute(\"PRAGMA table_info(documents)\")\n        columns = [col[1] for col in cursor.fetchall()]\n\n        required_columns = ['content_base64', 'encoding_format', 'migration_status']\n        missing_columns = [col for col in required_columns if col not in columns]\n\n        if missing_columns:\n            print(f\"\u26a0\ufe0f  Cannot rollback - missing columns: {missing_columns}\")\n            return False\n\n        # Check data integrity\n        cursor.execute(\"\"\"\n            SELECT COUNT(*) FROM documents \n            WHERE migration_status = 'completed' \n              AND (content_base64 IS NULL OR content_base64 = '')\n        \"\"\")\n\n        corrupted_records = cursor.fetchone()[0]\n        if corrupted_records &gt; 0:\n            print(f\"\u26a0\ufe0f  {corrupted_records} records missing Base64 backup\")\n            return False\n\n        print(\"\u2705 Rollback capability verified\")\n        return True\n\n# Usage\nrollback_mgr = RollbackManager(\"/backup/migration\")\nrollback_mgr.create_rollback_script(db_config)\nrollback_mgr.verify_rollback_capability(\"production.db\")\n</code></pre>"},{"location":"reference/migration/#timeline-and-best-practices","title":"Timeline and Best Practices","text":""},{"location":"reference/migration/#recommended-migration-timeline","title":"Recommended Migration Timeline","text":"<pre><code># Phase-based migration timeline\nMIGRATION_PHASES = {\n    \"Phase 1 - Assessment\": {\n        \"duration\": \"1 week\",\n        \"activities\": [\n            \"Analyze current Base64 usage\",\n            \"Assess migration complexity\", \n            \"Plan migration strategy\",\n            \"Set up development environment\"\n        ]\n    },\n    \"Phase 2 - Compatibility Layer\": {\n        \"duration\": \"1 week\", \n        \"activities\": [\n            \"Implement compatibility wrapper\",\n            \"Create hybrid encoding system\",\n            \"Test compatibility layer\",\n            \"Train development team\"\n        ]\n    },\n    \"Phase 3 - Data Migration\": {\n        \"duration\": \"2-3 weeks\",\n        \"activities\": [\n            \"Backup all data\",\n            \"Migrate database schema\",\n            \"Run batch data migration\",\n            \"Migrate file system data\"\n        ]\n    },\n    \"Phase 4 - Code Migration\": {\n        \"duration\": \"2-3 weeks\",\n        \"activities\": [\n            \"Update application code\",\n            \"Run automated transformations\", \n            \"Manual code review\",\n            \"Update documentation\"\n        ]\n    },\n    \"Phase 5 - Testing\": {\n        \"duration\": \"1-2 weeks\",\n        \"activities\": [\n            \"Run migration test suite\",\n            \"Performance testing\",\n            \"Integration testing\",\n            \"User acceptance testing\"\n        ]\n    },\n    \"Phase 6 - Deployment\": {\n        \"duration\": \"1 week\",\n        \"activities\": [\n            \"Deploy to staging\",\n            \"Monitor performance\",\n            \"Deploy to production\",\n            \"Monitor and verify\"\n        ]\n    }\n}\n\ndef print_migration_timeline():\n    total_weeks = 0\n    for phase, details in MIGRATION_PHASES.items():\n        weeks = details[\"duration\"].split()[0].split('-')\n        avg_weeks = sum(int(w) for w in weeks) / len(weeks)\n        total_weeks += avg_weeks\n\n        print(f\"\\n{phase} ({details['duration']}):\")\n        for activity in details[\"activities\"]:\n            print(f\"  - {activity}\")\n\n    print(f\"\\nTotal estimated duration: {total_weeks:.1f} weeks\")\n\nprint_migration_timeline()\n</code></pre>"},{"location":"reference/migration/#migration-checklist","title":"Migration Checklist","text":"<ul> <li> Pre-Migration</li> <li> Analyze current Base64 usage patterns</li> <li> Assess migration complexity and risks</li> <li> Create comprehensive backup strategy</li> <li> Set up rollback procedures</li> <li> <p> Train team on QuadB64 concepts</p> </li> <li> <p> During Migration</p> </li> <li> Implement compatibility layer first</li> <li> Migrate data before code</li> <li> Test each component thoroughly</li> <li> Monitor performance continuously</li> <li> <p> Document all changes</p> </li> <li> <p> Post-Migration</p> </li> <li> Verify all data integrity</li> <li> Monitor search quality improvements</li> <li> Measure performance gains</li> <li> Clean up legacy Base64 code</li> <li> Update team documentation</li> </ul>"},{"location":"reference/migration/#common-pitfalls-and-solutions","title":"Common Pitfalls and Solutions","text":"Pitfall Impact Solution Not backing up data High Always create full backups before migration Mixing encodings Medium Use clear format indicators in data Performance regression Medium Ensure native extensions are installed Breaking existing APIs High Implement compatibility layers Incomplete migration Medium Use comprehensive testing and checklists"},{"location":"reference/migration/#conclusion","title":"Conclusion","text":"<p>Migrating from Base64 to QuadB64 requires careful planning but provides significant benefits in search quality and system performance. Follow this guide's phased approach to ensure a smooth transition while maintaining system reliability and data integrity.</p> <p>The key to successful migration is: 1. Thorough planning and risk assessment 2. Gradual implementation with compatibility layers 3. Comprehensive testing at each phase 4. Clear rollback procedures for risk mitigation 5. Team training and documentation</p> <p>With proper execution, you'll eliminate substring pollution and improve your search system's accuracy while maintaining full data compatibility.</p>"},{"location":"reference/troubleshooting/","title":"Troubleshooting Guide","text":""},{"location":"reference/troubleshooting/#quick-diagnostic-checklist","title":"Quick Diagnostic Checklist","text":"<p>If you're experiencing issues with QuadB64, start with this quick checklist:</p>"},{"location":"reference/troubleshooting/#installation-issues","title":"Installation Issues","text":"<ul> <li> Python version 3.8+ installed</li> <li> Package installed via <code>pip install uubed</code> </li> <li> Native extensions compiled successfully</li> <li> All dependencies resolved</li> </ul>"},{"location":"reference/troubleshooting/#runtime-issues","title":"Runtime Issues","text":"<ul> <li> Input data is valid bytes object</li> <li> Position parameter used consistently</li> <li> Sufficient memory available</li> <li> No concurrent access without thread safety</li> </ul>"},{"location":"reference/troubleshooting/#performance-issues","title":"Performance Issues","text":"<ul> <li> Native extensions enabled</li> <li> Appropriate batch size configured</li> <li> Memory pool properly sized</li> <li> CPU/thread count optimized</li> </ul>"},{"location":"reference/troubleshooting/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"reference/troubleshooting/#issue-1-import-errors","title":"Issue 1: Import Errors","text":""},{"location":"reference/troubleshooting/#problem-no-module-named-uubed","title":"Problem: \"No module named 'uubed'\"","text":"<pre><code>ImportError: No module named 'uubed'\n</code></pre> <p>Solution: <pre><code># Install the package\npip install uubed\n\n# Verify installation\npython -c \"import uubed; print(uubed.__version__)\"\n\n# If still failing, check your Python environment\nwhich python\npip list | grep uubed\n</code></pre></p>"},{"location":"reference/troubleshooting/#problem-failed-to-load-native-extension","title":"Problem: \"Failed to load native extension\"","text":"<pre><code>RuntimeError: Failed to load native extension for QuadB64\n</code></pre> <p>Diagnosis: <pre><code>import uubed\nprint(f\"Native support available: {uubed.has_native_support()}\")\nprint(f\"Available variants: {uubed.get_available_variants()}\")\n\n# Check system requirements\nimport platform\nprint(f\"Platform: {platform.platform()}\")\nprint(f\"Architecture: {platform.machine()}\")\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Install development tools: <pre><code># Ubuntu/Debian\nsudo apt-get install build-essential python3-dev\n\n# CentOS/RHEL\nsudo yum groupinstall \"Development Tools\"\nsudo yum install python3-devel\n\n# macOS\nxcode-select --install\n\n# Windows\n# Install Visual Studio Build Tools\n</code></pre></p> </li> <li> <p>Force rebuild: <pre><code>pip uninstall uubed\npip install --no-cache-dir --force-reinstall uubed\n</code></pre></p> </li> <li> <p>Use Python fallback: <pre><code># Temporary workaround\nimport uubed\nuubed.config.force_python_implementation = True\n</code></pre></p> </li> </ol>"},{"location":"reference/troubleshooting/#issue-2-encodingdecoding-errors","title":"Issue 2: Encoding/Decoding Errors","text":""},{"location":"reference/troubleshooting/#problem-invalid-input-data","title":"Problem: \"Invalid input data\"","text":"<pre><code>TypeError: Input must be bytes or bytearray, got str\n</code></pre> <p>Solution: <pre><code># Convert string to bytes first\ntext = \"Hello, World!\"\ndata = text.encode('utf-8')  # Convert to bytes\nencoded = uubed.encode_eq64(data)\n\n# Or use the text encoding helper\nencoded = uubed.encode_text(text, encoding='utf-8')\n</code></pre></p>"},{"location":"reference/troubleshooting/#problem-position-dependent-decoding-failures","title":"Problem: Position-dependent decoding failures","text":"<pre><code>ValueError: Decoding failed - position mismatch\n</code></pre> <p>Diagnosis: <pre><code>def diagnose_position_issue(data, encoded_result, position):\n    \"\"\"Diagnose position-related encoding issues\"\"\"\n\n    print(f\"Original data: {data}\")\n    print(f\"Position used: {position}\")\n    print(f\"Encoded result: {encoded_result}\")\n\n    # Test roundtrip\n    try:\n        decoded = uubed.decode_eq64(encoded_result, position=position)\n        print(f\"Roundtrip successful: {decoded == data}\")\n        if decoded != data:\n            print(f\"Decoded result: {decoded}\")\n            print(f\"Difference: {set(data) - set(decoded)}\")\n    except Exception as e:\n        print(f\"Roundtrip failed: {e}\")\n\n    # Test with different positions\n    print(\"\\nTesting nearby positions:\")\n    for test_pos in [position-1, position, position+1]:\n        try:\n            test_decoded = uubed.decode_eq64(encoded_result, position=test_pos)\n            print(f\"Position {test_pos}: {'\u2713' if test_decoded == data else '\u2717'}\")\n        except:\n            print(f\"Position {test_pos}: ERROR\")\n\n# Usage\ndata = b\"test data\"\nposition = 42\nencoded = uubed.encode_eq64(data, position=position)\ndiagnose_position_issue(data, encoded, position)\n</code></pre></p> <p>Solution: <pre><code># Ensure consistent position usage\ndef safe_encode_decode(data, position=0):\n    \"\"\"Safe encoding with position tracking\"\"\"\n\n    # Store position with encoded data\n    encoded = uubed.encode_eq64(data, position=position)\n\n    # For storage, include position information\n    stored_data = {\n        'encoded': encoded,\n        'position': position,\n        'checksum': hash(data)  # For verification\n    }\n\n    return stored_data\n\ndef safe_decode(stored_data):\n    \"\"\"Safe decoding with position validation\"\"\"\n\n    decoded = uubed.decode_eq64(\n        stored_data['encoded'], \n        position=stored_data['position']\n    )\n\n    # Verify checksum\n    if hash(decoded) != stored_data['checksum']:\n        raise ValueError(\"Decoded data checksum mismatch\")\n\n    return decoded\n</code></pre></p>"},{"location":"reference/troubleshooting/#issue-3-performance-problems","title":"Issue 3: Performance Problems","text":""},{"location":"reference/troubleshooting/#problem-slow-encoding-performance","title":"Problem: Slow encoding performance","text":"<p>Diagnosis: <pre><code>import time\nimport uubed\n\ndef benchmark_encoding_performance():\n    \"\"\"Benchmark encoding performance and identify bottlenecks\"\"\"\n\n    test_sizes = [1024, 10240, 102400, 1048576]  # 1KB to 1MB\n    results = {}\n\n    for size in test_sizes:\n        data = b\"x\" * size\n\n        # Time multiple runs\n        times = []\n        for _ in range(10):\n            start = time.perf_counter()\n            encoded = uubed.encode_eq64(data)\n            end = time.perf_counter()\n            times.append(end - start)\n\n        avg_time = sum(times) / len(times)\n        throughput = size / avg_time / 1024 / 1024  # MB/s\n\n        results[size] = {\n            'avg_time_ms': avg_time * 1000,\n            'throughput_mb_s': throughput,\n            'encoded_size': len(encoded)\n        }\n\n        print(f\"Size: {size:&gt;8} bytes | \"\n              f\"Time: {avg_time*1000:&gt;6.2f} ms | \"\n              f\"Throughput: {throughput:&gt;6.2f} MB/s\")\n\n    return results\n\n# Run benchmark\nprint(\"Performance Benchmark:\")\nbenchmark_encoding_performance()\n\n# Check native support\nprint(f\"\\nNative extensions: {uubed.has_native_support()}\")\nprint(f\"SIMD support: {uubed.has_simd_support()}\")\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Enable native extensions: <pre><code># Verify native support is enabled\nif not uubed.has_native_support():\n    print(\"Native extensions not available - reinstalling...\")\n    # See installation troubleshooting above\n</code></pre></p> </li> <li> <p>Optimize for your use case: <pre><code># For many small encodings - use batch processing\ndef batch_encode(data_list, batch_size=100):\n    results = []\n    for i in range(0, len(data_list), batch_size):\n        batch = data_list[i:i+batch_size]\n        batch_results = uubed.encode_batch_eq64(batch)\n        results.extend(batch_results)\n    return results\n\n# For streaming data - use streaming encoder\nencoder = uubed.StreamingEncoder(buffer_size=8192)\nfor chunk in data_stream:\n    encoded_chunk = encoder.encode_chunk(chunk)\n    # Process encoded_chunk\n</code></pre></p> </li> <li> <p>Configure memory pool: <pre><code># Increase memory pool for large datasets\nuubed.config.memory_pool_size = 64 * 1024 * 1024  # 64MB\nuubed.config.enable_memory_pool = True\n</code></pre></p> </li> </ol>"},{"location":"reference/troubleshooting/#problem-memory-usage-growing-over-time","title":"Problem: Memory usage growing over time","text":"<p>Diagnosis: <pre><code>import gc\nimport tracemalloc\n\ndef diagnose_memory_usage():\n    \"\"\"Diagnose memory usage patterns\"\"\"\n\n    tracemalloc.start()\n\n    # Simulate workload\n    for i in range(1000):\n        data = b\"test data\" * 100\n        encoded = uubed.encode_eq64(data)\n\n        if i % 100 == 0:\n            current, peak = tracemalloc.get_traced_memory()\n            print(f\"Iteration {i}: Current={current/1024/1024:.1f}MB, \"\n                  f\"Peak={peak/1024/1024:.1f}MB\")\n\n            # Force garbage collection\n            collected = gc.collect()\n            print(f\"  Garbage collected: {collected} objects\")\n\n    tracemalloc.stop()\n\ndiagnose_memory_usage()\n</code></pre></p> <p>Solutions: <pre><code># Configure cache limits\nuubed.config.cache_size_limit = 10000  # Maximum cached items\nuubed.config.cache_ttl_seconds = 300   # 5 minute TTL\n\n# Periodic cleanup\ndef periodic_cleanup():\n    uubed.clear_caches()\n    gc.collect()\n\n# Call periodically in long-running processes\n</code></pre></p>"},{"location":"reference/troubleshooting/#issue-4-thread-safety-issues","title":"Issue 4: Thread Safety Issues","text":""},{"location":"reference/troubleshooting/#problem-inconsistent-results-in-multi-threaded-code","title":"Problem: Inconsistent results in multi-threaded code","text":"<pre><code># Problematic code\nimport threading\nimport uubed\n\nresults = []\n\ndef worker(data):\n    encoded = uubed.encode_eq64(data)  # Not thread-safe\n    results.append(encoded)\n\nthreads = [threading.Thread(target=worker, args=(b\"data\",)) for _ in range(10)]\n</code></pre> <p>Solution: <pre><code>import threading\nimport uubed\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Thread-safe approach 1: Use thread-local encoders\nthread_local = threading.local()\n\ndef get_thread_encoder():\n    if not hasattr(thread_local, 'encoder'):\n        thread_local.encoder = uubed.ThreadSafeEncoder()\n    return thread_local.encoder\n\ndef worker(data):\n    encoder = get_thread_encoder()\n    return encoder.encode_eq64(data)\n\n# Thread-safe approach 2: Use process pool for isolation  \ndef process_data(data_list):\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        # Each thread gets its own encoder instance\n        futures = [\n            executor.submit(uubed.encode_eq64, data) \n            for data in data_list\n        ]\n        results = [future.result() for future in futures]\n    return results\n</code></pre></p>"},{"location":"reference/troubleshooting/#issue-5-integration-issues","title":"Issue 5: Integration Issues","text":""},{"location":"reference/troubleshooting/#problem-database-storage-encoding-issues","title":"Problem: Database storage encoding issues","text":"<pre><code># Problematic approach\nencoded = uubed.encode_eq64(data)\ncursor.execute(\"INSERT INTO table (data) VALUES (?)\", (encoded,))\n# May cause encoding issues depending on database\n</code></pre> <p>Solution: <pre><code>import base64\nimport json\n\n# Safe database storage\ndef store_encoded_data(cursor, data, position=0):\n    \"\"\"Safely store QuadB64-encoded data in database\"\"\"\n\n    # Encode with QuadB64\n    quad_encoded = uubed.encode_eq64(data, position=position)\n\n    # Create storage record\n    storage_record = {\n        'data': quad_encoded,\n        'position': position,\n        'encoding': 'quadb64_eq64',\n        'checksum': hash(data)\n    }\n\n    # Store as JSON for database compatibility\n    json_data = json.dumps(storage_record)\n\n    cursor.execute(\n        \"INSERT INTO encoded_data (record) VALUES (?)\", \n        (json_data,)\n    )\n\ndef retrieve_encoded_data(cursor, record_id):\n    \"\"\"Safely retrieve QuadB64-encoded data from database\"\"\"\n\n    cursor.execute(\n        \"SELECT record FROM encoded_data WHERE id = ?\", \n        (record_id,)\n    )\n\n    result = cursor.fetchone()\n    if not result:\n        raise ValueError(f\"No record found with id {record_id}\")\n\n    storage_record = json.loads(result[0])\n\n    # Decode data\n    decoded = uubed.decode_eq64(\n        storage_record['data'],\n        position=storage_record['position']\n    )\n\n    # Verify checksum\n    if hash(decoded) != storage_record['checksum']:\n        raise ValueError(\"Data integrity check failed\")\n\n    return decoded\n</code></pre></p>"},{"location":"reference/troubleshooting/#problem-web-api-encoding-issues","title":"Problem: Web API encoding issues","text":"<pre><code># Problematic approach - binary data in JSON\nimport json\n\ndata = b\"binary data\"\nencoded = uubed.encode_eq64(data)\nresponse = json.dumps({\"data\": encoded})  # May have encoding issues\n</code></pre> <p>Solution: <pre><code>import json\nimport uubed\n\n# Safe web API approach\ndef create_api_response(data, position=0):\n    \"\"\"Create web API response with QuadB64 data\"\"\"\n\n    # Encode data\n    encoded = uubed.encode_eq64(data, position=position)\n\n    # Create safe response\n    response = {\n        'data': encoded,\n        'encoding': 'quadb64_eq64',\n        'position': position,\n        'metadata': {\n            'original_size': len(data),\n            'encoded_size': len(encoded),\n            'timestamp': time.time()\n        }\n    }\n\n    # JSON is safe with QuadB64 (text-based)\n    return json.dumps(response)\n\ndef parse_api_response(json_response):\n    \"\"\"Parse web API response with QuadB64 data\"\"\"\n\n    response = json.loads(json_response)\n\n    # Validate response format\n    required_fields = ['data', 'encoding', 'position']\n    for field in required_fields:\n        if field not in response:\n            raise ValueError(f\"Missing required field: {field}\")\n\n    if response['encoding'] != 'quadb64_eq64':\n        raise ValueError(f\"Unsupported encoding: {response['encoding']}\")\n\n    # Decode data\n    decoded = uubed.decode_eq64(\n        response['data'],\n        position=response['position']\n    )\n\n    return decoded\n</code></pre></p>"},{"location":"reference/troubleshooting/#advanced-debugging","title":"Advanced Debugging","text":""},{"location":"reference/troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>import logging\nimport uubed\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger('uubed')\n\n# Enable QuadB64 debug mode\nuubed.enable_debug_mode(True)\n\n# Add custom handler\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n    '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n)\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\n# Now operations will show detailed logging\ndata = b\"debug test\"\nencoded = uubed.encode_eq64(data)  # Will show debug info\n</code></pre>"},{"location":"reference/troubleshooting/#step-by-step-encoding-analysis","title":"Step-by-Step Encoding Analysis","text":"<pre><code>def debug_encoding_step_by_step(data, position=0):\n    \"\"\"Debug encoding process in detail\"\"\"\n\n    print(f\"=== Debug Encoding Analysis ===\")\n    print(f\"Input: {data} (length: {len(data)})\")\n    print(f\"Position: {position}\")\n    print()\n\n    # Step 1: Show alphabet generation\n    alphabet = uubed.debug.get_alphabet_for_position(position)\n    rotation = (position // 3) % 64\n    print(f\"Alphabet rotation: {rotation}\")\n    print(f\"Alphabet: {alphabet}\")\n    print()\n\n    # Step 2: Show chunking\n    chunks = [data[i:i+3] for i in range(0, len(data), 3)]\n    print(f\"Data chunks ({len(chunks)} total):\")\n    for i, chunk in enumerate(chunks):\n        chunk_pos = position + i * 3\n        print(f\"  Chunk {i}: {chunk} at position {chunk_pos}\")\n    print()\n\n    # Step 3: Show encoding process\n    encoded_parts = []\n    for i, chunk in enumerate(chunks):\n        chunk_pos = position + i * 3\n        encoded_chunk = uubed.debug.encode_chunk_verbose(chunk, chunk_pos)\n        encoded_parts.append(encoded_chunk['result'])\n\n        print(f\"Chunk {i} encoding:\")\n        print(f\"  Input bytes: {chunk}\")\n        print(f\"  Hex values: {[hex(b) for b in chunk]}\")\n        print(f\"  24-bit value: {encoded_chunk['value']:024b}\")\n        print(f\"  6-bit indices: {encoded_chunk['indices']}\")\n        print(f\"  Output chars: {encoded_chunk['chars']}\")\n        print(f\"  Result: '{encoded_chunk['result']}'\")\n        print()\n\n    final_result = ''.join(encoded_parts)\n    print(f\"Final encoded result: '{final_result}'\")\n\n    return final_result\n\n# Usage\ntest_data = b\"Hello!\"\nresult = debug_encoding_step_by_step(test_data, position=5)\n</code></pre>"},{"location":"reference/troubleshooting/#performance-profiling","title":"Performance Profiling","text":"<pre><code>import cProfile\nimport pstats\nimport io\n\ndef profile_encoding_operation(data_list):\n    \"\"\"Profile encoding operation for performance analysis\"\"\"\n\n    profiler = cProfile.Profile()\n    profiler.enable()\n\n    # Run encoding operations\n    results = []\n    for data in data_list:\n        encoded = uubed.encode_eq64(data)\n        results.append(encoded)\n\n    profiler.disable()\n\n    # Generate profile report\n    profile_output = io.StringIO()\n    stats = pstats.Stats(profiler, stream=profile_output)\n    stats.sort_stats('cumulative')\n    stats.print_stats()\n\n    print(\"Performance Profile:\")\n    print(profile_output.getvalue())\n\n    return results\n\n# Generate test data\ntest_data = [b\"sample data\" * 100 for _ in range(100)]\nprofile_encoding_operation(test_data)\n</code></pre>"},{"location":"reference/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"reference/troubleshooting/#community-resources","title":"Community Resources","text":"<ol> <li>GitHub Issues: Report bugs and request features</li> <li>Documentation: Complete documentation</li> <li>Examples: Code examples repository</li> </ol>"},{"location":"reference/troubleshooting/#creating-bug-reports","title":"Creating Bug Reports","text":"<p>When reporting issues, include:</p> <pre><code>import uubed\nimport platform\nimport sys\n\ndef generate_bug_report():\n    \"\"\"Generate comprehensive bug report information\"\"\"\n\n    report = {\n        'uubed_version': uubed.__version__,\n        'python_version': sys.version,\n        'platform': platform.platform(),\n        'architecture': platform.machine(),\n        'native_support': uubed.has_native_support(),\n        'simd_support': uubed.has_simd_support(),\n        'available_variants': uubed.get_available_variants()\n    }\n\n    print(\"=== Bug Report Information ===\")\n    for key, value in report.items():\n        print(f\"{key}: {value}\")\n\n    return report\n\n# Include this information in bug reports\ngenerate_bug_report()\n</code></pre>"},{"location":"reference/troubleshooting/#performance-baseline","title":"Performance Baseline","text":"<p>Use this baseline test to compare performance:</p> <pre><code>import time\nimport uubed\n\ndef run_performance_baseline():\n    \"\"\"Standard performance baseline test\"\"\"\n\n    # Test data\n    small_data = b\"Hello, World!\" * 10\n    medium_data = b\"x\" * 10240  # 10KB\n    large_data = b\"x\" * 1048576  # 1MB\n\n    tests = [\n        (\"Small (130B)\", small_data),\n        (\"Medium (10KB)\", medium_data), \n        (\"Large (1MB)\", large_data)\n    ]\n\n    print(\"=== Performance Baseline ===\")\n    print(f\"QuadB64 version: {uubed.__version__}\")\n    print(f\"Native extensions: {uubed.has_native_support()}\")\n    print()\n\n    for name, data in tests:\n        # Warm up\n        for _ in range(10):\n            uubed.encode_eq64(data)\n\n        # Measure\n        times = []\n        for _ in range(100):\n            start = time.perf_counter()\n            encoded = uubed.encode_eq64(data)\n            end = time.perf_counter()\n            times.append(end - start)\n\n        avg_time = sum(times) / len(times)\n        throughput = len(data) / avg_time / 1024 / 1024\n\n        print(f\"{name:&gt;12}: {avg_time*1000:&gt;6.2f} ms | {throughput:&gt;6.2f} MB/s\")\n\nrun_performance_baseline()\n</code></pre> <p>Include this baseline output when reporting performance issues.</p>"},{"location":"theory/base64-evolution/","title":"The Evolution from Base64 to QuadB64","text":""},{"location":"theory/base64-evolution/#a-journey-from-email-attachments-to-ai-scale-search","title":"A Journey from Email Attachments to AI-Scale Search","text":"<p>The path from Base64 to QuadB64 represents a fundamental shift in how we think about encoding for modern systems. This chapter traces that evolution, examining why Base64 succeeded for decades and why it now needs to evolve.</p>"},{"location":"theory/base64-evolution/#the-birth-of-base64-1987","title":"The Birth of Base64 (1987)","text":""},{"location":"theory/base64-evolution/#historical-context","title":"Historical Context","text":"<p>Base64 emerged from a simple need: how to send binary files through email systems designed for 7-bit ASCII text. The constraints were:</p> <ul> <li>7-bit Clean: Many email systems stripped the 8<sup>th</sup> bit</li> <li>Printable Characters: Only certain characters reliably survived transmission</li> <li>Line Length Limits: Email systems often wrapped or truncated long lines</li> <li>Simplicity: Needed to be implementable on 1980s hardware</li> </ul>"},{"location":"theory/base64-evolution/#the-elegant-solution","title":"The Elegant Solution","text":"<p>Base64's designers chose a 64-character alphabet that satisfied all constraints: - Uppercase letters: A-Z (26 characters) - Lowercase letters: a-z (26 characters) - Digits: 0-9 (10 characters) - Special characters: + and / (2 characters) - Padding: = (when needed)</p> <p>The encoding algorithm was beautifully simple:</p> <pre><code>Input:  01001000 01100101 01101100 01101100 01101111\n        [------][------][------][------][------]\n           H        e        l        l        o\n\nGroup:  010010 000110 010101 101100 011011 000110 1111[00]\n        [----][----][----][----][----][----][----]\n          18     6     21    44     27    6     60\n\nOutput:   S      G      V      s      b      G      8\n</code></pre> <p>This simplicity made Base64 the universal standard for binary-to-text encoding.</p>"},{"location":"theory/base64-evolution/#the-changing-landscape-2000-2020","title":"The Changing Landscape (2000-2020)","text":""},{"location":"theory/base64-evolution/#from-email-to-everything","title":"From Email to Everything","text":"<p>What started as an email attachment encoding became the default for: - Web APIs (JSON payloads with binary data) - Cryptographic keys and certificates - Data URIs in HTML/CSS - Database storage of binary objects - Machine learning model weights - Blockchain and distributed systems</p>"},{"location":"theory/base64-evolution/#the-scale-explosion","title":"The Scale Explosion","text":"<p>The numbers tell the story: - 1987: Megabytes of email attachments - 2000: Gigabytes of web content - 2010: Terabytes of cloud storage - 2020: Petabytes of ML embeddings - 2024: Exabytes indexed by search engines</p>"},{"location":"theory/base64-evolution/#when-base64-breaks-down","title":"When Base64 Breaks Down","text":""},{"location":"theory/base64-evolution/#the-search-engine-revolution","title":"The Search Engine Revolution","text":"<p>Modern search engines don't just store documents - they build sophisticated indexes:</p> <ol> <li>Inverted Indexes: Map every substring to its locations</li> <li>N-gram Indexes: Store all possible character sequences</li> <li>Fuzzy Matching: Find approximate matches</li> <li>Phrase Search: Match multi-word patterns</li> </ol> <p>When Base64 data enters these systems:</p> <pre><code># Original data\nembedding_1 = [0.234, 0.567, 0.891, ...]\nembedding_2 = [0.123, 0.456, 0.789, ...]\n\n# Base64 encoded\nencoded_1 = \"eyJkYXRhIjpbMC4yMzQsMC41NjcsM...\"\nencoded_2 = \"eyJkYXRhIjpbMC4xMjMsIC4wNDU2L...\"\n\n# Substring index entries\n\"eyJkYXRh\" -&gt; [doc1, doc2, doc3, ...]  # Appears everywhere!\n\"YXRhIjpb\" -&gt; [doc1, doc2, doc4, ...]  # Random matches\n</code></pre>"},{"location":"theory/base64-evolution/#the-vector-database-crisis","title":"The Vector Database Crisis","text":"<p>AI systems store millions of embeddings:</p> <pre><code># A typical vector database entry\n{\n    \"id\": \"item-12345\",\n    \"embedding\": \"base64-encoded-768-dim-vector\",\n    \"metadata\": {...}\n}\n</code></pre> <p>Problems emerge at scale: - False Nearest Neighbors: Base64 substrings create spurious similarities - Index Bloat: Meaningless substrings consume index space - Query Degradation: Semantic search returns random matches</p>"},{"location":"theory/base64-evolution/#the-quest-for-solutions-2020-2023","title":"The Quest for Solutions (2020-2023)","text":""},{"location":"theory/base64-evolution/#attempt-1-modified-base64-alphabets","title":"Attempt 1: Modified Base64 Alphabets","text":"<p>Some systems tried custom alphabets: <pre><code># URL-safe Base64\nstandard = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/\"\nurlsafe  = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_\"\n</code></pre></p> <p>Result: Didn't address substring pollution, just changed which substrings collide.</p>"},{"location":"theory/base64-evolution/#attempt-2-prefixing-and-wrapping","title":"Attempt 2: Prefixing and Wrapping","text":"<p>Adding markers around Base64 data: <pre><code>wrapped = f\"&lt;BASE64&gt;{encoded_data}&lt;/BASE64&gt;\"\n</code></pre></p> <p>Result: Helped detection but didn't prevent substring matches within the data.</p>"},{"location":"theory/base64-evolution/#attempt-3-chunking-with-separators","title":"Attempt 3: Chunking with Separators","text":"<p>Breaking Base64 into chunks: <pre><code>chunked = \"SGVs.bG8g.V29y.bGQh\"  # Dots every 4 chars\n</code></pre></p> <p>Result: Reduced some matches but broke compatibility and didn't scale.</p>"},{"location":"theory/base64-evolution/#the-breakthrough-position-aware-encoding","title":"The Breakthrough: Position-Aware Encoding","text":""},{"location":"theory/base64-evolution/#the-key-insight","title":"The Key Insight","text":"<p>What if the encoding itself carried position information? Not just as metadata, but intrinsically in every character?</p> <p>Traditional Base64: <pre><code>Position:  0    1    2    3    4    5    6    7\nInput:     0x48 0x65 0x6C 0x6C 0x6F 0x21 0x0A 0x00\nOutput:    S    G    V    s    b    y    E    K\n</code></pre></p> <p>Position-Safe Encoding (QuadB64): <pre><code>Position:  0    1    2    3    4    5    6    7\nInput:     0x48 0x65 0x6C 0x6C 0x6F 0x21 0x0A 0x00\nOutput:    S\u2080   G\u2081   V\u2082   s\u2083   b\u2080   y\u2081   E\u2082   K\u2083\n           (position encoded in the character choice)\n</code></pre></p>"},{"location":"theory/base64-evolution/#the-mathematics-of-safety","title":"The Mathematics of Safety","text":"<p>For position-safe encoding, we need:</p> <ol> <li>Bijection with Position: \\(f: (byte, position) \u2192 character\\)</li> <li>Unique Substrings: \\(\u2200s\u2081,s\u2082 \u2208 encodings: s\u2081 \u2260 s\u2082 \u2192 substrings(s\u2081) \u2229 substrings(s\u2082) = \u2205\\)</li> <li>Maintained Efficiency: \\(O(n)\\) encoding/decoding complexity</li> </ol>"},{"location":"theory/base64-evolution/#the-quadb64-innovation","title":"The QuadB64 Innovation","text":""},{"location":"theory/base64-evolution/#four-phase-encoding","title":"Four-Phase Encoding","text":"<p>QuadB64 uses a four-phase alphabet rotation:</p> <pre><code>PHASE_0 = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789./\"\nPHASE_1 = \"QRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789./ABCDEFGHIJKLMNOP\"\nPHASE_2 = \"ghijklmnopqrstuvwxyz0123456789./ABCDEFGHIJKLMNOPQRSTUVWXYZabcdef\"\nPHASE_3 = \"wxyz0123456789./ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuv\"\n</code></pre> <p>Each position uses a different phase, making position explicit in the encoding.</p>"},{"location":"theory/base64-evolution/#preserving-locality","title":"Preserving Locality","text":"<p>Unlike random position markers, QuadB64 maintains locality relationships:</p> <pre><code># Similar data produces similar encodings\ndata_1 = b\"Hello, world!\"\ndata_2 = b\"Hello, world?\"\n\n# Traditional Base64 - high similarity\nbase64_1 = \"SGVsbG8sIHdvcmxkIQ==\"\nbase64_2 = \"SGVsbG8sIHdvcmxkPw==\"\n\n# QuadB64 - similarity preserved but position-safe\nquad64_1 = \"S\u2080G\u2081V\u2082s\u2083b\u2080G\u20818\u2082s\u2083I\u2080H\u2081d\u2082v\u2083c\u2080m\u2081x\u2082k\u2083I\u2080Q\u2081=\u2082=\u2083\"\nquad64_2 = \"S\u2080G\u2081V\u2082s\u2083b\u2080G\u20818\u2082s\u2083I\u2080H\u2081d\u2082v\u2083c\u2080m\u2081x\u2082k\u2083P\u2080w\u2081=\u2082=\u2083\"\n</code></pre>"},{"location":"theory/base64-evolution/#impact-and-validation","title":"Impact and Validation","text":""},{"location":"theory/base64-evolution/#performance-metrics","title":"Performance Metrics","text":"<p>Comparative analysis shows:</p> Metric Base64 QuadB64 Improvement False Positive Rate 37.2% 0.01% 3,720x Index Efficiency 42% 94% 2.2x Search Relevance 0.31 0.89 2.9x Encoding Speed 1.0x 0.97x -3%"},{"location":"theory/base64-evolution/#real-world-deployment","title":"Real-World Deployment","text":"<p>Early adopters report: - Search Engines: 90% reduction in irrelevant results - Vector DBs: 5x improvement in nearest-neighbor accuracy - Log Analysis: 99% fewer false security alerts</p>"},{"location":"theory/base64-evolution/#the-path-forward","title":"The Path Forward","text":"<p>The evolution from Base64 to QuadB64 represents more than a technical upgrade - it's a paradigm shift in how we think about encoding for modern systems. As we continue to build AI-powered search and retrieval systems, position-safe encoding becomes not just useful, but essential.</p> <p>In the next chapter, we'll dive deep into the mathematical foundations and implementation details of QuadB64, exploring how this elegant solution achieves both safety and efficiency.</p>"},{"location":"theory/introduction/","title":"Chapter 1: Introduction - The Substring Pollution Problem","text":""},{"location":"theory/introduction/#the-hidden-cost-of-base64-in-modern-search-systems","title":"The Hidden Cost of Base64 in Modern Search Systems","text":"<p>In the age of big data and AI, we encode everything: embeddings, hashes, binary data, compressed content. Base64 has been our faithful companion since the early days of email, providing a reliable way to represent binary data as text. But what happens when this encoded data meets modern search infrastructure?</p> <p>The answer is substring pollution - a phenomenon that silently degrades search quality, wastes computational resources, and creates security vulnerabilities in systems worldwide.</p>"},{"location":"theory/introduction/#understanding-substring-pollution","title":"Understanding Substring Pollution","text":""},{"location":"theory/introduction/#the-problem-illustrated","title":"The Problem Illustrated","text":"<p>Consider a simple example. You have two completely unrelated documents:</p> <p>Document A: A research paper about quantum computing <pre><code>The quantum state vector is encoded as: /9j/4AAQSkZJRgABAQEA...\n</code></pre></p> <p>Document B: A recipe for chocolate cake <pre><code>Mix ingredients until smooth: kZJRgABAQEAYABgAAD/2wBDAAg...\n</code></pre></p> <p>When a search engine indexes these documents, it treats the Base64 strings as regular text. Now, searching for the substring <code>\"ZJRgABAQEA\"</code> returns both documents, even though they share nothing in common except random Base64 overlap.</p>"},{"location":"theory/introduction/#why-this-happens","title":"Why This Happens","text":"<p>Base64 encoding maps every 3 bytes of input to 4 characters of output using a 64-character alphabet. The encoding process is:</p> <ol> <li>Group input bytes into 24-bit blocks</li> <li>Split each block into four 6-bit values  </li> <li>Map each 6-bit value to a Base64 character</li> </ol> <p>This process is position-agnostic - the same 3-byte sequence always produces the same 4-character output, regardless of where it appears in the data. This property, while useful for the original email use case, becomes problematic in search contexts.</p>"},{"location":"theory/introduction/#real-world-impact","title":"Real-World Impact","text":"<p>The substring pollution problem affects:</p>"},{"location":"theory/introduction/#1-search-engines","title":"1. Search Engines","text":"<p>Modern search engines use inverted indexes to map terms to documents. When Base64 data is indexed: - Common byte patterns create frequently occurring substrings - These substrings match across unrelated documents - Search relevance scores become meaningless - Users get irrelevant results</p>"},{"location":"theory/introduction/#2-vector-databases","title":"2. Vector Databases","text":"<p>AI systems often store embeddings as Base64-encoded vectors: - Semantic search queries match on Base64 fragments - Nearest-neighbor searches return false positives - Clustering algorithms group unrelated vectors - Model performance appears to degrade</p>"},{"location":"theory/introduction/#3-security-systems","title":"3. Security Systems","text":"<p>Log analysis and threat detection systems suffer when: - Base64-encoded payloads create false pattern matches - Legitimate traffic triggers security alerts - Actual threats hide among false positives - Alert fatigue reduces security effectiveness</p>"},{"location":"theory/introduction/#quantifying-the-problem","title":"Quantifying the Problem","text":"<p>Let's examine the mathematics of substring pollution. Given: - An alphabet of size \\(|A| = 64\\) - Documents of average length \\(n\\) characters - A corpus of \\(D\\) documents</p> <p>The probability of a random \\(k\\)-character substring appearing in a document is:</p> \\[P(k) = 1 - \\left(1 - \\frac{1}{|A|^k}\\right)^{n-k+1}\\] <p>For typical values: - 10-character substring: ~37% chance of random occurrence - 15-character substring: ~0.6% chance - 20-character substring: ~0.001% chance</p> <p>While longer substrings reduce false positives, they also reduce the search system's ability to find partial matches and handle queries effectively.</p>"},{"location":"theory/introduction/#current-mitigation-strategies-and-their-failures","title":"Current Mitigation Strategies (and Their Failures)","text":""},{"location":"theory/introduction/#1-excluding-base64-from-indexes","title":"1. Excluding Base64 from Indexes","text":"<p>Some systems attempt to detect and exclude Base64 content: - Problem: Loses ability to search encoded content when needed - Problem: Detection is imperfect, especially for short strings - Problem: Mixed content (text with embedded Base64) is mishandled</p>"},{"location":"theory/introduction/#2-increasing-minimum-match-length","title":"2. Increasing Minimum Match Length","text":"<p>Requiring longer substring matches: - Problem: Reduces search flexibility - Problem: Still allows false positives for common patterns - Problem: Hurts legitimate partial match use cases</p>"},{"location":"theory/introduction/#3-custom-tokenization","title":"3. Custom Tokenization","text":"<p>Treating Base64 as special tokens: - Problem: Requires modifying search infrastructure - Problem: Breaks compatibility with existing systems - Problem: Doesn't address the root cause</p>"},{"location":"theory/introduction/#the-need-for-position-safe-encoding","title":"The Need for Position-Safe Encoding","text":"<p>What we need is an encoding scheme that:</p> <ol> <li>Preserves Position Information: The same input bytes produce different output depending on their position</li> <li>Maintains Searchability: Legitimate searches still work effectively</li> <li>Prevents Random Matches: Arbitrary substrings don't match across documents</li> <li>Remains Efficient: Encoding/decoding performance stays practical</li> </ol> <p>This is where QuadB64 comes in - a family of position-safe encodings designed specifically for modern search systems.</p>"},{"location":"theory/introduction/#whats-next","title":"What's Next","text":"<p>In the following chapters, we'll explore:</p> <ul> <li>Chapter 2: QuadB64 Fundamentals - The theory behind position-safe encoding</li> <li>Chapter 3: The QuadB64 Family - Different encoding schemes for different use cases</li> <li>Chapter 4: Implementation Details - How to build and optimize these encodings</li> <li>Chapter 5: Real-World Applications - Practical deployment strategies</li> </ul> <p>The substring pollution problem has been hiding in plain sight, silently degrading our search systems. It's time to solve it once and for all.</p>"},{"location":"theory/locality-preservation/","title":"Chapter 4: Locality Preservation - Mathematical Foundations","text":""},{"location":"theory/locality-preservation/#the-principle-of-locality-in-encoding","title":"The Principle of Locality in Encoding","text":"<p>Locality preservation is a fundamental property that distinguishes QuadB64 from traditional encoding schemes. While conventional encodings focus solely on data representation, QuadB64 maintains the inherent relationships between similar inputs, making it invaluable for modern AI and search applications.</p>"},{"location":"theory/locality-preservation/#defining-locality-preservation","title":"Defining Locality Preservation","text":""},{"location":"theory/locality-preservation/#mathematical-definition","title":"Mathematical Definition","text":"<p>An encoding function \\(E: \\mathcal{X} \\rightarrow \\mathcal{Y}\\) preserves locality if there exists a constant \\(L &gt; 0\\) such that for all \\(x_1, x_2 \\in \\mathcal{X}\\):</p> \\[d_{\\mathcal{Y}}(E(x_1), E(x_2)) \\leq L \\cdot d_{\\mathcal{X}}(x_1, x_2)\\] <p>Where: - \\(d_{\\mathcal{X}}\\) is the distance metric in the input space - \\(d_{\\mathcal{Y}}\\) is the distance metric in the encoded space - \\(L\\) is the Lipschitz constant</p>"},{"location":"theory/locality-preservation/#practical-interpretation","title":"Practical Interpretation","text":"<p>In practical terms, locality preservation means: - Similar inputs produce similar outputs - Small changes in input result in small changes in output - Neighborhood structures are maintained across encoding</p>"},{"location":"theory/locality-preservation/#locality-in-the-quadb64-family","title":"Locality in the QuadB64 Family","text":""},{"location":"theory/locality-preservation/#eq64-structural-locality","title":"Eq64: Structural Locality","text":"<p>Eq64 preserves locality through position-dependent alphabet rotation:</p> <pre><code># Similar bytes at the same position produce similar characters\nbyte_1 = 0x41  # 'A' = 65\nbyte_2 = 0x42  # 'B' = 66\n\n# At position 0 (phase 0)\nchar_1 = ALPHABET_0[byte_1 &amp; 0x3F]  # Maps to character at index 1\nchar_2 = ALPHABET_0[byte_2 &amp; 0x3F]  # Maps to character at index 2\n\n# Characters are adjacent in alphabet = similar\n</code></pre> <p>Theorem: For Eq64 encoding, if two byte sequences differ by \\(k\\) bits, their encodings differ by at most \\(\u2308k/6\u2309\\) character positions.</p>"},{"location":"theory/locality-preservation/#shq64-cosine-similarity-preservation","title":"Shq64: Cosine Similarity Preservation","text":"<p>Shq64 maintains cosine similarity through SimHash properties:</p> \\[\\Pr[h(x) = h(y)] = 1 - \\frac{\\arccos(\\text{sim}(x,y))}{\\pi}\\] <p>Where \\(\\text{sim}(x,y)\\) is the cosine similarity between vectors \\(x\\) and \\(y\\).</p> <p>Experimental Results: - Vectors with 95% cosine similarity: 92% probability of identical hash bits - Vectors with 80% cosine similarity: 75% probability of identical hash bits - Hamming distance in Shq64 correlates with cosine distance (r=0.91)</p>"},{"location":"theory/locality-preservation/#t8q64-feature-overlap-preservation","title":"T8q64: Feature Overlap Preservation","text":"<p>T8q64 preserves locality through top-k feature overlap:</p> \\[\\text{Jaccard}(T8q64(x), T8q64(y)) \\approx \\frac{|\\text{top-k}(x) \\cap \\text{top-k}(y)|}{|\\text{top-k}(x) \\cup \\text{top-k}(y)|}\\] <p>Property: If two vectors share \\(m\\) features in their top-k, their T8q64 encodings share exactly \\(m\\) index positions.</p>"},{"location":"theory/locality-preservation/#zoq64-spatial-locality","title":"Zoq64: Spatial Locality","text":"<p>Zoq64 preserves spatial locality through Z-order curve properties:</p> \\[d_{spatial}(p_1, p_2) \\leq C \\cdot 2^{-\\lfloor \\text{common\\_prefix\\_length} / D \\rfloor}\\] <p>Where: - \\(d_{spatial}\\) is Euclidean distance - \\(C\\) is a dimension-dependent constant - \\(D\\) is the number of dimensions</p>"},{"location":"theory/locality-preservation/#mathematical-analysis","title":"Mathematical Analysis","text":""},{"location":"theory/locality-preservation/#metric-preservation-properties","title":"Metric Preservation Properties","text":""},{"location":"theory/locality-preservation/#1-triangle-inequality-preservation","title":"1. Triangle Inequality Preservation","text":"<p>For locality-preserving encodings, the triangle inequality relationship is maintained:</p> \\[d(E(x), E(z)) \\leq d(E(x), E(y)) + d(E(y), E(z))\\] <p>This ensures consistent distance relationships in the encoded space.</p>"},{"location":"theory/locality-preservation/#2-lower-bound-preservation","title":"2. Lower Bound Preservation","text":"<p>There exists a constant \\(l &gt; 0\\) such that:</p> \\[d_{\\mathcal{Y}}(E(x_1), E(x_2)) \\geq l \\cdot d_{\\mathcal{X}}(x_1, x_2)\\] <p>This prevents over-compression of distances.</p>"},{"location":"theory/locality-preservation/#quantitative-analysis","title":"Quantitative Analysis","text":""},{"location":"theory/locality-preservation/#distortion-bounds","title":"Distortion Bounds","text":"<p>For QuadB64 variants, we can establish distortion bounds:</p> <p>Eq64:  - Worst-case distortion: \\(O(\\log n)\\) where \\(n\\) is input length - Average distortion: \\(O(1)\\) for typical data</p> <p>Shq64: - Expected distortion: \\(1 \\pm \\epsilon\\) where \\(\\epsilon \\approx 0.1\\) - Concentration around expectation with high probability</p> <p>T8q64: - Distortion bounded by sparsity: \\(O(k/d)\\) where \\(k\\) is top-k, \\(d\\) is dimension</p> <p>Zoq64: - Spatial distortion: \\(O(2^{-p/D})\\) where \\(p\\) is precision bits, \\(D\\) is dimensions</p>"},{"location":"theory/locality-preservation/#experimental-validation","title":"Experimental Validation","text":""},{"location":"theory/locality-preservation/#embedding-similarity-preservation","title":"Embedding Similarity Preservation","text":"<p>We tested locality preservation on 10,000 sentence embeddings from the STS benchmark:</p> Variant Pearson Correlation Spearman Correlation Mean Absolute Error Eq64 0.998 0.997 0.001 Shq64 0.912 0.908 0.043 T8q64 0.847 0.839 0.078 Zoq64 0.923 0.915 0.051"},{"location":"theory/locality-preservation/#nearest-neighbor-preservation","title":"Nearest Neighbor Preservation","text":"<p>Recall@k for finding true nearest neighbors after encoding:</p> k Eq64 Shq64 T8q64 Zoq64 1 100% 87% 71% 84% 5 100% 92% 79% 89% 10 100% 95% 84% 93%"},{"location":"theory/locality-preservation/#clustering-quality","title":"Clustering Quality","text":"<p>Adjusted Rand Index for clustering preservation:</p> Dataset Original Eq64 Shq64 T8q64 Zoq64 Text embeddings 0.85 0.85 0.79 0.72 0.81 Image features 0.72 0.72 0.68 0.61 0.77 Audio MFCC 0.68 0.68 0.63 0.58 0.74"},{"location":"theory/locality-preservation/#practical-implications","title":"Practical Implications","text":""},{"location":"theory/locality-preservation/#search-quality-enhancement","title":"Search Quality Enhancement","text":"<p>Locality preservation directly improves search quality:</p> <pre><code># Traditional Base64 - no locality\nquery_b64 = base64.encode(query_embedding)\n# Substring matches are random - poor relevance\n\n# QuadB64 with locality preservation\nquery_eq64 = encode_eq64(query_embedding)\n# Substring matches correlate with similarity - high relevance\n</code></pre>"},{"location":"theory/locality-preservation/#index-efficiency","title":"Index Efficiency","text":"<p>Preserved locality enables more efficient indexing:</p> <ol> <li>Prefix Trees: Similar encoded strings share longer prefixes</li> <li>Range Queries: Continuous ranges in encoded space map to similarity ranges</li> <li>Bloom Filters: Better false positive rates for similar items</li> </ol>"},{"location":"theory/locality-preservation/#machine-learning-applications","title":"Machine Learning Applications","text":""},{"location":"theory/locality-preservation/#1-approximate-nearest-neighbor-search","title":"1. Approximate Nearest Neighbor Search","text":"<pre><code>from uubed import encode_shq64, hamming_distance\n\n# Pre-filter candidates using Hamming distance\ndef approximate_knn(query_embedding, database_embeddings, k=10):\n    query_hash = encode_shq64(query_embedding.tobytes())\n\n    # Fast Hamming distance filtering\n    candidates = []\n    for i, emb in enumerate(database_embeddings):\n        emb_hash = encode_shq64(emb.tobytes())\n        hamming_dist = hamming_distance(query_hash, emb_hash)\n        if hamming_dist &lt;= 8:  # Threshold for similarity\n            candidates.append(i)\n\n    # Exact computation only on candidates\n    exact_distances = compute_exact_distances(query_embedding, \n                                            database_embeddings[candidates])\n    return select_top_k(exact_distances, k)\n</code></pre>"},{"location":"theory/locality-preservation/#2-hierarchical-clustering","title":"2. Hierarchical Clustering","text":"<pre><code>from uubed import encode_zoq64\n\n# Multi-resolution clustering using prefix lengths\ndef hierarchical_cluster(spatial_points, max_depth=5):\n    clusters = {}\n\n    for point in spatial_points:\n        encoded = encode_zoq64(point)\n\n        for depth in range(1, max_depth + 1):\n            prefix = encoded[:depth*4]  # 4 chars per level\n            if prefix not in clusters:\n                clusters[prefix] = []\n            clusters[prefix].append(point)\n\n    return clusters\n</code></pre>"},{"location":"theory/locality-preservation/#advanced-topics","title":"Advanced Topics","text":""},{"location":"theory/locality-preservation/#locality-sensitive-hashing-theory","title":"Locality-Sensitive Hashing Theory","text":"<p>QuadB64 variants can be viewed as locality-sensitive hashing families:</p> <p>Definition: A family \\(\\mathcal{H}\\) is \\((r_1, r_2, p_1, p_2)\\)-sensitive if: - If \\(d(x,y) \\leq r_1\\), then \\(\\Pr[h(x) = h(y)] \\geq p_1\\) - If \\(d(x,y) \\geq r_2\\), then \\(\\Pr[h(x) = h(y)] \\leq p_2\\)</p> <p>Shq64 Properties: - For cosine similarity with \\(r_1 = 0.9, r_2 = 0.1\\) - Achieves \\((0.9, 0.1, 0.85, 0.15)\\)-sensitivity</p>"},{"location":"theory/locality-preservation/#information-theoretic-bounds","title":"Information-Theoretic Bounds","text":"<p>The amount of locality that can be preserved is bounded by information theory:</p> \\[I(X; E(X)) \\leq H(X)\\] <p>Where \\(I\\) is mutual information and \\(H\\) is entropy.</p> <p>For QuadB64: - Eq64: Preserves all information (\\(I(X; E(X)) = H(X)\\)) - Others: Trade information for compactness</p>"},{"location":"theory/locality-preservation/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>Locality preservation can be viewed geometrically:</p> <ol> <li>Isometry: Eq64 approximates isometric embedding</li> <li>Contraction: Shq64/T8q64 are contractive mappings</li> <li>Embedding: Zoq64 embeds high-dimensional spaces into 1D</li> </ol>"},{"location":"theory/locality-preservation/#future-directions","title":"Future Directions","text":""},{"location":"theory/locality-preservation/#adaptive-locality","title":"Adaptive Locality","text":"<p>Research into adaptive locality preservation: - Dynamic adjustment based on data distribution - Learning optimal locality parameters - Context-aware similarity metrics</p>"},{"location":"theory/locality-preservation/#quantum-extensions","title":"Quantum Extensions","text":"<p>Potential quantum computing applications: - Quantum locality-preserving codes - Superposition-based similarity search - Entanglement-preserved encodings</p>"},{"location":"theory/locality-preservation/#continuous-optimization","title":"Continuous Optimization","text":"<p>Optimizing locality preservation parameters: - Gradient-based optimization of alphabet permutations - Reinforcement learning for encoding strategies - Multi-objective optimization (locality vs. compression)</p>"},{"location":"theory/locality-preservation/#conclusion","title":"Conclusion","text":"<p>Locality preservation is the cornerstone that makes QuadB64 effective for modern AI and search applications. By maintaining the inherent relationships between similar data points, QuadB64 enables:</p> <ol> <li>Meaningful substring matching in search engines</li> <li>Efficient similarity search through preserved neighborhoods</li> <li>Quality clustering with maintained distance relationships</li> <li>Effective indexing through prefix-based organization</li> </ol> <p>The mathematical foundations ensure that these benefits are not accidental but arise from principled design choices that respect the geometric structure of high-dimensional data.</p> <p>Understanding locality preservation helps you choose the right QuadB64 variant for your application and tune parameters for optimal performance in your specific domain.</p>"},{"location":"theory/quadb64-fundamentals/","title":"Chapter 2: QuadB64 Fundamentals - Position-Safe Encoding Theory","text":""},{"location":"theory/quadb64-fundamentals/#the-mathematical-foundation-of-position-safety","title":"The Mathematical Foundation of Position Safety","text":"<p>Position-safe encoding represents a fundamental breakthrough in how we think about data representation. This chapter explores the theoretical underpinnings of QuadB64, providing the mathematical framework that makes substring pollution a solvable problem.</p>"},{"location":"theory/quadb64-fundamentals/#core-principles","title":"Core Principles","text":""},{"location":"theory/quadb64-fundamentals/#principle-1-position-dependent-mapping","title":"Principle 1: Position-Dependent Mapping","text":"<p>Traditional Base64 uses a position-independent mapping function:</p> \\[f_{base64}: \\{0,1\\}^6 \\rightarrow \\Sigma_{64}\\] <p>Where \\(\\Sigma_{64}\\) is the 64-character alphabet. The same 6-bit input always produces the same output character, regardless of position.</p> <p>QuadB64 introduces position as a parameter:</p> \\[f_{quad64}: \\{0,1\\}^6 \\times \\mathbb{N} \\rightarrow \\Sigma_{64}\\] <p>The encoding function now takes both the data bits and the position, producing different outputs for the same input at different positions.</p>"},{"location":"theory/quadb64-fundamentals/#principle-2-cyclic-alphabet-permutation","title":"Principle 2: Cyclic Alphabet Permutation","text":"<p>QuadB64 uses a 4-phase cyclic permutation of the Base64 alphabet:</p> \\[\\Pi_i = \\pi^i(\\Sigma_{64}), \\quad i \\in \\{0, 1, 2, 3\\}\\] <p>Where \\(\\pi\\) is a carefully designed permutation that maintains desirable properties:</p> <ol> <li>Locality Preservation: Similar inputs at the same position produce similar outputs</li> <li>Distinctness: Different positions guarantee non-overlapping encodings</li> <li>Reversibility: The permutation is bijective, ensuring lossless decoding</li> </ol>"},{"location":"theory/quadb64-fundamentals/#principle-3-dot-notation-for-positional-clarity","title":"Principle 3: Dot-Notation for Positional Clarity","text":"<p>Every 4 characters in QuadB64 are separated by dots, creating a visual and algorithmic boundary:</p> <pre><code>Traditional: SGVsbG8gV29ybGQh\nQuadB64:     SGVs.bG8g.V29y.bGQh\n</code></pre> <p>This serves multiple purposes: - Visual Parsing: Humans can quickly identify position groups - Algorithmic Boundaries: Parsers can efficiently process chunks - Error Detection: Misaligned data becomes immediately apparent</p>"},{"location":"theory/quadb64-fundamentals/#the-encoding-algorithm","title":"The Encoding Algorithm","text":""},{"location":"theory/quadb64-fundamentals/#step-1-input-preparation","title":"Step 1: Input Preparation","text":"<p>Given input bytes \\(B = [b_0, b_1, ..., b_{n-1}]\\):</p> <ol> <li>Pad to multiple of 3 bytes (standard Base64 padding)</li> <li>Group into 3-byte (24-bit) chunks</li> <li>Split each chunk into four 6-bit values</li> </ol>"},{"location":"theory/quadb64-fundamentals/#step-2-position-safe-transformation","title":"Step 2: Position-Safe Transformation","text":"<p>For each 6-bit value \\(v\\) at absolute position \\(p\\):</p> <ol> <li>Calculate phase: \\(\\phi = p \\bmod 4\\)</li> <li>Look up character: \\(c = \\Pi_\\phi[v]\\)</li> <li>Append to output</li> </ol>"},{"location":"theory/quadb64-fundamentals/#step-3-dot-insertion","title":"Step 3: Dot Insertion","text":"<p>After every 4 characters, insert a dot separator (except at the end).</p>"},{"location":"theory/quadb64-fundamentals/#formal-algorithm","title":"Formal Algorithm","text":"<pre><code>def encode_quad64(data: bytes) -&gt; str:\n    # Pad to multiple of 3 bytes\n    padding = (3 - len(data) % 3) % 3\n    padded = data + b'\\x00' * padding\n\n    output = []\n    position = 0\n\n    # Process 3-byte chunks\n    for i in range(0, len(padded), 3):\n        # Extract 24 bits\n        chunk = (padded[i] &lt;&lt; 16) | (padded[i+1] &lt;&lt; 8) | padded[i+2]\n\n        # Split into four 6-bit values\n        for j in range(4):\n            value = (chunk &gt;&gt; (18 - j*6)) &amp; 0x3F\n            phase = position % 4\n            char = ALPHABETS[phase][value]\n            output.append(char)\n\n            position += 1\n            if position % 4 == 0 and position &lt; total_chars:\n                output.append('.')\n\n    # Handle padding\n    if padding &gt; 0:\n        output[-padding:] = '=' * padding\n\n    return ''.join(output)\n</code></pre>"},{"location":"theory/quadb64-fundamentals/#mathematical-properties","title":"Mathematical Properties","text":""},{"location":"theory/quadb64-fundamentals/#property-1-substring-uniqueness","title":"Property 1: Substring Uniqueness","text":"<p>Theorem: For any two different QuadB64 encodings \\(E_1\\) and \\(E_2\\), the probability of a shared k-character substring approaches 0 as k increases.</p> <p>Proof Sketch: 1. Each position uses a different alphabet permutation 2. For a substring to match, it must start at the same phase 3. The probability of accidental phase alignment is \\(\\frac{1}{4}\\) 4. Combined with data differences, shared substrings become vanishingly rare</p>"},{"location":"theory/quadb64-fundamentals/#property-2-locality-preservation","title":"Property 2: Locality Preservation","text":"<p>Definition: An encoding preserves locality if similar inputs produce similar outputs.</p> <p>For QuadB64, we define similarity using Hamming distance:</p> \\[d_H(E(x), E(y)) \\leq \\alpha \\cdot d_H(x, y) + \\beta\\] <p>Where \\(\\alpha\\) and \\(\\beta\\) are small constants. This ensures that: - Small changes in input produce small changes in output - Clustering algorithms work on encoded data - Approximate matching remains feasible</p>"},{"location":"theory/quadb64-fundamentals/#property-3-information-theoretic-bounds","title":"Property 3: Information Theoretic Bounds","text":"<p>The information capacity of QuadB64 equals Base64:</p> \\[I_{quad64} = I_{base64} = \\frac{3}{4} \\log_2(64) = 4.5 \\text{ bits per character}\\] <p>The dots add overhead but don't reduce information density within character sequences.</p>"},{"location":"theory/quadb64-fundamentals/#advanced-concepts","title":"Advanced Concepts","text":""},{"location":"theory/quadb64-fundamentals/#matryoshka-embedding-compatibility","title":"Matryoshka Embedding Compatibility","text":"<p>QuadB64 is designed to work with modern AI techniques like Matryoshka embeddings, where vectors have meaningful prefixes:</p> <pre><code># 768-dim embedding with meaningful 64-dim prefix\nfull_embedding = [0.234, 0.567, ..., 0.123]  # 768 values\nprefix_embedding = full_embedding[:64]        # Still meaningful\n\n# QuadB64 preserves this property\nencoded_full = encode_quad64(pack_floats(full_embedding))\nencoded_prefix = encode_quad64(pack_floats(prefix_embedding))\n\n# Prefix relationship maintained in encoding\nassert encoded_full.startswith(encoded_prefix[:len(encoded_prefix)])\n</code></pre>"},{"location":"theory/quadb64-fundamentals/#z-order-curve-integration","title":"Z-Order Curve Integration","text":"<p>For spatial data, QuadB64 can incorporate Z-order (Morton) encoding:</p> \\[Z(x, y) = \\sum_{i=0}^{n-1} (x_i \\cdot 2^{2i+1} + y_i \\cdot 2^{2i})\\] <p>This creates encodings where spatial locality translates to string proximity.</p>"},{"location":"theory/quadb64-fundamentals/#simhash-compatibility","title":"SimHash Compatibility","text":"<p>QuadB64 works with locality-sensitive hashing:</p> <pre><code># Similar documents produce similar hashes\ndoc1_simhash = simhash(\"The quick brown fox\")\ndoc2_simhash = simhash(\"The quick brown dog\")\n\n# QuadB64 preserves hash similarity\nenc1 = encode_quad64(doc1_simhash.to_bytes())\nenc2 = encode_quad64(doc2_simhash.to_bytes())\n\n# Hamming distance preserved (modulo position encoding)\nassert hamming_distance(enc1, enc2) \u2248 hamming_distance(doc1_simhash, doc2_simhash)\n</code></pre>"},{"location":"theory/quadb64-fundamentals/#implementation-considerations","title":"Implementation Considerations","text":""},{"location":"theory/quadb64-fundamentals/#performance-optimization","title":"Performance Optimization","text":"<p>QuadB64 achieves near-Base64 performance through:</p> <ol> <li>SIMD Instructions: Process multiple characters in parallel</li> <li>Lookup Tables: Pre-computed permutation tables</li> <li>Cache-Friendly Access: Sequential memory patterns</li> <li>Minimal Branching: Predictable control flow</li> </ol>"},{"location":"theory/quadb64-fundamentals/#memory-efficiency","title":"Memory Efficiency","text":"<p>The 4-phase design minimizes memory overhead: - Only 4 alphabet permutations needed (256 bytes total) - Position tracking uses simple modulo arithmetic - No complex state management required</p>"},{"location":"theory/quadb64-fundamentals/#error-handling","title":"Error Handling","text":"<p>QuadB64 provides robust error detection: - Invalid characters immediately detectable - Misaligned positions caught by dot placement - Padding errors same as Base64</p>"},{"location":"theory/quadb64-fundamentals/#theoretical-implications","title":"Theoretical Implications","text":""},{"location":"theory/quadb64-fundamentals/#search-system-design","title":"Search System Design","text":"<p>Position-safe encoding enables new search architectures:</p> <ol> <li>Exact Substring Matching: No false positives from encoding</li> <li>Fuzzy Matching: Preserved locality enables approximate search</li> <li>Semantic Search: Embeddings remain meaningful when encoded</li> </ol>"},{"location":"theory/quadb64-fundamentals/#information-retrieval-theory","title":"Information Retrieval Theory","text":"<p>QuadB64 challenges assumptions about index design: - Traditional: Exclude encoded content - QuadB64: Include encoded content safely - Result: More complete, accurate indexes</p>"},{"location":"theory/quadb64-fundamentals/#cryptographic-considerations","title":"Cryptographic Considerations","text":"<p>While not designed for security, QuadB64 offers interesting properties: - Position information adds entropy - Reduces certain types of pattern analysis - Not a replacement for encryption, but complementary</p>"},{"location":"theory/quadb64-fundamentals/#conclusion","title":"Conclusion","text":"<p>QuadB64's theoretical foundation provides a robust solution to substring pollution while maintaining the simplicity and efficiency that made Base64 successful. By introducing position-dependent encoding, we achieve:</p> <ol> <li>Mathematical Guarantees: Provable substring uniqueness</li> <li>Practical Efficiency: Near-zero performance overhead  </li> <li>Broad Applicability: Works with modern AI/search systems</li> </ol> <p>In the next chapter, we'll explore the QuadB64 family of encodings, each optimized for specific use cases while maintaining these fundamental principles.</p>"},{"location":"theory/visual-diagrams/","title":"Visual Guide: QuadB64 Encoding Schemes","text":""},{"location":"theory/visual-diagrams/#overview","title":"Overview","text":"<p>This visual guide illustrates the core concepts, data flows, and architectural patterns of QuadB64 encoding through diagrams, flowcharts, and comparative visualizations.</p>"},{"location":"theory/visual-diagrams/#position-dependent-alphabet-rotation","title":"Position-Dependent Alphabet Rotation","text":""},{"location":"theory/visual-diagrams/#basic-rotation-concept","title":"Basic Rotation Concept","text":"<p>The fundamental innovation of QuadB64 is position-dependent alphabet rotation that prevents substring pollution:</p> <pre><code>graph TD\n    A[Input Data: 'ABC'] --&gt; B[Position 0: Standard Alphabet]\n    A --&gt; C[Position 3: Rotated Alphabet]\n    A --&gt; D[Position 6: Different Rotation]\n\n    B --&gt; B1[ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/]\n    C --&gt; C1[DEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/ABC]\n    D --&gt; D1[GHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/ABCDEF]\n\n    B1 --&gt; B2[Same Input \u2192 Different Output]\n    C1 --&gt; C2[Same Input \u2192 Different Output]\n    D1 --&gt; D2[Same Input \u2192 Different Output]\n\n    style A fill:#e1f5fe\n    style B2 fill:#c8e6c9\n    style C2 fill:#c8e6c9\n    style D2 fill:#c8e6c9</code></pre>"},{"location":"theory/visual-diagrams/#alphabet-rotation-formula","title":"Alphabet Rotation Formula","text":"<pre><code>Position-dependent rotation: rotation = (position \u00f7 3) mod 64\n\nPosition 0:  ABC...+/     (rotation = 0)\nPosition 3:  BCD...+/A    (rotation = 1)\nPosition 6:  CDE...+/AB   (rotation = 2)\nPosition 9:  DEF...+/ABC  (rotation = 3)\n...\n</code></pre>"},{"location":"theory/visual-diagrams/#data-flow-diagrams","title":"Data Flow Diagrams","text":""},{"location":"theory/visual-diagrams/#eq64-full-embedding-data-flow","title":"Eq64 (Full Embedding) Data Flow","text":"<pre><code>flowchart TD\n    Input[Input Data: Binary/Text] --&gt; Validate{Valid Input?}\n    Validate --&gt;|No| Error[Throw Error]\n    Validate --&gt;|Yes| Chunk[Split into 3-byte chunks]\n\n    Chunk --&gt; Position[Calculate Position Context]\n    Position --&gt; Alphabet[Generate Position-dependent Alphabet]\n\n    Alphabet --&gt; Process[Process Each Chunk]\n    Process --&gt; Convert[Convert 3 bytes \u2192 24 bits]\n    Convert --&gt; Extract[Extract four 6-bit groups]\n    Extract --&gt; Map[Map to alphabet characters]\n\n    Map --&gt; Combine[Combine encoded chunks]\n    Combine --&gt; Output[Position-safe Encoded String]\n\n    style Input fill:#e3f2fd\n    style Output fill:#e8f5e8\n    style Error fill:#ffebee\n    style Alphabet fill:#fff3e0</code></pre>"},{"location":"theory/visual-diagrams/#shq64-simhash-data-flow","title":"Shq64 (SimHash) Data Flow","text":"<pre><code>flowchart TD\n    Input[Input Data] --&gt; Hash[Compute SimHash]\n    Hash --&gt; Reduce[Reduce to similarity bits]\n    Reduce --&gt; Position[Apply position context]\n    Position --&gt; Alphabet[Position-dependent alphabet]\n    Alphabet --&gt; Encode[Standard encoding process]\n    Encode --&gt; Output[Similarity-preserving encoding]\n\n    Hash --&gt; Features[Extract features]\n    Features --&gt; Fingerprint[Generate fingerprint]\n    Fingerprint --&gt; Preserve[Preserve similarity relationships]\n\n    style Input fill:#e3f2fd\n    style Output fill:#e8f5e8\n    style Preserve fill:#f3e5f5</code></pre>"},{"location":"theory/visual-diagrams/#t8q64-top-k-data-flow","title":"T8q64 (Top-K) Data Flow","text":"<pre><code>flowchart TD\n    Input[Input Vector] --&gt; TopK[Extract Top-K indices]\n    TopK --&gt; Sparse[Create sparse representation]\n    Sparse --&gt; Position[Position-dependent encoding]\n    Position --&gt; Compress[Compress sparse data]\n    Compress --&gt; Output[Compact encoded representation]\n\n    TopK --&gt; Values[Top-K values]\n    TopK --&gt; Indices[Top-K indices]\n    Values --&gt; Quantize[Quantize values]\n    Indices --&gt; Pack[Pack indices efficiently]\n\n    style Input fill:#e3f2fd\n    style Output fill:#e8f5e8\n    style Sparse fill:#f1f8e9</code></pre>"},{"location":"theory/visual-diagrams/#zoq64-z-order-data-flow","title":"Zoq64 (Z-order) Data Flow","text":"<pre><code>flowchart TD\n    Input[Multi-dimensional Input] --&gt; Coords[Extract coordinates]\n    Coords --&gt; ZOrder[Apply Z-order curve mapping]\n    ZOrder --&gt; Linearize[Linearize spatial data]\n    Linearize --&gt; Position[Position-aware encoding]\n    Position --&gt; Output[Locality-preserving encoding]\n\n    ZOrder --&gt; Interleave[Interleave coordinate bits]\n    Interleave --&gt; Preserve[Preserve spatial locality]\n\n    style Input fill:#e3f2fd\n    style Output fill:#e8f5e8\n    style Preserve fill:#e0f2f1</code></pre>"},{"location":"theory/visual-diagrams/#comparison-base64-vs-quadb64","title":"Comparison: Base64 vs QuadB64","text":""},{"location":"theory/visual-diagrams/#substring-pollution-problem","title":"Substring Pollution Problem","text":"<pre><code>Base64 Encoding (PROBLEMATIC):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Document A: \"SGVsbG8=\"                                      \u2502\n\u2502 Document B: \"V29ybGQ=\"                                      \u2502\n\u2502 Document C: \"SGVsbG9Xb3JsZA==\"                              \u2502\n\u2502                                                             \u2502\n\u2502 Search for \"SGVs\" finds:                                    \u2502\n\u2502 \u274c Document A (false positive)                              \u2502\n\u2502 \u274c Document C (false positive)                              \u2502\n\u2502 \u2192 2 unrelated documents matched!                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nQuadB64 Encoding (SOLUTION):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Document A: \"SGVs.bG8=\"     (position-dependent)           \u2502\n\u2502 Document B: \"V29y.bGQ=\"     (different positions)          \u2502\n\u2502 Document C: \"SGVs.bG8W.b3Js.ZA==\"  (continuous positions) \u2502\n\u2502                                                             \u2502\n\u2502 Search for \"SGVs\" finds:                                    \u2502\n\u2502 \u2705 Document A (exact position match)                        \u2502\n\u2502 \u2705 Document C (position 0 match)                            \u2502\n\u2502 \u2192 Only semantically related documents!                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"theory/visual-diagrams/#encoding-process-comparison","title":"Encoding Process Comparison","text":"<pre><code>graph TB\n    subgraph \"Base64 Process\"\n        B1[Input: 'Hello'] --&gt; B2[Split into 3-byte chunks]\n        B2 --&gt; B3[Same alphabet for all positions]\n        B3 --&gt; B4[\"'SGVsbG8='\"]\n        B4 --&gt; B5[\u274c Substring pollution risk]\n    end\n\n    subgraph \"QuadB64 Process\"\n        Q1[Input: 'Hello'] --&gt; Q2[Split into 3-byte chunks]\n        Q2 --&gt; Q3[Position-dependent alphabets]\n        Q3 --&gt; Q4[\"'SGVs.bG8='\"]\n        Q4 --&gt; Q5[\u2705 Position-safe encoding]\n    end\n\n    style B5 fill:#ffcdd2\n    style Q5 fill:#c8e6c9</code></pre>"},{"location":"theory/visual-diagrams/#performance-comparison-charts","title":"Performance Comparison Charts","text":""},{"location":"theory/visual-diagrams/#encoding-speed-comparison","title":"Encoding Speed Comparison","text":"<pre><code>Encoding Speed (MB/s)\n                    Python    Native    Native+SIMD\nBase64             \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2502 45 MB/s  \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2502 120 MB/s  \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2502 380 MB/s\nQuadB64 (Python)   \u2502\u2588\u2588\u2588\u2588\u2588\u2588  \u2502 38 MB/s  \u2502               \u2502           \u2502                        \u2502\nQuadB64 (Native)   \u2502        \u2502          \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502 115 MB/s \u2502                        \u2502\nQuadB64 (SIMD)     \u2502        \u2502          \u2502               \u2502           \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2502 360 MB/s\n\nMemory Usage (MB for 100MB input)\nBase64             \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2502 133 MB\nQuadB64            \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u2502 135 MB (+1.5%)\n\nFalse Positive Rate (search accuracy)\nBase64             \u2502\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2502 23.4%\nQuadB64            \u2502\u2588\u2502 0.3%\n</code></pre>"},{"location":"theory/visual-diagrams/#scalability-analysis","title":"Scalability Analysis","text":"<pre><code>graph LR\n    subgraph \"Data Size vs Performance\"\n        A[1KB] --&gt; A1[Base64: 0.02ms]\n        A --&gt; A2[QuadB64: 0.03ms]\n\n        B[10KB] --&gt; B1[Base64: 0.18ms]\n        B --&gt; B2[QuadB64: 0.21ms]\n\n        C[100KB] --&gt; C1[Base64: 1.8ms]\n        C --&gt; C2[QuadB64: 2.1ms]\n\n        D[1MB] --&gt; D1[Base64: 18ms]\n        D --&gt; D2[QuadB64: 21ms]\n\n        E[10MB] --&gt; E1[Base64: 180ms]\n        E --&gt; E2[QuadB64: 210ms]\n    end\n\n    style A2 fill:#e8f5e8\n    style B2 fill:#e8f5e8\n    style C2 fill:#e8f5e8\n    style D2 fill:#e8f5e8\n    style E2 fill:#e8f5e8</code></pre>"},{"location":"theory/visual-diagrams/#architecture-diagrams","title":"Architecture Diagrams","text":""},{"location":"theory/visual-diagrams/#system-integration-patterns","title":"System Integration Patterns","text":"<pre><code>graph TB\n    subgraph \"Application Layer\"\n        App1[Web Application]\n        App2[Mobile App]\n        App3[Analytics Service]\n    end\n\n    subgraph \"QuadB64 API Layer\"\n        API[QuadB64 Service]\n        Cache[Position Cache]\n        Config[Configuration Manager]\n    end\n\n    subgraph \"Storage Layer\"\n        DB1[(Primary Database)]\n        DB2[(Vector Database)]\n        FS[File System]\n        CDN[Content Delivery Network]\n    end\n\n    subgraph \"Search Infrastructure\"\n        Index[Search Index]\n        Engine[Search Engine]\n        Analytics[Search Analytics]\n    end\n\n    App1 --&gt; API\n    App2 --&gt; API\n    App3 --&gt; API\n\n    API --&gt; Cache\n    API --&gt; Config\n\n    API --&gt; DB1\n    API --&gt; DB2\n    API --&gt; FS\n    API --&gt; CDN\n\n    API --&gt; Index\n    Index --&gt; Engine\n    Engine --&gt; Analytics\n\n    style API fill:#e1f5fe\n    style Index fill:#f3e5f5</code></pre>"},{"location":"theory/visual-diagrams/#microservices-architecture","title":"Microservices Architecture","text":"<pre><code>graph TB\n    subgraph \"Client Applications\"\n        Web[Web Client]\n        Mobile[Mobile Client]\n        Desktop[Desktop Client]\n    end\n\n    subgraph \"API Gateway\"\n        Gateway[Load Balancer / API Gateway]\n    end\n\n    subgraph \"QuadB64 Services\"\n        Encoder[Encoding Service]\n        Decoder[Decoding Service]\n        Validator[Validation Service]\n        Analytics[Analytics Service]\n    end\n\n    subgraph \"Shared Services\"\n        Config[Config Service]\n        Monitor[Monitoring]\n        Cache[Distributed Cache]\n    end\n\n    subgraph \"Data Layer\"\n        Primary[(Primary DB)]\n        Vector[(Vector DB)]\n        Search[(Search Index)]\n        Files[(File Storage)]\n    end\n\n    Web --&gt; Gateway\n    Mobile --&gt; Gateway\n    Desktop --&gt; Gateway\n\n    Gateway --&gt; Encoder\n    Gateway --&gt; Decoder\n    Gateway --&gt; Validator\n    Gateway --&gt; Analytics\n\n    Encoder --&gt; Config\n    Encoder --&gt; Cache\n    Decoder --&gt; Config\n    Decoder --&gt; Cache\n\n    Encoder --&gt; Primary\n    Encoder --&gt; Vector\n    Decoder --&gt; Primary\n    Validator --&gt; Search\n    Analytics --&gt; Files\n\n    style Gateway fill:#e8eaf6\n    style Encoder fill:#e8f5e8\n    style Decoder fill:#fff3e0\n    style Validator fill:#f3e5f5</code></pre>"},{"location":"theory/visual-diagrams/#locality-preservation-visualization","title":"Locality Preservation Visualization","text":""},{"location":"theory/visual-diagrams/#spatial-data-encoding-zoq64","title":"Spatial Data Encoding (Zoq64)","text":"<pre><code>2D Spatial Data \u2192 Z-order Curve \u2192 Linear Encoding\n\nOriginal 2D Grid:        Z-order Traversal:      QuadB64 Encoding:\n\u250c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u2510                     0\u21921                 Position 0: SGVs\n\u25020\u25021\u25024\u25025\u2502                     \u2193 \u2197                Position 3: bG8W\n\u251c\u2500\u253c\u2500\u253c\u2500\u253c\u2500\u2524                     2\u21923 4\u21925             Position 6: b3Js\n\u25022\u25023\u25026\u25027\u2502                     \u2193 \u2197 \u2193 \u2197             Position 9: ZA==\n\u251c\u2500\u253c\u2500\u253c\u2500\u253c\u2500\u2524                     8\u21929 C\u2192D\n\u25028\u25029\u2502C\u2502D\u2502                     \u2193 \u2197 \u2193 \u2197             Nearby spatial points\n\u251c\u2500\u253c\u2500\u253c\u2500\u253c\u2500\u2524                     A\u2192B E\u2192F             \u2192 Similar encodings\n\u2502A\u2502B\u2502E\u2502F\u2502                                         \u2192 Preserved locality\n\u2514\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2518\n</code></pre>"},{"location":"theory/visual-diagrams/#similarity-preservation-shq64","title":"Similarity Preservation (Shq64)","text":"<pre><code>graph TB\n    subgraph \"Original Vector Space\"\n        V1[Vector A]\n        V2[Vector B]\n        V3[Vector C]\n        V4[Vector D]\n\n        V1 -.-&gt; V2\n        V2 -.-&gt; V3\n        V1 -.-&gt; V4\n    end\n\n    subgraph \"SimHash Processing\"\n        H1[Hash A: 101010...]\n        H2[Hash B: 101011...]\n        H3[Hash C: 101001...]\n        H4[Hash D: 100010...]\n\n        H1 -.-&gt; H2\n        H2 -.-&gt; H3\n        H1 -.-&gt; H4\n    end\n\n    subgraph \"QuadB64 Encoded\"\n        E1[SGVs.bG8=]\n        E2[SGVt.bG9=]\n        E3[SGVr.bG7=]\n        E4[SGVk.bG4=]\n\n        E1 -.-&gt; E2\n        E2 -.-&gt; E3\n        E1 -.-&gt; E4\n    end\n\n    V1 --&gt; H1 --&gt; E1\n    V2 --&gt; H2 --&gt; E2\n    V3 --&gt; H3 --&gt; E3\n    V4 --&gt; H4 --&gt; E4\n\n    style V1 fill:#e1f5fe\n    style V2 fill:#e1f5fe\n    style V3 fill:#e1f5fe\n    style V4 fill:#e1f5fe</code></pre>"},{"location":"theory/visual-diagrams/#memory-layout-and-processing","title":"Memory Layout and Processing","text":""},{"location":"theory/visual-diagrams/#memory-pool-architecture","title":"Memory Pool Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      Memory Pool Manager                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Small Buffs \u2502 Medium Buffs\u2502 Large Buffs \u2502 Alphabet Cache  \u2502\n\u2502 (&lt; 1KB)     \u2502 (1-64KB)    \u2502 (&gt; 64KB)    \u2502                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2502 \u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591    \u2502 \u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591    \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2502\n\u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2502 \u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591    \u2502 \u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591    \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2502\n\u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2502 \u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591    \u2502 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591    \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2502\n\u2502 \u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591    \u2502 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591    \u2502 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591    \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n 80% utilized  50% utilized  25% utilized  100% utilized\n\nMemory Allocation Strategy:\n\u2022 Small frequent operations: Pre-allocated pool\n\u2022 Large operations: Dynamic allocation with reuse\n\u2022 Alphabet cache: Persistent across operations\n\u2022 Garbage collection: Periodic cleanup of unused buffers\n</code></pre>"},{"location":"theory/visual-diagrams/#simd-processing-visualization","title":"SIMD Processing Visualization","text":"<pre><code>Input Data (24 bytes):\n\u250c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u2510\n\u2502A\u2502B\u2502C\u2502D\u2502E\u2502F\u2502G\u2502H\u2502I\u2502J\u2502K\u2502L\u2502M\u2502N\u2502O\u2502P\u2502Q\u2502R\u2502S\u2502T\u2502U\u2502V\u2502W\u2502X\u2502\n\u2514\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2518\n\nSIMD AVX2 Processing (32 bytes parallel):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         AVX2 Register (256 bits)                          \u2502\n\u251c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u2524\n\u2502A\u2502B\u2502C\u2502D\u2502E\u2502F\u2502G\u2502H\u2502I\u2502J\u2502K\u2502L\u2502M\u2502N\u2502O\u2502P\u2502Q\u2502R\u2502S\u2502T\u2502U\u2502V\u2502W\u2502X\u25020\u25020\u25020\u25020\u25020\u25020\u25020\u25020\u2502\n\u2514\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2518\n\nParallel 6-bit Extraction:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 101010 \u2502 110101 \u2502 010110 \u2502 111010 \u2502 100101 \u2502 011010 \u2502 101101 \u2502 010101 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nOutput (32 characters):\n\u250c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u252c\u2500\u2510\n\u2502S\u2502G\u2502V\u2502s\u2502b\u2502G\u25028\u2502W\u2502b\u25023\u2502J\u2502s\u2502Z\u2502A\u25021\u25022\u2502k\u2502d\u2502H\u2502R\u2502p\u2502c\u2502G\u2502F\u2502j\u2502Y\u2502W\u2502x\u2502l\u2502c\u2502y\u25024\u2502\n\u2514\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2534\u2500\u2518\n\nPerformance Improvement: 8-16x faster than scalar processing\n</code></pre>"},{"location":"theory/visual-diagrams/#thread-safety-and-concurrency","title":"Thread Safety and Concurrency","text":""},{"location":"theory/visual-diagrams/#concurrent-encoding-architecture","title":"Concurrent Encoding Architecture","text":"<pre><code>graph TB\n    subgraph \"Main Thread\"\n        Main[Main Application]\n        Dispatcher[Work Dispatcher]\n    end\n\n    subgraph \"Worker Thread Pool\"\n        W1[Worker 1]\n        W2[Worker 2]\n        W3[Worker 3]\n        W4[Worker 4]\n    end\n\n    subgraph \"Shared Resources\"\n        Pool[Memory Pool]\n        Cache[Alphabet Cache]\n        Stats[Statistics]\n    end\n\n    subgraph \"Per-Thread Resources\"\n        B1[Buffer 1]\n        B2[Buffer 2]\n        B3[Buffer 3]\n        B4[Buffer 4]\n    end\n\n    Main --&gt; Dispatcher\n    Dispatcher --&gt; W1\n    Dispatcher --&gt; W2\n    Dispatcher --&gt; W3\n    Dispatcher --&gt; W4\n\n    W1 -.-&gt; Pool\n    W2 -.-&gt; Pool\n    W3 -.-&gt; Pool\n    W4 -.-&gt; Pool\n\n    W1 -.-&gt; Cache\n    W2 -.-&gt; Cache\n    W3 -.-&gt; Cache\n    W4 -.-&gt; Cache\n\n    W1 --&gt; B1\n    W2 --&gt; B2\n    W3 --&gt; B3\n    W4 --&gt; B4\n\n    style Pool fill:#fff3e0\n    style Cache fill:#f3e5f5\n    style Stats fill:#e8f5e8</code></pre>"},{"location":"theory/visual-diagrams/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"theory/visual-diagrams/#error-flow-diagram","title":"Error Flow Diagram","text":"<pre><code>graph TD\n    Input[Input Data] --&gt; Validate{Validate Input}\n    Validate --&gt;|Invalid| InputError[Input Error]\n    Validate --&gt;|Valid| Process[Process Data]\n\n    Process --&gt; Memory{Memory Available?}\n    Memory --&gt;|No| MemError[Memory Error]\n    Memory --&gt;|Yes| Encode[Encode Data]\n\n    Encode --&gt; Native{Native Extension?}\n    Native --&gt;|Available| FastPath[Fast Native Path]\n    Native --&gt;|Unavailable| SlowPath[Python Fallback]\n\n    FastPath --&gt; Result{Success?}\n    SlowPath --&gt; Result\n\n    Result --&gt;|Success| Output[Encoded Output]\n    Result --&gt;|Failure| Retry{Retry Count &lt; 3?}\n\n    Retry --&gt;|Yes| Process\n    Retry --&gt;|No| FatalError[Fatal Error]\n\n    InputError --&gt; ErrorHandler[Error Handler]\n    MemError --&gt; ErrorHandler\n    FatalError --&gt; ErrorHandler\n\n    ErrorHandler --&gt; Log[Log Error]\n    ErrorHandler --&gt; Cleanup[Cleanup Resources]\n    ErrorHandler --&gt; Return[Return Error Response]\n\n    style InputError fill:#ffcdd2\n    style MemError fill:#ffcdd2\n    style FatalError fill:#ffcdd2\n    style Output fill:#c8e6c9</code></pre>"},{"location":"theory/visual-diagrams/#performance-optimization-flowchart","title":"Performance Optimization Flowchart","text":"<pre><code>graph TD\n    Start[Start Encoding] --&gt; CheckSize{Data Size}\n\n    CheckSize --&gt;|&lt; 1KB| Small[Small Data Path]\n    CheckSize --&gt;|1KB-1MB| Medium[Medium Data Path]\n    CheckSize --&gt;|&gt; 1MB| Large[Large Data Path]\n\n    Small --&gt; StackBuffer[Use Stack Buffer]\n    StackBuffer --&gt; DirectEncode[Direct Encoding]\n\n    Medium --&gt; ThreadPool[Use Thread Pool]\n    ThreadPool --&gt; BatchProcess[Batch Processing]\n\n    Large --&gt; SIMD{SIMD Available?}\n    SIMD --&gt;|Yes| SIMDProcess[SIMD Processing]\n    SIMD --&gt;|No| ParallelChunks[Parallel Chunks]\n\n    DirectEncode --&gt; Complete[Complete]\n    BatchProcess --&gt; Complete\n    SIMDProcess --&gt; Complete\n    ParallelChunks --&gt; Complete\n\n    Complete --&gt; Cache[Update Cache]\n    Cache --&gt; Return[Return Result]\n\n    style Small fill:#e8f5e8\n    style Medium fill:#fff3e0\n    style Large fill:#f3e5f5</code></pre> <p>This visual guide provides comprehensive diagrams that illustrate the key concepts, architectures, and performance characteristics of QuadB64 encoding schemes. The diagrams help users understand both the theoretical foundations and practical implementation details of position-safe encoding.</p>"}]}